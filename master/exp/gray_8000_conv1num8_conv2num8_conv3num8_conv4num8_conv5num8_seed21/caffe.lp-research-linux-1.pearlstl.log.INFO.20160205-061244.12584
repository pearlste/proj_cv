Log file created at: 2016/02/05 06:12:44
Running on machine: lp-research-linux-1
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0205 06:12:44.464579 12584 caffe.cpp:177] Use CPU.
I0205 06:12:44.465428 12584 solver.cpp:48] Initializing solver from parameters: 
test_iter: 10
test_interval: 100
base_lr: 0.001
display: 10
max_iter: 1600
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 100
snapshot_prefix: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap_"
solver_mode: CPU
random_seed: 21
net: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/train_val.prototxt"
I0205 06:12:44.465595 12584 solver.cpp:91] Creating training net from net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/train_val.prototxt
I0205 06:12:44.466230 12584 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0205 06:12:44.466266 12584 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0205 06:12:44.466516 12584 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 06:12:44.466657 12584 layer_factory.hpp:77] Creating layer data
I0205 06:12:44.466842 12584 net.cpp:106] Creating Layer data
I0205 06:12:44.466862 12584 net.cpp:411] data -> data
I0205 06:12:44.466958 12584 net.cpp:411] data -> label
I0205 06:12:44.466990 12584 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto
I0205 06:12:44.467161 12585 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db
I0205 06:12:44.468119 12584 data_layer.cpp:41] output data size: 100,1,227,227
I0205 06:12:44.503989 12584 net.cpp:150] Setting up data
I0205 06:12:44.504053 12584 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 06:12:44.504063 12584 net.cpp:157] Top shape: 100 (100)
I0205 06:12:44.504070 12584 net.cpp:165] Memory required for data: 20612000
I0205 06:12:44.504091 12584 layer_factory.hpp:77] Creating layer conv1
I0205 06:12:44.504137 12584 net.cpp:106] Creating Layer conv1
I0205 06:12:44.504148 12584 net.cpp:454] conv1 <- data
I0205 06:12:44.504170 12584 net.cpp:411] conv1 -> conv1
I0205 06:12:44.504284 12584 net.cpp:150] Setting up conv1
I0205 06:12:44.504295 12584 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 06:12:44.504302 12584 net.cpp:165] Memory required for data: 30292000
I0205 06:12:44.504319 12584 layer_factory.hpp:77] Creating layer relu1
I0205 06:12:44.504333 12584 net.cpp:106] Creating Layer relu1
I0205 06:12:44.504338 12584 net.cpp:454] relu1 <- conv1
I0205 06:12:44.504348 12584 net.cpp:397] relu1 -> conv1 (in-place)
I0205 06:12:44.504362 12584 net.cpp:150] Setting up relu1
I0205 06:12:44.504370 12584 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 06:12:44.504375 12584 net.cpp:165] Memory required for data: 39972000
I0205 06:12:44.504381 12584 layer_factory.hpp:77] Creating layer pool1
I0205 06:12:44.504393 12584 net.cpp:106] Creating Layer pool1
I0205 06:12:44.504398 12584 net.cpp:454] pool1 <- conv1
I0205 06:12:44.504407 12584 net.cpp:411] pool1 -> pool1
I0205 06:12:44.504433 12584 net.cpp:150] Setting up pool1
I0205 06:12:44.504441 12584 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:12:44.504446 12584 net.cpp:165] Memory required for data: 42304800
I0205 06:12:44.504452 12584 layer_factory.hpp:77] Creating layer norm1
I0205 06:12:44.504474 12584 net.cpp:106] Creating Layer norm1
I0205 06:12:44.504493 12584 net.cpp:454] norm1 <- pool1
I0205 06:12:44.504503 12584 net.cpp:411] norm1 -> norm1
I0205 06:12:44.504520 12584 net.cpp:150] Setting up norm1
I0205 06:12:44.504528 12584 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:12:44.504534 12584 net.cpp:165] Memory required for data: 44637600
I0205 06:12:44.504539 12584 layer_factory.hpp:77] Creating layer conv2
I0205 06:12:44.504551 12584 net.cpp:106] Creating Layer conv2
I0205 06:12:44.504557 12584 net.cpp:454] conv2 <- norm1
I0205 06:12:44.504566 12584 net.cpp:411] conv2 -> conv2
I0205 06:12:44.504598 12584 net.cpp:150] Setting up conv2
I0205 06:12:44.504607 12584 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:12:44.504613 12584 net.cpp:165] Memory required for data: 46970400
I0205 06:12:44.504624 12584 layer_factory.hpp:77] Creating layer relu2
I0205 06:12:44.504633 12584 net.cpp:106] Creating Layer relu2
I0205 06:12:44.504639 12584 net.cpp:454] relu2 <- conv2
I0205 06:12:44.504647 12584 net.cpp:397] relu2 -> conv2 (in-place)
I0205 06:12:44.504655 12584 net.cpp:150] Setting up relu2
I0205 06:12:44.504662 12584 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:12:44.504667 12584 net.cpp:165] Memory required for data: 49303200
I0205 06:12:44.504673 12584 layer_factory.hpp:77] Creating layer pool2
I0205 06:12:44.504681 12584 net.cpp:106] Creating Layer pool2
I0205 06:12:44.504688 12584 net.cpp:454] pool2 <- conv2
I0205 06:12:44.504695 12584 net.cpp:411] pool2 -> pool2
I0205 06:12:44.504704 12584 net.cpp:150] Setting up pool2
I0205 06:12:44.504711 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.504716 12584 net.cpp:165] Memory required for data: 49844000
I0205 06:12:44.504721 12584 layer_factory.hpp:77] Creating layer norm2
I0205 06:12:44.504731 12584 net.cpp:106] Creating Layer norm2
I0205 06:12:44.504739 12584 net.cpp:454] norm2 <- pool2
I0205 06:12:44.504747 12584 net.cpp:411] norm2 -> norm2
I0205 06:12:44.504757 12584 net.cpp:150] Setting up norm2
I0205 06:12:44.504763 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.504768 12584 net.cpp:165] Memory required for data: 50384800
I0205 06:12:44.504776 12584 layer_factory.hpp:77] Creating layer conv3
I0205 06:12:44.504787 12584 net.cpp:106] Creating Layer conv3
I0205 06:12:44.504793 12584 net.cpp:454] conv3 <- norm2
I0205 06:12:44.504802 12584 net.cpp:411] conv3 -> conv3
I0205 06:12:44.504830 12584 net.cpp:150] Setting up conv3
I0205 06:12:44.504838 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.504843 12584 net.cpp:165] Memory required for data: 50925600
I0205 06:12:44.504853 12584 layer_factory.hpp:77] Creating layer relu3
I0205 06:12:44.504861 12584 net.cpp:106] Creating Layer relu3
I0205 06:12:44.504868 12584 net.cpp:454] relu3 <- conv3
I0205 06:12:44.504878 12584 net.cpp:397] relu3 -> conv3 (in-place)
I0205 06:12:44.504885 12584 net.cpp:150] Setting up relu3
I0205 06:12:44.504891 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.504896 12584 net.cpp:165] Memory required for data: 51466400
I0205 06:12:44.504901 12584 layer_factory.hpp:77] Creating layer conv4
I0205 06:12:44.504914 12584 net.cpp:106] Creating Layer conv4
I0205 06:12:44.504919 12584 net.cpp:454] conv4 <- conv3
I0205 06:12:44.504926 12584 net.cpp:411] conv4 -> conv4
I0205 06:12:44.504948 12584 net.cpp:150] Setting up conv4
I0205 06:12:44.504956 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.504961 12584 net.cpp:165] Memory required for data: 52007200
I0205 06:12:44.504974 12584 layer_factory.hpp:77] Creating layer relu4
I0205 06:12:44.504982 12584 net.cpp:106] Creating Layer relu4
I0205 06:12:44.504988 12584 net.cpp:454] relu4 <- conv4
I0205 06:12:44.504995 12584 net.cpp:397] relu4 -> conv4 (in-place)
I0205 06:12:44.505005 12584 net.cpp:150] Setting up relu4
I0205 06:12:44.505012 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.505018 12584 net.cpp:165] Memory required for data: 52548000
I0205 06:12:44.505023 12584 layer_factory.hpp:77] Creating layer conv5
I0205 06:12:44.505040 12584 net.cpp:106] Creating Layer conv5
I0205 06:12:44.505051 12584 net.cpp:454] conv5 <- conv4
I0205 06:12:44.505061 12584 net.cpp:411] conv5 -> conv5
I0205 06:12:44.505085 12584 net.cpp:150] Setting up conv5
I0205 06:12:44.505095 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.505100 12584 net.cpp:165] Memory required for data: 53088800
I0205 06:12:44.505110 12584 layer_factory.hpp:77] Creating layer relu5
I0205 06:12:44.505118 12584 net.cpp:106] Creating Layer relu5
I0205 06:12:44.505125 12584 net.cpp:454] relu5 <- conv5
I0205 06:12:44.505131 12584 net.cpp:397] relu5 -> conv5 (in-place)
I0205 06:12:44.505141 12584 net.cpp:150] Setting up relu5
I0205 06:12:44.505148 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.505153 12584 net.cpp:165] Memory required for data: 53629600
I0205 06:12:44.505158 12584 layer_factory.hpp:77] Creating layer pool5
I0205 06:12:44.505168 12584 net.cpp:106] Creating Layer pool5
I0205 06:12:44.505173 12584 net.cpp:454] pool5 <- conv5
I0205 06:12:44.505182 12584 net.cpp:411] pool5 -> pool5
I0205 06:12:44.505192 12584 net.cpp:150] Setting up pool5
I0205 06:12:44.505198 12584 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0205 06:12:44.505204 12584 net.cpp:165] Memory required for data: 53744800
I0205 06:12:44.505209 12584 layer_factory.hpp:77] Creating layer fc6
I0205 06:12:44.505225 12584 net.cpp:106] Creating Layer fc6
I0205 06:12:44.505231 12584 net.cpp:454] fc6 <- pool5
I0205 06:12:44.505240 12584 net.cpp:411] fc6 -> fc6
I0205 06:12:44.506073 12584 net.cpp:150] Setting up fc6
I0205 06:12:44.506088 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.506094 12584 net.cpp:165] Memory required for data: 53847200
I0205 06:12:44.506103 12584 layer_factory.hpp:77] Creating layer relu6
I0205 06:12:44.506113 12584 net.cpp:106] Creating Layer relu6
I0205 06:12:44.506119 12584 net.cpp:454] relu6 <- fc6
I0205 06:12:44.506127 12584 net.cpp:397] relu6 -> fc6 (in-place)
I0205 06:12:44.506137 12584 net.cpp:150] Setting up relu6
I0205 06:12:44.506144 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.506148 12584 net.cpp:165] Memory required for data: 53949600
I0205 06:12:44.506155 12584 layer_factory.hpp:77] Creating layer drop6
I0205 06:12:44.506165 12584 net.cpp:106] Creating Layer drop6
I0205 06:12:44.506171 12584 net.cpp:454] drop6 <- fc6
I0205 06:12:44.506178 12584 net.cpp:397] drop6 -> fc6 (in-place)
I0205 06:12:44.506196 12584 net.cpp:150] Setting up drop6
I0205 06:12:44.506202 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.506207 12584 net.cpp:165] Memory required for data: 54052000
I0205 06:12:44.506213 12584 layer_factory.hpp:77] Creating layer fc7
I0205 06:12:44.506225 12584 net.cpp:106] Creating Layer fc7
I0205 06:12:44.506233 12584 net.cpp:454] fc7 <- fc6
I0205 06:12:44.506242 12584 net.cpp:411] fc7 -> fc7
I0205 06:12:44.506932 12584 net.cpp:150] Setting up fc7
I0205 06:12:44.506944 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.506950 12584 net.cpp:165] Memory required for data: 54154400
I0205 06:12:44.506958 12584 layer_factory.hpp:77] Creating layer relu7
I0205 06:12:44.506973 12584 net.cpp:106] Creating Layer relu7
I0205 06:12:44.506981 12584 net.cpp:454] relu7 <- fc7
I0205 06:12:44.506989 12584 net.cpp:397] relu7 -> fc7 (in-place)
I0205 06:12:44.506997 12584 net.cpp:150] Setting up relu7
I0205 06:12:44.507004 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.507009 12584 net.cpp:165] Memory required for data: 54256800
I0205 06:12:44.507015 12584 layer_factory.hpp:77] Creating layer drop7
I0205 06:12:44.507027 12584 net.cpp:106] Creating Layer drop7
I0205 06:12:44.507035 12584 net.cpp:454] drop7 <- fc7
I0205 06:12:44.507041 12584 net.cpp:397] drop7 -> fc7 (in-place)
I0205 06:12:44.507051 12584 net.cpp:150] Setting up drop7
I0205 06:12:44.507057 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.507062 12584 net.cpp:165] Memory required for data: 54359200
I0205 06:12:44.507068 12584 layer_factory.hpp:77] Creating layer fc8
I0205 06:12:44.507082 12584 net.cpp:106] Creating Layer fc8
I0205 06:12:44.507092 12584 net.cpp:454] fc8 <- fc7
I0205 06:12:44.507107 12584 net.cpp:411] fc8 -> fc8
I0205 06:12:44.507141 12584 net.cpp:150] Setting up fc8
I0205 06:12:44.507149 12584 net.cpp:157] Top shape: 100 2 (200)
I0205 06:12:44.507154 12584 net.cpp:165] Memory required for data: 54360000
I0205 06:12:44.507164 12584 layer_factory.hpp:77] Creating layer loss
I0205 06:12:44.507174 12584 net.cpp:106] Creating Layer loss
I0205 06:12:44.507179 12584 net.cpp:454] loss <- fc8
I0205 06:12:44.507186 12584 net.cpp:454] loss <- label
I0205 06:12:44.507197 12584 net.cpp:411] loss -> loss
I0205 06:12:44.507211 12584 layer_factory.hpp:77] Creating layer loss
I0205 06:12:44.507236 12584 net.cpp:150] Setting up loss
I0205 06:12:44.507242 12584 net.cpp:157] Top shape: (1)
I0205 06:12:44.507248 12584 net.cpp:160]     with loss weight 1
I0205 06:12:44.507278 12584 net.cpp:165] Memory required for data: 54360004
I0205 06:12:44.507297 12584 net.cpp:226] loss needs backward computation.
I0205 06:12:44.507304 12584 net.cpp:226] fc8 needs backward computation.
I0205 06:12:44.507309 12584 net.cpp:226] drop7 needs backward computation.
I0205 06:12:44.507315 12584 net.cpp:226] relu7 needs backward computation.
I0205 06:12:44.507320 12584 net.cpp:226] fc7 needs backward computation.
I0205 06:12:44.507325 12584 net.cpp:226] drop6 needs backward computation.
I0205 06:12:44.507331 12584 net.cpp:226] relu6 needs backward computation.
I0205 06:12:44.507336 12584 net.cpp:226] fc6 needs backward computation.
I0205 06:12:44.507342 12584 net.cpp:226] pool5 needs backward computation.
I0205 06:12:44.507349 12584 net.cpp:226] relu5 needs backward computation.
I0205 06:12:44.507354 12584 net.cpp:226] conv5 needs backward computation.
I0205 06:12:44.507359 12584 net.cpp:226] relu4 needs backward computation.
I0205 06:12:44.507364 12584 net.cpp:226] conv4 needs backward computation.
I0205 06:12:44.507369 12584 net.cpp:226] relu3 needs backward computation.
I0205 06:12:44.507375 12584 net.cpp:226] conv3 needs backward computation.
I0205 06:12:44.507385 12584 net.cpp:226] norm2 needs backward computation.
I0205 06:12:44.507390 12584 net.cpp:226] pool2 needs backward computation.
I0205 06:12:44.507396 12584 net.cpp:226] relu2 needs backward computation.
I0205 06:12:44.507401 12584 net.cpp:226] conv2 needs backward computation.
I0205 06:12:44.507407 12584 net.cpp:226] norm1 needs backward computation.
I0205 06:12:44.507412 12584 net.cpp:226] pool1 needs backward computation.
I0205 06:12:44.507418 12584 net.cpp:226] relu1 needs backward computation.
I0205 06:12:44.507426 12584 net.cpp:226] conv1 needs backward computation.
I0205 06:12:44.507431 12584 net.cpp:228] data does not need backward computation.
I0205 06:12:44.507437 12584 net.cpp:270] This network produces output loss
I0205 06:12:44.507467 12584 net.cpp:283] Network initialization done.
I0205 06:12:44.508227 12584 solver.cpp:181] Creating test net (#0) specified by net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/train_val.prototxt
I0205 06:12:44.508285 12584 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0205 06:12:44.508584 12584 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 06:12:44.508761 12584 layer_factory.hpp:77] Creating layer data
I0205 06:12:44.508925 12584 net.cpp:106] Creating Layer data
I0205 06:12:44.508940 12584 net.cpp:411] data -> data
I0205 06:12:44.508955 12584 net.cpp:411] data -> label
I0205 06:12:44.508972 12584 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto
I0205 06:12:44.509237 12589 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db
I0205 06:12:44.509935 12584 data_layer.cpp:41] output data size: 100,1,227,227
I0205 06:12:44.543457 12584 net.cpp:150] Setting up data
I0205 06:12:44.543488 12584 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 06:12:44.543495 12584 net.cpp:157] Top shape: 100 (100)
I0205 06:12:44.543501 12584 net.cpp:165] Memory required for data: 20612000
I0205 06:12:44.543510 12584 layer_factory.hpp:77] Creating layer label_data_1_split
I0205 06:12:44.543527 12584 net.cpp:106] Creating Layer label_data_1_split
I0205 06:12:44.543534 12584 net.cpp:454] label_data_1_split <- label
I0205 06:12:44.543546 12584 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0205 06:12:44.543560 12584 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0205 06:12:44.543572 12584 net.cpp:150] Setting up label_data_1_split
I0205 06:12:44.543579 12584 net.cpp:157] Top shape: 100 (100)
I0205 06:12:44.543586 12584 net.cpp:157] Top shape: 100 (100)
I0205 06:12:44.543591 12584 net.cpp:165] Memory required for data: 20612800
I0205 06:12:44.543598 12584 layer_factory.hpp:77] Creating layer conv1
I0205 06:12:44.543612 12584 net.cpp:106] Creating Layer conv1
I0205 06:12:44.543618 12584 net.cpp:454] conv1 <- data
I0205 06:12:44.543627 12584 net.cpp:411] conv1 -> conv1
I0205 06:12:44.543678 12584 net.cpp:150] Setting up conv1
I0205 06:12:44.543689 12584 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 06:12:44.543694 12584 net.cpp:165] Memory required for data: 30292800
I0205 06:12:44.543709 12584 layer_factory.hpp:77] Creating layer relu1
I0205 06:12:44.543720 12584 net.cpp:106] Creating Layer relu1
I0205 06:12:44.543727 12584 net.cpp:454] relu1 <- conv1
I0205 06:12:44.543736 12584 net.cpp:397] relu1 -> conv1 (in-place)
I0205 06:12:44.543745 12584 net.cpp:150] Setting up relu1
I0205 06:12:44.543752 12584 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 06:12:44.543757 12584 net.cpp:165] Memory required for data: 39972800
I0205 06:12:44.543763 12584 layer_factory.hpp:77] Creating layer pool1
I0205 06:12:44.543774 12584 net.cpp:106] Creating Layer pool1
I0205 06:12:44.543779 12584 net.cpp:454] pool1 <- conv1
I0205 06:12:44.543788 12584 net.cpp:411] pool1 -> pool1
I0205 06:12:44.543802 12584 net.cpp:150] Setting up pool1
I0205 06:12:44.543809 12584 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:12:44.543815 12584 net.cpp:165] Memory required for data: 42305600
I0205 06:12:44.543820 12584 layer_factory.hpp:77] Creating layer norm1
I0205 06:12:44.543831 12584 net.cpp:106] Creating Layer norm1
I0205 06:12:44.543838 12584 net.cpp:454] norm1 <- pool1
I0205 06:12:44.543845 12584 net.cpp:411] norm1 -> norm1
I0205 06:12:44.543855 12584 net.cpp:150] Setting up norm1
I0205 06:12:44.543862 12584 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:12:44.543869 12584 net.cpp:165] Memory required for data: 44638400
I0205 06:12:44.543876 12584 layer_factory.hpp:77] Creating layer conv2
I0205 06:12:44.543886 12584 net.cpp:106] Creating Layer conv2
I0205 06:12:44.543892 12584 net.cpp:454] conv2 <- norm1
I0205 06:12:44.543901 12584 net.cpp:411] conv2 -> conv2
I0205 06:12:44.543931 12584 net.cpp:150] Setting up conv2
I0205 06:12:44.543941 12584 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:12:44.543946 12584 net.cpp:165] Memory required for data: 46971200
I0205 06:12:44.543956 12584 layer_factory.hpp:77] Creating layer relu2
I0205 06:12:44.543970 12584 net.cpp:106] Creating Layer relu2
I0205 06:12:44.543977 12584 net.cpp:454] relu2 <- conv2
I0205 06:12:44.543984 12584 net.cpp:397] relu2 -> conv2 (in-place)
I0205 06:12:44.544005 12584 net.cpp:150] Setting up relu2
I0205 06:12:44.544021 12584 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:12:44.544028 12584 net.cpp:165] Memory required for data: 49304000
I0205 06:12:44.544036 12584 layer_factory.hpp:77] Creating layer pool2
I0205 06:12:44.544046 12584 net.cpp:106] Creating Layer pool2
I0205 06:12:44.544052 12584 net.cpp:454] pool2 <- conv2
I0205 06:12:44.544059 12584 net.cpp:411] pool2 -> pool2
I0205 06:12:44.544072 12584 net.cpp:150] Setting up pool2
I0205 06:12:44.544080 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.544085 12584 net.cpp:165] Memory required for data: 49844800
I0205 06:12:44.544091 12584 layer_factory.hpp:77] Creating layer norm2
I0205 06:12:44.544100 12584 net.cpp:106] Creating Layer norm2
I0205 06:12:44.544106 12584 net.cpp:454] norm2 <- pool2
I0205 06:12:44.544114 12584 net.cpp:411] norm2 -> norm2
I0205 06:12:44.544123 12584 net.cpp:150] Setting up norm2
I0205 06:12:44.544131 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.544136 12584 net.cpp:165] Memory required for data: 50385600
I0205 06:12:44.544140 12584 layer_factory.hpp:77] Creating layer conv3
I0205 06:12:44.544152 12584 net.cpp:106] Creating Layer conv3
I0205 06:12:44.544157 12584 net.cpp:454] conv3 <- norm2
I0205 06:12:44.544164 12584 net.cpp:411] conv3 -> conv3
I0205 06:12:44.544193 12584 net.cpp:150] Setting up conv3
I0205 06:12:44.544201 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.544206 12584 net.cpp:165] Memory required for data: 50926400
I0205 06:12:44.544216 12584 layer_factory.hpp:77] Creating layer relu3
I0205 06:12:44.544225 12584 net.cpp:106] Creating Layer relu3
I0205 06:12:44.544231 12584 net.cpp:454] relu3 <- conv3
I0205 06:12:44.544241 12584 net.cpp:397] relu3 -> conv3 (in-place)
I0205 06:12:44.544250 12584 net.cpp:150] Setting up relu3
I0205 06:12:44.544257 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.544262 12584 net.cpp:165] Memory required for data: 51467200
I0205 06:12:44.544267 12584 layer_factory.hpp:77] Creating layer conv4
I0205 06:12:44.544277 12584 net.cpp:106] Creating Layer conv4
I0205 06:12:44.544282 12584 net.cpp:454] conv4 <- conv3
I0205 06:12:44.544291 12584 net.cpp:411] conv4 -> conv4
I0205 06:12:44.544311 12584 net.cpp:150] Setting up conv4
I0205 06:12:44.544318 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.544323 12584 net.cpp:165] Memory required for data: 52008000
I0205 06:12:44.544332 12584 layer_factory.hpp:77] Creating layer relu4
I0205 06:12:44.544340 12584 net.cpp:106] Creating Layer relu4
I0205 06:12:44.544345 12584 net.cpp:454] relu4 <- conv4
I0205 06:12:44.544353 12584 net.cpp:397] relu4 -> conv4 (in-place)
I0205 06:12:44.544360 12584 net.cpp:150] Setting up relu4
I0205 06:12:44.544369 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.544374 12584 net.cpp:165] Memory required for data: 52548800
I0205 06:12:44.544380 12584 layer_factory.hpp:77] Creating layer conv5
I0205 06:12:44.544390 12584 net.cpp:106] Creating Layer conv5
I0205 06:12:44.544395 12584 net.cpp:454] conv5 <- conv4
I0205 06:12:44.544404 12584 net.cpp:411] conv5 -> conv5
I0205 06:12:44.544427 12584 net.cpp:150] Setting up conv5
I0205 06:12:44.544435 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.544440 12584 net.cpp:165] Memory required for data: 53089600
I0205 06:12:44.544450 12584 layer_factory.hpp:77] Creating layer relu5
I0205 06:12:44.544457 12584 net.cpp:106] Creating Layer relu5
I0205 06:12:44.544463 12584 net.cpp:454] relu5 <- conv5
I0205 06:12:44.544471 12584 net.cpp:397] relu5 -> conv5 (in-place)
I0205 06:12:44.544478 12584 net.cpp:150] Setting up relu5
I0205 06:12:44.544484 12584 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:12:44.544489 12584 net.cpp:165] Memory required for data: 53630400
I0205 06:12:44.544494 12584 layer_factory.hpp:77] Creating layer pool5
I0205 06:12:44.544504 12584 net.cpp:106] Creating Layer pool5
I0205 06:12:44.544510 12584 net.cpp:454] pool5 <- conv5
I0205 06:12:44.544518 12584 net.cpp:411] pool5 -> pool5
I0205 06:12:44.544535 12584 net.cpp:150] Setting up pool5
I0205 06:12:44.544550 12584 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0205 06:12:44.544555 12584 net.cpp:165] Memory required for data: 53745600
I0205 06:12:44.544560 12584 layer_factory.hpp:77] Creating layer fc6
I0205 06:12:44.544571 12584 net.cpp:106] Creating Layer fc6
I0205 06:12:44.544577 12584 net.cpp:454] fc6 <- pool5
I0205 06:12:44.544587 12584 net.cpp:411] fc6 -> fc6
I0205 06:12:44.545328 12584 net.cpp:150] Setting up fc6
I0205 06:12:44.545341 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.545347 12584 net.cpp:165] Memory required for data: 53848000
I0205 06:12:44.545356 12584 layer_factory.hpp:77] Creating layer relu6
I0205 06:12:44.545367 12584 net.cpp:106] Creating Layer relu6
I0205 06:12:44.545373 12584 net.cpp:454] relu6 <- fc6
I0205 06:12:44.545383 12584 net.cpp:397] relu6 -> fc6 (in-place)
I0205 06:12:44.545392 12584 net.cpp:150] Setting up relu6
I0205 06:12:44.545398 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.545403 12584 net.cpp:165] Memory required for data: 53950400
I0205 06:12:44.545409 12584 layer_factory.hpp:77] Creating layer drop6
I0205 06:12:44.545418 12584 net.cpp:106] Creating Layer drop6
I0205 06:12:44.545424 12584 net.cpp:454] drop6 <- fc6
I0205 06:12:44.545433 12584 net.cpp:397] drop6 -> fc6 (in-place)
I0205 06:12:44.545444 12584 net.cpp:150] Setting up drop6
I0205 06:12:44.545450 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.545455 12584 net.cpp:165] Memory required for data: 54052800
I0205 06:12:44.545460 12584 layer_factory.hpp:77] Creating layer fc7
I0205 06:12:44.545474 12584 net.cpp:106] Creating Layer fc7
I0205 06:12:44.545480 12584 net.cpp:454] fc7 <- fc6
I0205 06:12:44.545490 12584 net.cpp:411] fc7 -> fc7
I0205 06:12:44.546221 12584 net.cpp:150] Setting up fc7
I0205 06:12:44.546232 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.546238 12584 net.cpp:165] Memory required for data: 54155200
I0205 06:12:44.546248 12584 layer_factory.hpp:77] Creating layer relu7
I0205 06:12:44.546257 12584 net.cpp:106] Creating Layer relu7
I0205 06:12:44.546262 12584 net.cpp:454] relu7 <- fc7
I0205 06:12:44.546272 12584 net.cpp:397] relu7 -> fc7 (in-place)
I0205 06:12:44.546280 12584 net.cpp:150] Setting up relu7
I0205 06:12:44.546288 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.546293 12584 net.cpp:165] Memory required for data: 54257600
I0205 06:12:44.546298 12584 layer_factory.hpp:77] Creating layer drop7
I0205 06:12:44.546305 12584 net.cpp:106] Creating Layer drop7
I0205 06:12:44.546311 12584 net.cpp:454] drop7 <- fc7
I0205 06:12:44.546319 12584 net.cpp:397] drop7 -> fc7 (in-place)
I0205 06:12:44.546327 12584 net.cpp:150] Setting up drop7
I0205 06:12:44.546334 12584 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:12:44.546340 12584 net.cpp:165] Memory required for data: 54360000
I0205 06:12:44.546345 12584 layer_factory.hpp:77] Creating layer fc8
I0205 06:12:44.546357 12584 net.cpp:106] Creating Layer fc8
I0205 06:12:44.546363 12584 net.cpp:454] fc8 <- fc7
I0205 06:12:44.546373 12584 net.cpp:411] fc8 -> fc8
I0205 06:12:44.546397 12584 net.cpp:150] Setting up fc8
I0205 06:12:44.546409 12584 net.cpp:157] Top shape: 100 2 (200)
I0205 06:12:44.546414 12584 net.cpp:165] Memory required for data: 54360800
I0205 06:12:44.546422 12584 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0205 06:12:44.546432 12584 net.cpp:106] Creating Layer fc8_fc8_0_split
I0205 06:12:44.546437 12584 net.cpp:454] fc8_fc8_0_split <- fc8
I0205 06:12:44.546443 12584 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0205 06:12:44.546452 12584 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0205 06:12:44.546460 12584 net.cpp:150] Setting up fc8_fc8_0_split
I0205 06:12:44.546468 12584 net.cpp:157] Top shape: 100 2 (200)
I0205 06:12:44.546473 12584 net.cpp:157] Top shape: 100 2 (200)
I0205 06:12:44.546478 12584 net.cpp:165] Memory required for data: 54362400
I0205 06:12:44.546485 12584 layer_factory.hpp:77] Creating layer accuracy
I0205 06:12:44.546500 12584 net.cpp:106] Creating Layer accuracy
I0205 06:12:44.546510 12584 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0205 06:12:44.546525 12584 net.cpp:454] accuracy <- label_data_1_split_0
I0205 06:12:44.546535 12584 net.cpp:411] accuracy -> accuracy
I0205 06:12:44.546546 12584 net.cpp:150] Setting up accuracy
I0205 06:12:44.546553 12584 net.cpp:157] Top shape: (1)
I0205 06:12:44.546561 12584 net.cpp:165] Memory required for data: 54362404
I0205 06:12:44.546566 12584 layer_factory.hpp:77] Creating layer loss
I0205 06:12:44.546574 12584 net.cpp:106] Creating Layer loss
I0205 06:12:44.546581 12584 net.cpp:454] loss <- fc8_fc8_0_split_1
I0205 06:12:44.546586 12584 net.cpp:454] loss <- label_data_1_split_1
I0205 06:12:44.546594 12584 net.cpp:411] loss -> loss
I0205 06:12:44.546604 12584 layer_factory.hpp:77] Creating layer loss
I0205 06:12:44.546627 12584 net.cpp:150] Setting up loss
I0205 06:12:44.546634 12584 net.cpp:157] Top shape: (1)
I0205 06:12:44.546639 12584 net.cpp:160]     with loss weight 1
I0205 06:12:44.546653 12584 net.cpp:165] Memory required for data: 54362408
I0205 06:12:44.546663 12584 net.cpp:226] loss needs backward computation.
I0205 06:12:44.546669 12584 net.cpp:228] accuracy does not need backward computation.
I0205 06:12:44.546675 12584 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0205 06:12:44.546681 12584 net.cpp:226] fc8 needs backward computation.
I0205 06:12:44.546687 12584 net.cpp:226] drop7 needs backward computation.
I0205 06:12:44.546692 12584 net.cpp:226] relu7 needs backward computation.
I0205 06:12:44.546697 12584 net.cpp:226] fc7 needs backward computation.
I0205 06:12:44.546702 12584 net.cpp:226] drop6 needs backward computation.
I0205 06:12:44.546708 12584 net.cpp:226] relu6 needs backward computation.
I0205 06:12:44.546713 12584 net.cpp:226] fc6 needs backward computation.
I0205 06:12:44.546718 12584 net.cpp:226] pool5 needs backward computation.
I0205 06:12:44.546725 12584 net.cpp:226] relu5 needs backward computation.
I0205 06:12:44.546732 12584 net.cpp:226] conv5 needs backward computation.
I0205 06:12:44.546738 12584 net.cpp:226] relu4 needs backward computation.
I0205 06:12:44.546743 12584 net.cpp:226] conv4 needs backward computation.
I0205 06:12:44.546749 12584 net.cpp:226] relu3 needs backward computation.
I0205 06:12:44.546754 12584 net.cpp:226] conv3 needs backward computation.
I0205 06:12:44.546761 12584 net.cpp:226] norm2 needs backward computation.
I0205 06:12:44.546766 12584 net.cpp:226] pool2 needs backward computation.
I0205 06:12:44.546772 12584 net.cpp:226] relu2 needs backward computation.
I0205 06:12:44.546777 12584 net.cpp:226] conv2 needs backward computation.
I0205 06:12:44.546782 12584 net.cpp:226] norm1 needs backward computation.
I0205 06:12:44.546787 12584 net.cpp:226] pool1 needs backward computation.
I0205 06:12:44.546793 12584 net.cpp:226] relu1 needs backward computation.
I0205 06:12:44.546798 12584 net.cpp:226] conv1 needs backward computation.
I0205 06:12:44.546805 12584 net.cpp:228] label_data_1_split does not need backward computation.
I0205 06:12:44.546811 12584 net.cpp:228] data does not need backward computation.
I0205 06:12:44.546818 12584 net.cpp:270] This network produces output accuracy
I0205 06:12:44.546824 12584 net.cpp:270] This network produces output loss
I0205 06:12:44.546854 12584 net.cpp:283] Network initialization done.
I0205 06:12:44.546960 12584 solver.cpp:60] Solver scaffolding done.
I0205 06:12:44.547049 12584 caffe.cpp:212] Starting Optimization
I0205 06:12:44.547058 12584 solver.cpp:288] Solving CaffeNet
I0205 06:12:44.547065 12584 solver.cpp:289] Learning Rate Policy: step
I0205 06:12:44.547549 12584 solver.cpp:341] Iteration 0, Testing net (#0)
I0205 06:12:44.547613 12584 blocking_queue.cpp:50] Data layer prefetch queue empty
I0205 06:12:47.033162 12584 solver.cpp:409]     Test net output #0: accuracy = 0.496
I0205 06:12:47.033216 12584 solver.cpp:409]     Test net output #1: loss = 3.77563 (* 1 = 3.77563 loss)
I0205 06:12:47.561034 12584 solver.cpp:237] Iteration 0, loss = 8.16621
I0205 06:12:47.561082 12584 solver.cpp:253]     Train net output #0: loss = 8.16621 (* 1 = 8.16621 loss)
I0205 06:12:47.561105 12584 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0205 06:12:52.417449 12584 solver.cpp:237] Iteration 10, loss = 1.50316
I0205 06:12:52.417503 12584 solver.cpp:253]     Train net output #0: loss = 1.50316 (* 1 = 1.50316 loss)
I0205 06:12:52.417515 12584 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0205 06:12:57.268642 12584 solver.cpp:237] Iteration 20, loss = 0.928402
I0205 06:12:57.268695 12584 solver.cpp:253]     Train net output #0: loss = 0.928402 (* 1 = 0.928402 loss)
I0205 06:12:57.268707 12584 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0205 06:13:02.132728 12584 solver.cpp:237] Iteration 30, loss = 0.929248
I0205 06:13:02.132778 12584 solver.cpp:253]     Train net output #0: loss = 0.929248 (* 1 = 0.929248 loss)
I0205 06:13:02.132789 12584 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0205 06:13:07.003878 12584 solver.cpp:237] Iteration 40, loss = 0.96963
I0205 06:13:07.003932 12584 solver.cpp:253]     Train net output #0: loss = 0.96963 (* 1 = 0.96963 loss)
I0205 06:13:07.003943 12584 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0205 06:13:11.877063 12584 solver.cpp:237] Iteration 50, loss = 0.803035
I0205 06:13:11.877113 12584 solver.cpp:253]     Train net output #0: loss = 0.803035 (* 1 = 0.803035 loss)
I0205 06:13:11.877125 12584 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0205 06:13:16.745944 12584 solver.cpp:237] Iteration 60, loss = 0.86636
I0205 06:13:16.746064 12584 solver.cpp:253]     Train net output #0: loss = 0.86636 (* 1 = 0.86636 loss)
I0205 06:13:16.746076 12584 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0205 06:13:21.615761 12584 solver.cpp:237] Iteration 70, loss = 0.749966
I0205 06:13:21.615818 12584 solver.cpp:253]     Train net output #0: loss = 0.749966 (* 1 = 0.749966 loss)
I0205 06:13:21.615829 12584 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0205 06:13:26.487251 12584 solver.cpp:237] Iteration 80, loss = 0.777512
I0205 06:13:26.487301 12584 solver.cpp:253]     Train net output #0: loss = 0.777512 (* 1 = 0.777512 loss)
I0205 06:13:26.487313 12584 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0205 06:13:31.359359 12584 solver.cpp:237] Iteration 90, loss = 0.742554
I0205 06:13:31.359412 12584 solver.cpp:253]     Train net output #0: loss = 0.742554 (* 1 = 0.742554 loss)
I0205 06:13:31.359423 12584 sgd_solver.cpp:106] Iteration 90, lr = 0.001
I0205 06:13:35.744130 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_100.caffemodel
I0205 06:13:35.746868 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_100.solverstate
I0205 06:13:35.747673 12584 solver.cpp:341] Iteration 100, Testing net (#0)
I0205 06:13:38.092340 12584 solver.cpp:409]     Test net output #0: accuracy = 0.505
I0205 06:13:38.092388 12584 solver.cpp:409]     Test net output #1: loss = 0.692693 (* 1 = 0.692693 loss)
I0205 06:13:38.579509 12584 solver.cpp:237] Iteration 100, loss = 0.771415
I0205 06:13:38.579558 12584 solver.cpp:253]     Train net output #0: loss = 0.771415 (* 1 = 0.771415 loss)
I0205 06:13:38.579569 12584 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0205 06:13:43.450561 12584 solver.cpp:237] Iteration 110, loss = 0.729076
I0205 06:13:43.450614 12584 solver.cpp:253]     Train net output #0: loss = 0.729076 (* 1 = 0.729076 loss)
I0205 06:13:43.450625 12584 sgd_solver.cpp:106] Iteration 110, lr = 0.001
I0205 06:13:48.320281 12584 solver.cpp:237] Iteration 120, loss = 0.735015
I0205 06:13:48.320459 12584 solver.cpp:253]     Train net output #0: loss = 0.735015 (* 1 = 0.735015 loss)
I0205 06:13:48.320472 12584 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0205 06:13:53.188838 12584 solver.cpp:237] Iteration 130, loss = 0.74753
I0205 06:13:53.188896 12584 solver.cpp:253]     Train net output #0: loss = 0.74753 (* 1 = 0.74753 loss)
I0205 06:13:53.188906 12584 sgd_solver.cpp:106] Iteration 130, lr = 0.001
I0205 06:13:58.059393 12584 solver.cpp:237] Iteration 140, loss = 0.780024
I0205 06:13:58.059443 12584 solver.cpp:253]     Train net output #0: loss = 0.780024 (* 1 = 0.780024 loss)
I0205 06:13:58.059454 12584 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0205 06:14:02.934061 12584 solver.cpp:237] Iteration 150, loss = 0.778617
I0205 06:14:02.934110 12584 solver.cpp:253]     Train net output #0: loss = 0.778617 (* 1 = 0.778617 loss)
I0205 06:14:02.934120 12584 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0205 06:14:07.804832 12584 solver.cpp:237] Iteration 160, loss = 0.734196
I0205 06:14:07.804884 12584 solver.cpp:253]     Train net output #0: loss = 0.734196 (* 1 = 0.734196 loss)
I0205 06:14:07.804895 12584 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0205 06:14:12.677585 12584 solver.cpp:237] Iteration 170, loss = 0.779257
I0205 06:14:12.677634 12584 solver.cpp:253]     Train net output #0: loss = 0.779257 (* 1 = 0.779257 loss)
I0205 06:14:12.677645 12584 sgd_solver.cpp:106] Iteration 170, lr = 0.001
I0205 06:14:17.547791 12584 solver.cpp:237] Iteration 180, loss = 0.750292
I0205 06:14:17.547842 12584 solver.cpp:253]     Train net output #0: loss = 0.750292 (* 1 = 0.750292 loss)
I0205 06:14:17.547852 12584 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0205 06:14:22.418925 12584 solver.cpp:237] Iteration 190, loss = 0.747069
I0205 06:14:22.419152 12584 solver.cpp:253]     Train net output #0: loss = 0.747069 (* 1 = 0.747069 loss)
I0205 06:14:22.419165 12584 sgd_solver.cpp:106] Iteration 190, lr = 0.001
I0205 06:14:26.806687 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_200.caffemodel
I0205 06:14:26.808678 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_200.solverstate
I0205 06:14:26.809471 12584 solver.cpp:341] Iteration 200, Testing net (#0)
I0205 06:14:29.152814 12584 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 06:14:29.152866 12584 solver.cpp:409]     Test net output #1: loss = 0.694336 (* 1 = 0.694336 loss)
I0205 06:14:29.639938 12584 solver.cpp:237] Iteration 200, loss = 0.716676
I0205 06:14:29.639992 12584 solver.cpp:253]     Train net output #0: loss = 0.716676 (* 1 = 0.716676 loss)
I0205 06:14:29.640004 12584 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0205 06:14:34.509855 12584 solver.cpp:237] Iteration 210, loss = 0.71808
I0205 06:14:34.509905 12584 solver.cpp:253]     Train net output #0: loss = 0.71808 (* 1 = 0.71808 loss)
I0205 06:14:34.509917 12584 sgd_solver.cpp:106] Iteration 210, lr = 0.001
I0205 06:14:39.381003 12584 solver.cpp:237] Iteration 220, loss = 0.78785
I0205 06:14:39.381055 12584 solver.cpp:253]     Train net output #0: loss = 0.78785 (* 1 = 0.78785 loss)
I0205 06:14:39.381065 12584 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0205 06:14:44.255856 12584 solver.cpp:237] Iteration 230, loss = 0.712788
I0205 06:14:44.255908 12584 solver.cpp:253]     Train net output #0: loss = 0.712788 (* 1 = 0.712788 loss)
I0205 06:14:44.255919 12584 sgd_solver.cpp:106] Iteration 230, lr = 0.001
I0205 06:14:49.129766 12584 solver.cpp:237] Iteration 240, loss = 0.719832
I0205 06:14:49.129824 12584 solver.cpp:253]     Train net output #0: loss = 0.719832 (* 1 = 0.719832 loss)
I0205 06:14:49.129837 12584 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0205 06:14:54.006587 12584 solver.cpp:237] Iteration 250, loss = 0.743028
I0205 06:14:54.006780 12584 solver.cpp:253]     Train net output #0: loss = 0.743028 (* 1 = 0.743028 loss)
I0205 06:14:54.006794 12584 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0205 06:14:58.876868 12584 solver.cpp:237] Iteration 260, loss = 0.68787
I0205 06:14:58.876922 12584 solver.cpp:253]     Train net output #0: loss = 0.68787 (* 1 = 0.68787 loss)
I0205 06:14:58.876934 12584 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0205 06:15:03.753168 12584 solver.cpp:237] Iteration 270, loss = 0.735112
I0205 06:15:03.753232 12584 solver.cpp:253]     Train net output #0: loss = 0.735112 (* 1 = 0.735112 loss)
I0205 06:15:03.753242 12584 sgd_solver.cpp:106] Iteration 270, lr = 0.001
I0205 06:15:08.625738 12584 solver.cpp:237] Iteration 280, loss = 0.69813
I0205 06:15:08.625795 12584 solver.cpp:253]     Train net output #0: loss = 0.69813 (* 1 = 0.69813 loss)
I0205 06:15:08.625807 12584 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0205 06:15:13.505722 12584 solver.cpp:237] Iteration 290, loss = 0.718094
I0205 06:15:13.505771 12584 solver.cpp:253]     Train net output #0: loss = 0.718094 (* 1 = 0.718094 loss)
I0205 06:15:13.505782 12584 sgd_solver.cpp:106] Iteration 290, lr = 0.001
I0205 06:15:17.901789 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_300.caffemodel
I0205 06:15:17.904259 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_300.solverstate
I0205 06:15:17.905040 12584 solver.cpp:341] Iteration 300, Testing net (#0)
I0205 06:15:20.242053 12584 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 06:15:20.242100 12584 solver.cpp:409]     Test net output #1: loss = 0.709646 (* 1 = 0.709646 loss)
I0205 06:15:20.728930 12584 solver.cpp:237] Iteration 300, loss = 0.772908
I0205 06:15:20.728983 12584 solver.cpp:253]     Train net output #0: loss = 0.772908 (* 1 = 0.772908 loss)
I0205 06:15:20.728996 12584 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0205 06:15:25.600070 12584 solver.cpp:237] Iteration 310, loss = 0.698162
I0205 06:15:25.600270 12584 solver.cpp:253]     Train net output #0: loss = 0.698162 (* 1 = 0.698162 loss)
I0205 06:15:25.600285 12584 sgd_solver.cpp:106] Iteration 310, lr = 0.001
I0205 06:15:30.472367 12584 solver.cpp:237] Iteration 320, loss = 0.681967
I0205 06:15:30.472414 12584 solver.cpp:253]     Train net output #0: loss = 0.681967 (* 1 = 0.681967 loss)
I0205 06:15:30.472425 12584 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0205 06:15:35.345909 12584 solver.cpp:237] Iteration 330, loss = 0.727161
I0205 06:15:35.345969 12584 solver.cpp:253]     Train net output #0: loss = 0.727161 (* 1 = 0.727161 loss)
I0205 06:15:35.345981 12584 sgd_solver.cpp:106] Iteration 330, lr = 0.001
I0205 06:15:40.218109 12584 solver.cpp:237] Iteration 340, loss = 0.683049
I0205 06:15:40.218158 12584 solver.cpp:253]     Train net output #0: loss = 0.683049 (* 1 = 0.683049 loss)
I0205 06:15:40.218169 12584 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0205 06:15:45.088510 12584 solver.cpp:237] Iteration 350, loss = 0.731268
I0205 06:15:45.088565 12584 solver.cpp:253]     Train net output #0: loss = 0.731268 (* 1 = 0.731268 loss)
I0205 06:15:45.088577 12584 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I0205 06:15:49.970177 12584 solver.cpp:237] Iteration 360, loss = 0.690552
I0205 06:15:49.970233 12584 solver.cpp:253]     Train net output #0: loss = 0.690552 (* 1 = 0.690552 loss)
I0205 06:15:49.970245 12584 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0205 06:15:54.857831 12584 solver.cpp:237] Iteration 370, loss = 0.717694
I0205 06:15:54.857879 12584 solver.cpp:253]     Train net output #0: loss = 0.717694 (* 1 = 0.717694 loss)
I0205 06:15:54.857890 12584 sgd_solver.cpp:106] Iteration 370, lr = 0.001
I0205 06:15:59.729720 12584 solver.cpp:237] Iteration 380, loss = 0.761305
I0205 06:15:59.729915 12584 solver.cpp:253]     Train net output #0: loss = 0.761305 (* 1 = 0.761305 loss)
I0205 06:15:59.729928 12584 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0205 06:16:04.603514 12584 solver.cpp:237] Iteration 390, loss = 0.71038
I0205 06:16:04.603569 12584 solver.cpp:253]     Train net output #0: loss = 0.71038 (* 1 = 0.71038 loss)
I0205 06:16:04.603580 12584 sgd_solver.cpp:106] Iteration 390, lr = 0.001
I0205 06:16:08.989632 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_400.caffemodel
I0205 06:16:08.991644 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_400.solverstate
I0205 06:16:08.992462 12584 solver.cpp:341] Iteration 400, Testing net (#0)
I0205 06:16:11.336766 12584 solver.cpp:409]     Test net output #0: accuracy = 0.505
I0205 06:16:11.336812 12584 solver.cpp:409]     Test net output #1: loss = 0.693529 (* 1 = 0.693529 loss)
I0205 06:16:11.823168 12584 solver.cpp:237] Iteration 400, loss = 0.678039
I0205 06:16:11.823213 12584 solver.cpp:253]     Train net output #0: loss = 0.678039 (* 1 = 0.678039 loss)
I0205 06:16:11.823223 12584 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0205 06:16:16.692137 12584 solver.cpp:237] Iteration 410, loss = 0.749089
I0205 06:16:16.692190 12584 solver.cpp:253]     Train net output #0: loss = 0.749089 (* 1 = 0.749089 loss)
I0205 06:16:16.692200 12584 sgd_solver.cpp:106] Iteration 410, lr = 0.001
I0205 06:16:21.559510 12584 solver.cpp:237] Iteration 420, loss = 0.704644
I0205 06:16:21.559562 12584 solver.cpp:253]     Train net output #0: loss = 0.704644 (* 1 = 0.704644 loss)
I0205 06:16:21.559574 12584 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0205 06:16:26.427634 12584 solver.cpp:237] Iteration 430, loss = 0.733018
I0205 06:16:26.427685 12584 solver.cpp:253]     Train net output #0: loss = 0.733018 (* 1 = 0.733018 loss)
I0205 06:16:26.427695 12584 sgd_solver.cpp:106] Iteration 430, lr = 0.001
I0205 06:16:31.294941 12584 solver.cpp:237] Iteration 440, loss = 0.702398
I0205 06:16:31.295173 12584 solver.cpp:253]     Train net output #0: loss = 0.702398 (* 1 = 0.702398 loss)
I0205 06:16:31.295187 12584 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0205 06:16:36.165014 12584 solver.cpp:237] Iteration 450, loss = 0.71482
I0205 06:16:36.165066 12584 solver.cpp:253]     Train net output #0: loss = 0.71482 (* 1 = 0.71482 loss)
I0205 06:16:36.165077 12584 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I0205 06:16:41.038450 12584 solver.cpp:237] Iteration 460, loss = 0.754425
I0205 06:16:41.038506 12584 solver.cpp:253]     Train net output #0: loss = 0.754425 (* 1 = 0.754425 loss)
I0205 06:16:41.038517 12584 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0205 06:16:45.910186 12584 solver.cpp:237] Iteration 470, loss = 0.692052
I0205 06:16:45.910239 12584 solver.cpp:253]     Train net output #0: loss = 0.692052 (* 1 = 0.692052 loss)
I0205 06:16:45.910250 12584 sgd_solver.cpp:106] Iteration 470, lr = 0.001
I0205 06:16:50.779542 12584 solver.cpp:237] Iteration 480, loss = 0.664498
I0205 06:16:50.779595 12584 solver.cpp:253]     Train net output #0: loss = 0.664498 (* 1 = 0.664498 loss)
I0205 06:16:50.779606 12584 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0205 06:16:55.646658 12584 solver.cpp:237] Iteration 490, loss = 0.77572
I0205 06:16:55.646710 12584 solver.cpp:253]     Train net output #0: loss = 0.77572 (* 1 = 0.77572 loss)
I0205 06:16:55.646721 12584 sgd_solver.cpp:106] Iteration 490, lr = 0.001
I0205 06:17:00.035056 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_500.caffemodel
I0205 06:17:00.037050 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_500.solverstate
I0205 06:17:00.037936 12584 solver.cpp:341] Iteration 500, Testing net (#0)
I0205 06:17:02.380280 12584 solver.cpp:409]     Test net output #0: accuracy = 0.547
I0205 06:17:02.380463 12584 solver.cpp:409]     Test net output #1: loss = 0.679206 (* 1 = 0.679206 loss)
I0205 06:17:02.868993 12584 solver.cpp:237] Iteration 500, loss = 0.687288
I0205 06:17:02.869042 12584 solver.cpp:253]     Train net output #0: loss = 0.687288 (* 1 = 0.687288 loss)
I0205 06:17:02.869053 12584 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0205 06:17:07.744791 12584 solver.cpp:237] Iteration 510, loss = 0.712469
I0205 06:17:07.744843 12584 solver.cpp:253]     Train net output #0: loss = 0.712469 (* 1 = 0.712469 loss)
I0205 06:17:07.744853 12584 sgd_solver.cpp:106] Iteration 510, lr = 0.001
I0205 06:17:12.611618 12584 solver.cpp:237] Iteration 520, loss = 0.696563
I0205 06:17:12.611666 12584 solver.cpp:253]     Train net output #0: loss = 0.696563 (* 1 = 0.696563 loss)
I0205 06:17:12.611677 12584 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I0205 06:17:17.479003 12584 solver.cpp:237] Iteration 530, loss = 0.694249
I0205 06:17:17.479058 12584 solver.cpp:253]     Train net output #0: loss = 0.694249 (* 1 = 0.694249 loss)
I0205 06:17:17.479070 12584 sgd_solver.cpp:106] Iteration 530, lr = 0.001
I0205 06:17:22.349010 12584 solver.cpp:237] Iteration 540, loss = 0.702415
I0205 06:17:22.349058 12584 solver.cpp:253]     Train net output #0: loss = 0.702415 (* 1 = 0.702415 loss)
I0205 06:17:22.349068 12584 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I0205 06:17:27.214092 12584 solver.cpp:237] Iteration 550, loss = 0.665023
I0205 06:17:27.214140 12584 solver.cpp:253]     Train net output #0: loss = 0.665023 (* 1 = 0.665023 loss)
I0205 06:17:27.214151 12584 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I0205 06:17:32.080341 12584 solver.cpp:237] Iteration 560, loss = 0.671203
I0205 06:17:32.080394 12584 solver.cpp:253]     Train net output #0: loss = 0.671203 (* 1 = 0.671203 loss)
I0205 06:17:32.080405 12584 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I0205 06:17:36.943414 12584 solver.cpp:237] Iteration 570, loss = 0.678551
I0205 06:17:36.943622 12584 solver.cpp:253]     Train net output #0: loss = 0.678551 (* 1 = 0.678551 loss)
I0205 06:17:36.943636 12584 sgd_solver.cpp:106] Iteration 570, lr = 0.001
I0205 06:17:41.808910 12584 solver.cpp:237] Iteration 580, loss = 0.635253
I0205 06:17:41.808971 12584 solver.cpp:253]     Train net output #0: loss = 0.635253 (* 1 = 0.635253 loss)
I0205 06:17:41.808984 12584 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I0205 06:17:46.674721 12584 solver.cpp:237] Iteration 590, loss = 0.633926
I0205 06:17:46.674777 12584 solver.cpp:253]     Train net output #0: loss = 0.633926 (* 1 = 0.633926 loss)
I0205 06:17:46.674787 12584 sgd_solver.cpp:106] Iteration 590, lr = 0.001
I0205 06:17:51.053360 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_600.caffemodel
I0205 06:17:51.055352 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_600.solverstate
I0205 06:17:51.056143 12584 solver.cpp:341] Iteration 600, Testing net (#0)
I0205 06:17:53.396083 12584 solver.cpp:409]     Test net output #0: accuracy = 0.636
I0205 06:17:53.396134 12584 solver.cpp:409]     Test net output #1: loss = 0.611048 (* 1 = 0.611048 loss)
I0205 06:17:53.882349 12584 solver.cpp:237] Iteration 600, loss = 0.649754
I0205 06:17:53.882396 12584 solver.cpp:253]     Train net output #0: loss = 0.649754 (* 1 = 0.649754 loss)
I0205 06:17:53.882408 12584 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0205 06:17:58.754988 12584 solver.cpp:237] Iteration 610, loss = 0.603655
I0205 06:17:58.755044 12584 solver.cpp:253]     Train net output #0: loss = 0.603655 (* 1 = 0.603655 loss)
I0205 06:17:58.755056 12584 sgd_solver.cpp:106] Iteration 610, lr = 0.001
I0205 06:18:03.614087 12584 solver.cpp:237] Iteration 620, loss = 0.535837
I0205 06:18:03.614137 12584 solver.cpp:253]     Train net output #0: loss = 0.535837 (* 1 = 0.535837 loss)
I0205 06:18:03.614150 12584 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I0205 06:18:08.465555 12584 solver.cpp:237] Iteration 630, loss = 0.516525
I0205 06:18:08.465739 12584 solver.cpp:253]     Train net output #0: loss = 0.516525 (* 1 = 0.516525 loss)
I0205 06:18:08.465751 12584 sgd_solver.cpp:106] Iteration 630, lr = 0.001
I0205 06:18:13.308576 12584 solver.cpp:237] Iteration 640, loss = 0.464245
I0205 06:18:13.308641 12584 solver.cpp:253]     Train net output #0: loss = 0.464245 (* 1 = 0.464245 loss)
I0205 06:18:13.308652 12584 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I0205 06:18:18.150390 12584 solver.cpp:237] Iteration 650, loss = 0.511427
I0205 06:18:18.150444 12584 solver.cpp:253]     Train net output #0: loss = 0.511427 (* 1 = 0.511427 loss)
I0205 06:18:18.150454 12584 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I0205 06:18:22.988723 12584 solver.cpp:237] Iteration 660, loss = 0.431526
I0205 06:18:22.988772 12584 solver.cpp:253]     Train net output #0: loss = 0.431526 (* 1 = 0.431526 loss)
I0205 06:18:22.988783 12584 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I0205 06:18:27.830394 12584 solver.cpp:237] Iteration 670, loss = 0.414306
I0205 06:18:27.830445 12584 solver.cpp:253]     Train net output #0: loss = 0.414306 (* 1 = 0.414306 loss)
I0205 06:18:27.830454 12584 sgd_solver.cpp:106] Iteration 670, lr = 0.001
I0205 06:18:32.664713 12584 solver.cpp:237] Iteration 680, loss = 0.327709
I0205 06:18:32.664768 12584 solver.cpp:253]     Train net output #0: loss = 0.327709 (* 1 = 0.327709 loss)
I0205 06:18:32.664779 12584 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I0205 06:18:37.499869 12584 solver.cpp:237] Iteration 690, loss = 0.354662
I0205 06:18:37.499919 12584 solver.cpp:253]     Train net output #0: loss = 0.354662 (* 1 = 0.354662 loss)
I0205 06:18:37.499930 12584 sgd_solver.cpp:106] Iteration 690, lr = 0.001
I0205 06:18:41.854274 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_700.caffemodel
I0205 06:18:41.856453 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_700.solverstate
I0205 06:18:41.857291 12584 solver.cpp:341] Iteration 700, Testing net (#0)
I0205 06:18:44.179208 12584 solver.cpp:409]     Test net output #0: accuracy = 0.922
I0205 06:18:44.179256 12584 solver.cpp:409]     Test net output #1: loss = 0.206704 (* 1 = 0.206704 loss)
I0205 06:18:44.661800 12584 solver.cpp:237] Iteration 700, loss = 0.379358
I0205 06:18:44.661854 12584 solver.cpp:253]     Train net output #0: loss = 0.379358 (* 1 = 0.379358 loss)
I0205 06:18:44.661864 12584 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0205 06:18:49.502650 12584 solver.cpp:237] Iteration 710, loss = 0.342275
I0205 06:18:49.502701 12584 solver.cpp:253]     Train net output #0: loss = 0.342275 (* 1 = 0.342275 loss)
I0205 06:18:49.502712 12584 sgd_solver.cpp:106] Iteration 710, lr = 0.001
I0205 06:18:54.336125 12584 solver.cpp:237] Iteration 720, loss = 0.263794
I0205 06:18:54.336181 12584 solver.cpp:253]     Train net output #0: loss = 0.263794 (* 1 = 0.263794 loss)
I0205 06:18:54.336192 12584 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I0205 06:18:59.170516 12584 solver.cpp:237] Iteration 730, loss = 0.187547
I0205 06:18:59.170567 12584 solver.cpp:253]     Train net output #0: loss = 0.187547 (* 1 = 0.187547 loss)
I0205 06:18:59.170578 12584 sgd_solver.cpp:106] Iteration 730, lr = 0.001
I0205 06:19:04.001034 12584 solver.cpp:237] Iteration 740, loss = 0.183833
I0205 06:19:04.001083 12584 solver.cpp:253]     Train net output #0: loss = 0.183833 (* 1 = 0.183833 loss)
I0205 06:19:04.001093 12584 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I0205 06:19:08.832499 12584 solver.cpp:237] Iteration 750, loss = 0.0867901
I0205 06:19:08.832556 12584 solver.cpp:253]     Train net output #0: loss = 0.0867901 (* 1 = 0.0867901 loss)
I0205 06:19:08.832567 12584 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I0205 06:19:13.661748 12584 solver.cpp:237] Iteration 760, loss = 0.0917157
I0205 06:19:13.661923 12584 solver.cpp:253]     Train net output #0: loss = 0.0917157 (* 1 = 0.0917157 loss)
I0205 06:19:13.661936 12584 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I0205 06:19:18.519001 12584 solver.cpp:237] Iteration 770, loss = 0.278009
I0205 06:19:18.519055 12584 solver.cpp:253]     Train net output #0: loss = 0.278009 (* 1 = 0.278009 loss)
I0205 06:19:18.519078 12584 sgd_solver.cpp:106] Iteration 770, lr = 0.001
I0205 06:19:23.358389 12584 solver.cpp:237] Iteration 780, loss = 0.185527
I0205 06:19:23.358443 12584 solver.cpp:253]     Train net output #0: loss = 0.185527 (* 1 = 0.185527 loss)
I0205 06:19:23.358453 12584 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I0205 06:19:28.187505 12584 solver.cpp:237] Iteration 790, loss = 0.109779
I0205 06:19:28.187551 12584 solver.cpp:253]     Train net output #0: loss = 0.109779 (* 1 = 0.109779 loss)
I0205 06:19:28.187562 12584 sgd_solver.cpp:106] Iteration 790, lr = 0.001
I0205 06:19:32.535569 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_800.caffemodel
I0205 06:19:32.537549 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_800.solverstate
I0205 06:19:32.538343 12584 solver.cpp:341] Iteration 800, Testing net (#0)
I0205 06:19:34.859719 12584 solver.cpp:409]     Test net output #0: accuracy = 0.944
I0205 06:19:34.859767 12584 solver.cpp:409]     Test net output #1: loss = 0.138435 (* 1 = 0.138435 loss)
I0205 06:19:35.342605 12584 solver.cpp:237] Iteration 800, loss = 0.196555
I0205 06:19:35.342651 12584 solver.cpp:253]     Train net output #0: loss = 0.196555 (* 1 = 0.196555 loss)
I0205 06:19:35.342661 12584 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0205 06:19:40.190678 12584 solver.cpp:237] Iteration 810, loss = 0.0793208
I0205 06:19:40.190728 12584 solver.cpp:253]     Train net output #0: loss = 0.0793208 (* 1 = 0.0793208 loss)
I0205 06:19:40.190739 12584 sgd_solver.cpp:106] Iteration 810, lr = 0.001
I0205 06:19:45.035837 12584 solver.cpp:237] Iteration 820, loss = 0.0831386
I0205 06:19:45.415011 12584 solver.cpp:253]     Train net output #0: loss = 0.0831386 (* 1 = 0.0831386 loss)
I0205 06:19:45.415031 12584 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I0205 06:19:50.277575 12584 solver.cpp:237] Iteration 830, loss = 0.143193
I0205 06:19:50.277631 12584 solver.cpp:253]     Train net output #0: loss = 0.143193 (* 1 = 0.143193 loss)
I0205 06:19:50.277642 12584 sgd_solver.cpp:106] Iteration 830, lr = 0.001
I0205 06:19:55.118461 12584 solver.cpp:237] Iteration 840, loss = 0.179545
I0205 06:19:55.118515 12584 solver.cpp:253]     Train net output #0: loss = 0.179545 (* 1 = 0.179545 loss)
I0205 06:19:55.118526 12584 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I0205 06:19:59.964346 12584 solver.cpp:237] Iteration 850, loss = 0.0453149
I0205 06:19:59.964401 12584 solver.cpp:253]     Train net output #0: loss = 0.045315 (* 1 = 0.045315 loss)
I0205 06:19:59.964413 12584 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I0205 06:20:04.812234 12584 solver.cpp:237] Iteration 860, loss = 0.0603369
I0205 06:20:04.812286 12584 solver.cpp:253]     Train net output #0: loss = 0.060337 (* 1 = 0.060337 loss)
I0205 06:20:04.812299 12584 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I0205 06:20:09.657793 12584 solver.cpp:237] Iteration 870, loss = 0.112287
I0205 06:20:09.657847 12584 solver.cpp:253]     Train net output #0: loss = 0.112287 (* 1 = 0.112287 loss)
I0205 06:20:09.657860 12584 sgd_solver.cpp:106] Iteration 870, lr = 0.001
I0205 06:20:14.511104 12584 solver.cpp:237] Iteration 880, loss = 0.0893928
I0205 06:20:14.511157 12584 solver.cpp:253]     Train net output #0: loss = 0.0893929 (* 1 = 0.0893929 loss)
I0205 06:20:14.511168 12584 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I0205 06:20:19.368751 12584 solver.cpp:237] Iteration 890, loss = 0.073108
I0205 06:20:19.368960 12584 solver.cpp:253]     Train net output #0: loss = 0.073108 (* 1 = 0.073108 loss)
I0205 06:20:19.368980 12584 sgd_solver.cpp:106] Iteration 890, lr = 0.001
I0205 06:20:23.744232 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_900.caffemodel
I0205 06:20:23.746261 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_900.solverstate
I0205 06:20:23.747054 12584 solver.cpp:341] Iteration 900, Testing net (#0)
I0205 06:20:26.085952 12584 solver.cpp:409]     Test net output #0: accuracy = 0.99
I0205 06:20:26.086006 12584 solver.cpp:409]     Test net output #1: loss = 0.0326817 (* 1 = 0.0326817 loss)
I0205 06:20:26.572892 12584 solver.cpp:237] Iteration 900, loss = 0.0620041
I0205 06:20:26.572940 12584 solver.cpp:253]     Train net output #0: loss = 0.0620041 (* 1 = 0.0620041 loss)
I0205 06:20:26.572950 12584 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0205 06:20:31.450472 12584 solver.cpp:237] Iteration 910, loss = 0.111772
I0205 06:20:31.450523 12584 solver.cpp:253]     Train net output #0: loss = 0.111772 (* 1 = 0.111772 loss)
I0205 06:20:31.450534 12584 sgd_solver.cpp:106] Iteration 910, lr = 0.001
I0205 06:20:36.332403 12584 solver.cpp:237] Iteration 920, loss = 0.0990881
I0205 06:20:36.332455 12584 solver.cpp:253]     Train net output #0: loss = 0.0990882 (* 1 = 0.0990882 loss)
I0205 06:20:36.332466 12584 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I0205 06:20:41.205961 12584 solver.cpp:237] Iteration 930, loss = 0.0628128
I0205 06:20:41.206017 12584 solver.cpp:253]     Train net output #0: loss = 0.0628128 (* 1 = 0.0628128 loss)
I0205 06:20:41.206027 12584 sgd_solver.cpp:106] Iteration 930, lr = 0.001
I0205 06:20:46.079815 12584 solver.cpp:237] Iteration 940, loss = 0.0321453
I0205 06:20:46.079870 12584 solver.cpp:253]     Train net output #0: loss = 0.0321453 (* 1 = 0.0321453 loss)
I0205 06:20:46.079881 12584 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I0205 06:20:50.961408 12584 solver.cpp:237] Iteration 950, loss = 0.0506263
I0205 06:20:50.961602 12584 solver.cpp:253]     Train net output #0: loss = 0.0506263 (* 1 = 0.0506263 loss)
I0205 06:20:50.961616 12584 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I0205 06:20:55.848783 12584 solver.cpp:237] Iteration 960, loss = 0.0489928
I0205 06:20:55.848835 12584 solver.cpp:253]     Train net output #0: loss = 0.0489929 (* 1 = 0.0489929 loss)
I0205 06:20:55.848846 12584 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I0205 06:21:00.739939 12584 solver.cpp:237] Iteration 970, loss = 0.0227721
I0205 06:21:01.090808 12584 solver.cpp:253]     Train net output #0: loss = 0.0227722 (* 1 = 0.0227722 loss)
I0205 06:21:01.090843 12584 sgd_solver.cpp:106] Iteration 970, lr = 0.001
I0205 06:21:05.992353 12584 solver.cpp:237] Iteration 980, loss = 0.0368686
I0205 06:21:05.992408 12584 solver.cpp:253]     Train net output #0: loss = 0.0368687 (* 1 = 0.0368687 loss)
I0205 06:21:05.992420 12584 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I0205 06:21:10.890682 12584 solver.cpp:237] Iteration 990, loss = 0.0170428
I0205 06:21:10.890733 12584 solver.cpp:253]     Train net output #0: loss = 0.0170429 (* 1 = 0.0170429 loss)
I0205 06:21:10.890744 12584 sgd_solver.cpp:106] Iteration 990, lr = 0.001
I0205 06:21:15.292721 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1000.caffemodel
I0205 06:21:15.294706 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1000.solverstate
I0205 06:21:15.295508 12584 solver.cpp:341] Iteration 1000, Testing net (#0)
I0205 06:21:17.643753 12584 solver.cpp:409]     Test net output #0: accuracy = 0.994
I0205 06:21:17.643801 12584 solver.cpp:409]     Test net output #1: loss = 0.0220706 (* 1 = 0.0220706 loss)
I0205 06:21:18.130584 12584 solver.cpp:237] Iteration 1000, loss = 0.0870789
I0205 06:21:18.130631 12584 solver.cpp:253]     Train net output #0: loss = 0.0870789 (* 1 = 0.0870789 loss)
I0205 06:21:18.130643 12584 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0205 06:21:23.023213 12584 solver.cpp:237] Iteration 1010, loss = 0.0233821
I0205 06:21:23.023427 12584 solver.cpp:253]     Train net output #0: loss = 0.0233822 (* 1 = 0.0233822 loss)
I0205 06:21:23.023442 12584 sgd_solver.cpp:106] Iteration 1010, lr = 0.001
I0205 06:21:27.923673 12584 solver.cpp:237] Iteration 1020, loss = 0.0126579
I0205 06:21:27.923725 12584 solver.cpp:253]     Train net output #0: loss = 0.0126579 (* 1 = 0.0126579 loss)
I0205 06:21:27.923737 12584 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I0205 06:21:32.818908 12584 solver.cpp:237] Iteration 1030, loss = 0.117637
I0205 06:21:32.818961 12584 solver.cpp:253]     Train net output #0: loss = 0.117637 (* 1 = 0.117637 loss)
I0205 06:21:32.818977 12584 sgd_solver.cpp:106] Iteration 1030, lr = 0.001
I0205 06:21:37.716920 12584 solver.cpp:237] Iteration 1040, loss = 0.12211
I0205 06:21:37.716985 12584 solver.cpp:253]     Train net output #0: loss = 0.12211 (* 1 = 0.12211 loss)
I0205 06:21:37.716998 12584 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I0205 06:21:42.619796 12584 solver.cpp:237] Iteration 1050, loss = 0.0145867
I0205 06:21:42.619848 12584 solver.cpp:253]     Train net output #0: loss = 0.0145867 (* 1 = 0.0145867 loss)
I0205 06:21:42.619859 12584 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I0205 06:21:47.533689 12584 solver.cpp:237] Iteration 1060, loss = 0.0298846
I0205 06:21:47.533743 12584 solver.cpp:253]     Train net output #0: loss = 0.0298847 (* 1 = 0.0298847 loss)
I0205 06:21:47.533756 12584 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I0205 06:21:52.441938 12584 solver.cpp:237] Iteration 1070, loss = 0.0182793
I0205 06:21:52.441999 12584 solver.cpp:253]     Train net output #0: loss = 0.0182793 (* 1 = 0.0182793 loss)
I0205 06:21:52.442010 12584 sgd_solver.cpp:106] Iteration 1070, lr = 0.001
I0205 06:21:57.337461 12584 solver.cpp:237] Iteration 1080, loss = 0.0474267
I0205 06:21:57.337646 12584 solver.cpp:253]     Train net output #0: loss = 0.0474267 (* 1 = 0.0474267 loss)
I0205 06:21:57.337661 12584 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I0205 06:22:02.236944 12584 solver.cpp:237] Iteration 1090, loss = 0.0135775
I0205 06:22:02.237004 12584 solver.cpp:253]     Train net output #0: loss = 0.0135775 (* 1 = 0.0135775 loss)
I0205 06:22:02.237015 12584 sgd_solver.cpp:106] Iteration 1090, lr = 0.001
I0205 06:22:06.657518 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1100.caffemodel
I0205 06:22:06.659507 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1100.solverstate
I0205 06:22:06.660293 12584 solver.cpp:341] Iteration 1100, Testing net (#0)
I0205 06:22:09.012926 12584 solver.cpp:409]     Test net output #0: accuracy = 0.996
I0205 06:22:09.012981 12584 solver.cpp:409]     Test net output #1: loss = 0.0155991 (* 1 = 0.0155991 loss)
I0205 06:22:09.500836 12584 solver.cpp:237] Iteration 1100, loss = 0.021268
I0205 06:22:09.500882 12584 solver.cpp:253]     Train net output #0: loss = 0.021268 (* 1 = 0.021268 loss)
I0205 06:22:09.500893 12584 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I0205 06:22:14.401677 12584 solver.cpp:237] Iteration 1110, loss = 0.0791886
I0205 06:22:14.401732 12584 solver.cpp:253]     Train net output #0: loss = 0.0791887 (* 1 = 0.0791887 loss)
I0205 06:22:14.401744 12584 sgd_solver.cpp:106] Iteration 1110, lr = 0.001
I0205 06:22:19.326017 12584 solver.cpp:237] Iteration 1120, loss = 0.0316792
I0205 06:22:19.326071 12584 solver.cpp:253]     Train net output #0: loss = 0.0316793 (* 1 = 0.0316793 loss)
I0205 06:22:19.326081 12584 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I0205 06:22:24.246837 12584 solver.cpp:237] Iteration 1130, loss = 0.0210557
I0205 06:22:24.246893 12584 solver.cpp:253]     Train net output #0: loss = 0.0210557 (* 1 = 0.0210557 loss)
I0205 06:22:24.246904 12584 sgd_solver.cpp:106] Iteration 1130, lr = 0.001
I0205 06:22:29.164624 12584 solver.cpp:237] Iteration 1140, loss = 0.0564476
I0205 06:22:29.164835 12584 solver.cpp:253]     Train net output #0: loss = 0.0564476 (* 1 = 0.0564476 loss)
I0205 06:22:29.164849 12584 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I0205 06:22:34.076174 12584 solver.cpp:237] Iteration 1150, loss = 0.022243
I0205 06:22:34.076227 12584 solver.cpp:253]     Train net output #0: loss = 0.0222431 (* 1 = 0.0222431 loss)
I0205 06:22:34.076238 12584 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I0205 06:22:39.014858 12584 solver.cpp:237] Iteration 1160, loss = 0.0421757
I0205 06:22:39.014910 12584 solver.cpp:253]     Train net output #0: loss = 0.0421757 (* 1 = 0.0421757 loss)
I0205 06:22:39.014921 12584 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I0205 06:22:43.946454 12584 solver.cpp:237] Iteration 1170, loss = 0.0275406
I0205 06:22:43.946506 12584 solver.cpp:253]     Train net output #0: loss = 0.0275407 (* 1 = 0.0275407 loss)
I0205 06:22:43.946517 12584 sgd_solver.cpp:106] Iteration 1170, lr = 0.001
I0205 06:22:48.868532 12584 solver.cpp:237] Iteration 1180, loss = 0.0106564
I0205 06:22:48.868583 12584 solver.cpp:253]     Train net output #0: loss = 0.0106564 (* 1 = 0.0106564 loss)
I0205 06:22:48.868594 12584 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I0205 06:22:53.785938 12584 solver.cpp:237] Iteration 1190, loss = 0.0385094
I0205 06:22:53.786000 12584 solver.cpp:253]     Train net output #0: loss = 0.0385094 (* 1 = 0.0385094 loss)
I0205 06:22:53.786011 12584 sgd_solver.cpp:106] Iteration 1190, lr = 0.001
I0205 06:22:58.212276 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1200.caffemodel
I0205 06:22:58.214293 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1200.solverstate
I0205 06:22:58.215134 12584 solver.cpp:341] Iteration 1200, Testing net (#0)
I0205 06:23:00.580296 12584 solver.cpp:409]     Test net output #0: accuracy = 0.995
I0205 06:23:00.580587 12584 solver.cpp:409]     Test net output #1: loss = 0.0182655 (* 1 = 0.0182655 loss)
I0205 06:23:01.071172 12584 solver.cpp:237] Iteration 1200, loss = 0.0381153
I0205 06:23:01.071223 12584 solver.cpp:253]     Train net output #0: loss = 0.0381154 (* 1 = 0.0381154 loss)
I0205 06:23:01.071234 12584 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0205 06:23:05.997727 12584 solver.cpp:237] Iteration 1210, loss = 0.0130816
I0205 06:23:05.997783 12584 solver.cpp:253]     Train net output #0: loss = 0.0130816 (* 1 = 0.0130816 loss)
I0205 06:23:05.997794 12584 sgd_solver.cpp:106] Iteration 1210, lr = 0.001
I0205 06:23:10.918898 12584 solver.cpp:237] Iteration 1220, loss = 0.0626926
I0205 06:23:10.918949 12584 solver.cpp:253]     Train net output #0: loss = 0.0626927 (* 1 = 0.0626927 loss)
I0205 06:23:10.918961 12584 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I0205 06:23:15.845125 12584 solver.cpp:237] Iteration 1230, loss = 0.108657
I0205 06:23:15.845181 12584 solver.cpp:253]     Train net output #0: loss = 0.108657 (* 1 = 0.108657 loss)
I0205 06:23:15.845192 12584 sgd_solver.cpp:106] Iteration 1230, lr = 0.001
I0205 06:23:20.763339 12584 solver.cpp:237] Iteration 1240, loss = 0.120839
I0205 06:23:20.763391 12584 solver.cpp:253]     Train net output #0: loss = 0.120839 (* 1 = 0.120839 loss)
I0205 06:23:20.763401 12584 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I0205 06:23:25.686898 12584 solver.cpp:237] Iteration 1250, loss = 0.01888
I0205 06:23:25.686954 12584 solver.cpp:253]     Train net output #0: loss = 0.01888 (* 1 = 0.01888 loss)
I0205 06:23:25.686970 12584 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I0205 06:23:30.604282 12584 solver.cpp:237] Iteration 1260, loss = 0.00545524
I0205 06:23:30.604475 12584 solver.cpp:253]     Train net output #0: loss = 0.00545529 (* 1 = 0.00545529 loss)
I0205 06:23:30.604488 12584 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I0205 06:23:35.527228 12584 solver.cpp:237] Iteration 1270, loss = 0.0312611
I0205 06:23:35.527283 12584 solver.cpp:253]     Train net output #0: loss = 0.0312612 (* 1 = 0.0312612 loss)
I0205 06:23:35.527294 12584 sgd_solver.cpp:106] Iteration 1270, lr = 0.001
I0205 06:23:40.450368 12584 solver.cpp:237] Iteration 1280, loss = 0.0678046
I0205 06:23:40.450410 12584 solver.cpp:253]     Train net output #0: loss = 0.0678046 (* 1 = 0.0678046 loss)
I0205 06:23:40.450422 12584 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I0205 06:23:45.386289 12584 solver.cpp:237] Iteration 1290, loss = 0.00314461
I0205 06:23:45.386340 12584 solver.cpp:253]     Train net output #0: loss = 0.00314465 (* 1 = 0.00314465 loss)
I0205 06:23:45.386351 12584 sgd_solver.cpp:106] Iteration 1290, lr = 0.001
I0205 06:23:49.815742 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1300.caffemodel
I0205 06:23:49.817765 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1300.solverstate
I0205 06:23:49.818593 12584 solver.cpp:341] Iteration 1300, Testing net (#0)
I0205 06:23:52.186455 12584 solver.cpp:409]     Test net output #0: accuracy = 0.986
I0205 06:23:52.186502 12584 solver.cpp:409]     Test net output #1: loss = 0.0488642 (* 1 = 0.0488642 loss)
I0205 06:23:52.677091 12584 solver.cpp:237] Iteration 1300, loss = 0.066796
I0205 06:23:52.677139 12584 solver.cpp:253]     Train net output #0: loss = 0.0667961 (* 1 = 0.0667961 loss)
I0205 06:23:52.677150 12584 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I0205 06:23:57.603340 12584 solver.cpp:237] Iteration 1310, loss = 0.0133795
I0205 06:23:57.603392 12584 solver.cpp:253]     Train net output #0: loss = 0.0133796 (* 1 = 0.0133796 loss)
I0205 06:23:57.603404 12584 sgd_solver.cpp:106] Iteration 1310, lr = 0.001
I0205 06:24:02.524129 12584 solver.cpp:237] Iteration 1320, loss = 0.111713
I0205 06:24:02.524327 12584 solver.cpp:253]     Train net output #0: loss = 0.111713 (* 1 = 0.111713 loss)
I0205 06:24:02.524340 12584 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I0205 06:24:07.438395 12584 solver.cpp:237] Iteration 1330, loss = 0.0763248
I0205 06:24:07.438449 12584 solver.cpp:253]     Train net output #0: loss = 0.0763249 (* 1 = 0.0763249 loss)
I0205 06:24:07.438462 12584 sgd_solver.cpp:106] Iteration 1330, lr = 0.001
I0205 06:24:12.351492 12584 solver.cpp:237] Iteration 1340, loss = 0.00468538
I0205 06:24:12.351547 12584 solver.cpp:253]     Train net output #0: loss = 0.00468544 (* 1 = 0.00468544 loss)
I0205 06:24:12.351557 12584 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I0205 06:24:17.251672 12584 solver.cpp:237] Iteration 1350, loss = 0.0150846
I0205 06:24:17.251729 12584 solver.cpp:253]     Train net output #0: loss = 0.0150847 (* 1 = 0.0150847 loss)
I0205 06:24:17.251739 12584 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I0205 06:24:22.154203 12584 solver.cpp:237] Iteration 1360, loss = 0.0923386
I0205 06:24:22.154253 12584 solver.cpp:253]     Train net output #0: loss = 0.0923387 (* 1 = 0.0923387 loss)
I0205 06:24:22.154265 12584 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I0205 06:24:27.073408 12584 solver.cpp:237] Iteration 1370, loss = 0.041249
I0205 06:24:27.073460 12584 solver.cpp:253]     Train net output #0: loss = 0.0412491 (* 1 = 0.0412491 loss)
I0205 06:24:27.073472 12584 sgd_solver.cpp:106] Iteration 1370, lr = 0.001
I0205 06:24:31.987344 12584 solver.cpp:237] Iteration 1380, loss = 0.0210822
I0205 06:24:31.987393 12584 solver.cpp:253]     Train net output #0: loss = 0.0210822 (* 1 = 0.0210822 loss)
I0205 06:24:31.987404 12584 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I0205 06:24:36.896463 12584 solver.cpp:237] Iteration 1390, loss = 0.0395966
I0205 06:24:36.896651 12584 solver.cpp:253]     Train net output #0: loss = 0.0395967 (* 1 = 0.0395967 loss)
I0205 06:24:36.896663 12584 sgd_solver.cpp:106] Iteration 1390, lr = 0.001
I0205 06:24:41.308343 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1400.caffemodel
I0205 06:24:41.310369 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1400.solverstate
I0205 06:24:41.311193 12584 solver.cpp:341] Iteration 1400, Testing net (#0)
I0205 06:24:43.667580 12584 solver.cpp:409]     Test net output #0: accuracy = 0.994
I0205 06:24:43.667629 12584 solver.cpp:409]     Test net output #1: loss = 0.016625 (* 1 = 0.016625 loss)
I0205 06:24:44.157405 12584 solver.cpp:237] Iteration 1400, loss = 0.043534
I0205 06:24:44.157454 12584 solver.cpp:253]     Train net output #0: loss = 0.043534 (* 1 = 0.043534 loss)
I0205 06:24:44.157464 12584 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0205 06:24:49.067445 12584 solver.cpp:237] Iteration 1410, loss = 0.0545963
I0205 06:24:49.067498 12584 solver.cpp:253]     Train net output #0: loss = 0.0545964 (* 1 = 0.0545964 loss)
I0205 06:24:49.067509 12584 sgd_solver.cpp:106] Iteration 1410, lr = 0.001
I0205 06:24:53.968531 12584 solver.cpp:237] Iteration 1420, loss = 0.0395878
I0205 06:24:53.968582 12584 solver.cpp:253]     Train net output #0: loss = 0.0395879 (* 1 = 0.0395879 loss)
I0205 06:24:53.968593 12584 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I0205 06:24:58.864099 12584 solver.cpp:237] Iteration 1430, loss = 0.016005
I0205 06:24:58.864152 12584 solver.cpp:253]     Train net output #0: loss = 0.016005 (* 1 = 0.016005 loss)
I0205 06:24:58.864163 12584 sgd_solver.cpp:106] Iteration 1430, lr = 0.001
I0205 06:25:03.766288 12584 solver.cpp:237] Iteration 1440, loss = 0.0222957
I0205 06:25:03.766338 12584 solver.cpp:253]     Train net output #0: loss = 0.0222958 (* 1 = 0.0222958 loss)
I0205 06:25:03.766350 12584 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I0205 06:25:08.682019 12584 solver.cpp:237] Iteration 1450, loss = 0.0230163
I0205 06:25:08.682227 12584 solver.cpp:253]     Train net output #0: loss = 0.0230163 (* 1 = 0.0230163 loss)
I0205 06:25:08.682240 12584 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I0205 06:25:13.588546 12584 solver.cpp:237] Iteration 1460, loss = 0.046521
I0205 06:25:13.588599 12584 solver.cpp:253]     Train net output #0: loss = 0.046521 (* 1 = 0.046521 loss)
I0205 06:25:13.588610 12584 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I0205 06:25:18.515879 12584 solver.cpp:237] Iteration 1470, loss = 0.040174
I0205 06:25:18.515933 12584 solver.cpp:253]     Train net output #0: loss = 0.0401741 (* 1 = 0.0401741 loss)
I0205 06:25:18.515944 12584 sgd_solver.cpp:106] Iteration 1470, lr = 0.001
I0205 06:25:23.437707 12584 solver.cpp:237] Iteration 1480, loss = 0.0166647
I0205 06:25:23.437763 12584 solver.cpp:253]     Train net output #0: loss = 0.0166647 (* 1 = 0.0166647 loss)
I0205 06:25:23.437774 12584 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I0205 06:25:28.355443 12584 solver.cpp:237] Iteration 1490, loss = 0.0545792
I0205 06:25:28.355494 12584 solver.cpp:253]     Train net output #0: loss = 0.0545793 (* 1 = 0.0545793 loss)
I0205 06:25:28.355506 12584 sgd_solver.cpp:106] Iteration 1490, lr = 0.001
I0205 06:25:32.788667 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1500.caffemodel
I0205 06:25:32.790686 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1500.solverstate
I0205 06:25:32.791509 12584 solver.cpp:341] Iteration 1500, Testing net (#0)
I0205 06:25:35.151031 12584 solver.cpp:409]     Test net output #0: accuracy = 0.995
I0205 06:25:35.151079 12584 solver.cpp:409]     Test net output #1: loss = 0.0188895 (* 1 = 0.0188895 loss)
I0205 06:25:35.639003 12584 solver.cpp:237] Iteration 1500, loss = 0.00217353
I0205 06:25:35.639067 12584 solver.cpp:253]     Train net output #0: loss = 0.00217359 (* 1 = 0.00217359 loss)
I0205 06:25:35.639078 12584 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I0205 06:25:40.552659 12584 solver.cpp:237] Iteration 1510, loss = 0.041016
I0205 06:25:40.552892 12584 solver.cpp:253]     Train net output #0: loss = 0.041016 (* 1 = 0.041016 loss)
I0205 06:25:40.552906 12584 sgd_solver.cpp:106] Iteration 1510, lr = 0.001
I0205 06:25:45.467659 12584 solver.cpp:237] Iteration 1520, loss = 0.0155849
I0205 06:25:45.467717 12584 solver.cpp:253]     Train net output #0: loss = 0.015585 (* 1 = 0.015585 loss)
I0205 06:25:45.467730 12584 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I0205 06:25:50.385895 12584 solver.cpp:237] Iteration 1530, loss = 0.0407407
I0205 06:25:50.385946 12584 solver.cpp:253]     Train net output #0: loss = 0.0407407 (* 1 = 0.0407407 loss)
I0205 06:25:50.385957 12584 sgd_solver.cpp:106] Iteration 1530, lr = 0.001
I0205 06:25:55.302178 12584 solver.cpp:237] Iteration 1540, loss = 0.0203447
I0205 06:25:55.302232 12584 solver.cpp:253]     Train net output #0: loss = 0.0203448 (* 1 = 0.0203448 loss)
I0205 06:25:55.302243 12584 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I0205 06:26:00.221572 12584 solver.cpp:237] Iteration 1550, loss = 0.00298689
I0205 06:26:00.221629 12584 solver.cpp:253]     Train net output #0: loss = 0.00298696 (* 1 = 0.00298696 loss)
I0205 06:26:00.221640 12584 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I0205 06:26:05.136626 12584 solver.cpp:237] Iteration 1560, loss = 0.0340739
I0205 06:26:05.136677 12584 solver.cpp:253]     Train net output #0: loss = 0.034074 (* 1 = 0.034074 loss)
I0205 06:26:05.136689 12584 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I0205 06:26:10.062290 12584 solver.cpp:237] Iteration 1570, loss = 0.00680894
I0205 06:26:10.062342 12584 solver.cpp:253]     Train net output #0: loss = 0.006809 (* 1 = 0.006809 loss)
I0205 06:26:10.062353 12584 sgd_solver.cpp:106] Iteration 1570, lr = 0.001
I0205 06:26:14.994236 12584 solver.cpp:237] Iteration 1580, loss = 0.0107948
I0205 06:26:14.994426 12584 solver.cpp:253]     Train net output #0: loss = 0.0107949 (* 1 = 0.0107949 loss)
I0205 06:26:14.994439 12584 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I0205 06:26:19.924485 12584 solver.cpp:237] Iteration 1590, loss = 0.0406486
I0205 06:26:19.924537 12584 solver.cpp:253]     Train net output #0: loss = 0.0406487 (* 1 = 0.0406487 loss)
I0205 06:26:19.924548 12584 sgd_solver.cpp:106] Iteration 1590, lr = 0.001
I0205 06:26:24.359745 12584 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1600.caffemodel
I0205 06:26:24.361759 12584 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed21/snaps/snap__iter_1600.solverstate
I0205 06:26:24.602288 12584 solver.cpp:321] Iteration 1600, loss = 0.0616312
I0205 06:26:24.602334 12584 solver.cpp:341] Iteration 1600, Testing net (#0)
I0205 06:26:26.975579 12584 solver.cpp:409]     Test net output #0: accuracy = 0.992
I0205 06:26:26.975625 12584 solver.cpp:409]     Test net output #1: loss = 0.0274248 (* 1 = 0.0274248 loss)
I0205 06:26:26.975635 12584 solver.cpp:326] Optimization Done.
I0205 06:26:26.975641 12584 caffe.cpp:215] Optimization Done.
