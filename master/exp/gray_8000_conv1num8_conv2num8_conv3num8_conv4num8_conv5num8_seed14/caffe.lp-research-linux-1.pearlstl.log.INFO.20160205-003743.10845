Log file created at: 2016/02/05 00:37:43
Running on machine: lp-research-linux-1
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0205 00:37:43.733460 10845 caffe.cpp:177] Use CPU.
I0205 00:37:43.734344 10845 solver.cpp:48] Initializing solver from parameters: 
test_iter: 10
test_interval: 100
base_lr: 0.001
display: 10
max_iter: 1600
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 100
snapshot_prefix: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap_"
solver_mode: CPU
random_seed: 14
net: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/train_val.prototxt"
I0205 00:37:43.734519 10845 solver.cpp:91] Creating training net from net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/train_val.prototxt
I0205 00:37:43.735159 10845 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0205 00:37:43.735195 10845 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0205 00:37:43.735450 10845 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 00:37:43.735594 10845 layer_factory.hpp:77] Creating layer data
I0205 00:37:43.735782 10845 net.cpp:106] Creating Layer data
I0205 00:37:43.735801 10845 net.cpp:411] data -> data
I0205 00:37:43.735888 10845 net.cpp:411] data -> label
I0205 00:37:43.735913 10845 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto
I0205 00:37:43.736089 10846 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db
I0205 00:37:43.736999 10845 data_layer.cpp:41] output data size: 100,1,227,227
I0205 00:37:43.768534 10845 net.cpp:150] Setting up data
I0205 00:37:43.768578 10845 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 00:37:43.768587 10845 net.cpp:157] Top shape: 100 (100)
I0205 00:37:43.768594 10845 net.cpp:165] Memory required for data: 20612000
I0205 00:37:43.768615 10845 layer_factory.hpp:77] Creating layer conv1
I0205 00:37:43.768661 10845 net.cpp:106] Creating Layer conv1
I0205 00:37:43.768671 10845 net.cpp:454] conv1 <- data
I0205 00:37:43.768697 10845 net.cpp:411] conv1 -> conv1
I0205 00:37:43.768813 10845 net.cpp:150] Setting up conv1
I0205 00:37:43.768826 10845 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 00:37:43.768831 10845 net.cpp:165] Memory required for data: 30292000
I0205 00:37:43.768851 10845 layer_factory.hpp:77] Creating layer relu1
I0205 00:37:43.768864 10845 net.cpp:106] Creating Layer relu1
I0205 00:37:43.768872 10845 net.cpp:454] relu1 <- conv1
I0205 00:37:43.768880 10845 net.cpp:397] relu1 -> conv1 (in-place)
I0205 00:37:43.768894 10845 net.cpp:150] Setting up relu1
I0205 00:37:43.768903 10845 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 00:37:43.768908 10845 net.cpp:165] Memory required for data: 39972000
I0205 00:37:43.768913 10845 layer_factory.hpp:77] Creating layer pool1
I0205 00:37:43.768925 10845 net.cpp:106] Creating Layer pool1
I0205 00:37:43.768934 10845 net.cpp:454] pool1 <- conv1
I0205 00:37:43.768942 10845 net.cpp:411] pool1 -> pool1
I0205 00:37:43.768972 10845 net.cpp:150] Setting up pool1
I0205 00:37:43.768982 10845 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 00:37:43.768987 10845 net.cpp:165] Memory required for data: 42304800
I0205 00:37:43.768993 10845 layer_factory.hpp:77] Creating layer norm1
I0205 00:37:43.769016 10845 net.cpp:106] Creating Layer norm1
I0205 00:37:43.769032 10845 net.cpp:454] norm1 <- pool1
I0205 00:37:43.769042 10845 net.cpp:411] norm1 -> norm1
I0205 00:37:43.769060 10845 net.cpp:150] Setting up norm1
I0205 00:37:43.769069 10845 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 00:37:43.769075 10845 net.cpp:165] Memory required for data: 44637600
I0205 00:37:43.769081 10845 layer_factory.hpp:77] Creating layer conv2
I0205 00:37:43.769099 10845 net.cpp:106] Creating Layer conv2
I0205 00:37:43.769109 10845 net.cpp:454] conv2 <- norm1
I0205 00:37:43.769119 10845 net.cpp:411] conv2 -> conv2
I0205 00:37:43.769151 10845 net.cpp:150] Setting up conv2
I0205 00:37:43.769160 10845 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 00:37:43.769165 10845 net.cpp:165] Memory required for data: 46970400
I0205 00:37:43.769176 10845 layer_factory.hpp:77] Creating layer relu2
I0205 00:37:43.769186 10845 net.cpp:106] Creating Layer relu2
I0205 00:37:43.769191 10845 net.cpp:454] relu2 <- conv2
I0205 00:37:43.769199 10845 net.cpp:397] relu2 -> conv2 (in-place)
I0205 00:37:43.769208 10845 net.cpp:150] Setting up relu2
I0205 00:37:43.769215 10845 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 00:37:43.769220 10845 net.cpp:165] Memory required for data: 49303200
I0205 00:37:43.769227 10845 layer_factory.hpp:77] Creating layer pool2
I0205 00:37:43.769234 10845 net.cpp:106] Creating Layer pool2
I0205 00:37:43.769240 10845 net.cpp:454] pool2 <- conv2
I0205 00:37:43.769253 10845 net.cpp:411] pool2 -> pool2
I0205 00:37:43.769263 10845 net.cpp:150] Setting up pool2
I0205 00:37:43.769271 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.769276 10845 net.cpp:165] Memory required for data: 49844000
I0205 00:37:43.769281 10845 layer_factory.hpp:77] Creating layer norm2
I0205 00:37:43.769292 10845 net.cpp:106] Creating Layer norm2
I0205 00:37:43.769299 10845 net.cpp:454] norm2 <- pool2
I0205 00:37:43.769307 10845 net.cpp:411] norm2 -> norm2
I0205 00:37:43.769317 10845 net.cpp:150] Setting up norm2
I0205 00:37:43.769325 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.769330 10845 net.cpp:165] Memory required for data: 50384800
I0205 00:37:43.769335 10845 layer_factory.hpp:77] Creating layer conv3
I0205 00:37:43.769346 10845 net.cpp:106] Creating Layer conv3
I0205 00:37:43.769352 10845 net.cpp:454] conv3 <- norm2
I0205 00:37:43.769361 10845 net.cpp:411] conv3 -> conv3
I0205 00:37:43.769392 10845 net.cpp:150] Setting up conv3
I0205 00:37:43.769402 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.769407 10845 net.cpp:165] Memory required for data: 50925600
I0205 00:37:43.769417 10845 layer_factory.hpp:77] Creating layer relu3
I0205 00:37:43.769425 10845 net.cpp:106] Creating Layer relu3
I0205 00:37:43.769431 10845 net.cpp:454] relu3 <- conv3
I0205 00:37:43.769439 10845 net.cpp:397] relu3 -> conv3 (in-place)
I0205 00:37:43.769448 10845 net.cpp:150] Setting up relu3
I0205 00:37:43.769454 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.769460 10845 net.cpp:165] Memory required for data: 51466400
I0205 00:37:43.769465 10845 layer_factory.hpp:77] Creating layer conv4
I0205 00:37:43.769476 10845 net.cpp:106] Creating Layer conv4
I0205 00:37:43.769482 10845 net.cpp:454] conv4 <- conv3
I0205 00:37:43.769490 10845 net.cpp:411] conv4 -> conv4
I0205 00:37:43.769516 10845 net.cpp:150] Setting up conv4
I0205 00:37:43.769526 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.769531 10845 net.cpp:165] Memory required for data: 52007200
I0205 00:37:43.769538 10845 layer_factory.hpp:77] Creating layer relu4
I0205 00:37:43.769547 10845 net.cpp:106] Creating Layer relu4
I0205 00:37:43.769552 10845 net.cpp:454] relu4 <- conv4
I0205 00:37:43.769562 10845 net.cpp:397] relu4 -> conv4 (in-place)
I0205 00:37:43.769569 10845 net.cpp:150] Setting up relu4
I0205 00:37:43.769577 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.769582 10845 net.cpp:165] Memory required for data: 52548000
I0205 00:37:43.769587 10845 layer_factory.hpp:77] Creating layer conv5
I0205 00:37:43.769603 10845 net.cpp:106] Creating Layer conv5
I0205 00:37:43.769615 10845 net.cpp:454] conv5 <- conv4
I0205 00:37:43.769625 10845 net.cpp:411] conv5 -> conv5
I0205 00:37:43.769649 10845 net.cpp:150] Setting up conv5
I0205 00:37:43.769657 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.769664 10845 net.cpp:165] Memory required for data: 53088800
I0205 00:37:43.769675 10845 layer_factory.hpp:77] Creating layer relu5
I0205 00:37:43.769685 10845 net.cpp:106] Creating Layer relu5
I0205 00:37:43.769690 10845 net.cpp:454] relu5 <- conv5
I0205 00:37:43.769697 10845 net.cpp:397] relu5 -> conv5 (in-place)
I0205 00:37:43.769706 10845 net.cpp:150] Setting up relu5
I0205 00:37:43.769713 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.769718 10845 net.cpp:165] Memory required for data: 53629600
I0205 00:37:43.769723 10845 layer_factory.hpp:77] Creating layer pool5
I0205 00:37:43.769732 10845 net.cpp:106] Creating Layer pool5
I0205 00:37:43.769738 10845 net.cpp:454] pool5 <- conv5
I0205 00:37:43.769747 10845 net.cpp:411] pool5 -> pool5
I0205 00:37:43.769757 10845 net.cpp:150] Setting up pool5
I0205 00:37:43.769767 10845 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0205 00:37:43.769773 10845 net.cpp:165] Memory required for data: 53744800
I0205 00:37:43.769778 10845 layer_factory.hpp:77] Creating layer fc6
I0205 00:37:43.769794 10845 net.cpp:106] Creating Layer fc6
I0205 00:37:43.769801 10845 net.cpp:454] fc6 <- pool5
I0205 00:37:43.769810 10845 net.cpp:411] fc6 -> fc6
I0205 00:37:43.770623 10845 net.cpp:150] Setting up fc6
I0205 00:37:43.770638 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.770644 10845 net.cpp:165] Memory required for data: 53847200
I0205 00:37:43.770653 10845 layer_factory.hpp:77] Creating layer relu6
I0205 00:37:43.770663 10845 net.cpp:106] Creating Layer relu6
I0205 00:37:43.770668 10845 net.cpp:454] relu6 <- fc6
I0205 00:37:43.770678 10845 net.cpp:397] relu6 -> fc6 (in-place)
I0205 00:37:43.770687 10845 net.cpp:150] Setting up relu6
I0205 00:37:43.770694 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.770699 10845 net.cpp:165] Memory required for data: 53949600
I0205 00:37:43.770705 10845 layer_factory.hpp:77] Creating layer drop6
I0205 00:37:43.770719 10845 net.cpp:106] Creating Layer drop6
I0205 00:37:43.770725 10845 net.cpp:454] drop6 <- fc6
I0205 00:37:43.770732 10845 net.cpp:397] drop6 -> fc6 (in-place)
I0205 00:37:43.770751 10845 net.cpp:150] Setting up drop6
I0205 00:37:43.770759 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.770764 10845 net.cpp:165] Memory required for data: 54052000
I0205 00:37:43.770771 10845 layer_factory.hpp:77] Creating layer fc7
I0205 00:37:43.770783 10845 net.cpp:106] Creating Layer fc7
I0205 00:37:43.770789 10845 net.cpp:454] fc7 <- fc6
I0205 00:37:43.770798 10845 net.cpp:411] fc7 -> fc7
I0205 00:37:43.771494 10845 net.cpp:150] Setting up fc7
I0205 00:37:43.771509 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.771515 10845 net.cpp:165] Memory required for data: 54154400
I0205 00:37:43.771524 10845 layer_factory.hpp:77] Creating layer relu7
I0205 00:37:43.771533 10845 net.cpp:106] Creating Layer relu7
I0205 00:37:43.771538 10845 net.cpp:454] relu7 <- fc7
I0205 00:37:43.771545 10845 net.cpp:397] relu7 -> fc7 (in-place)
I0205 00:37:43.771555 10845 net.cpp:150] Setting up relu7
I0205 00:37:43.771561 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.771567 10845 net.cpp:165] Memory required for data: 54256800
I0205 00:37:43.771572 10845 layer_factory.hpp:77] Creating layer drop7
I0205 00:37:43.771584 10845 net.cpp:106] Creating Layer drop7
I0205 00:37:43.771589 10845 net.cpp:454] drop7 <- fc7
I0205 00:37:43.771596 10845 net.cpp:397] drop7 -> fc7 (in-place)
I0205 00:37:43.771606 10845 net.cpp:150] Setting up drop7
I0205 00:37:43.771613 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.771618 10845 net.cpp:165] Memory required for data: 54359200
I0205 00:37:43.771626 10845 layer_factory.hpp:77] Creating layer fc8
I0205 00:37:43.771639 10845 net.cpp:106] Creating Layer fc8
I0205 00:37:43.771651 10845 net.cpp:454] fc8 <- fc7
I0205 00:37:43.771666 10845 net.cpp:411] fc8 -> fc8
I0205 00:37:43.771692 10845 net.cpp:150] Setting up fc8
I0205 00:37:43.771699 10845 net.cpp:157] Top shape: 100 2 (200)
I0205 00:37:43.771704 10845 net.cpp:165] Memory required for data: 54360000
I0205 00:37:43.771713 10845 layer_factory.hpp:77] Creating layer loss
I0205 00:37:43.771721 10845 net.cpp:106] Creating Layer loss
I0205 00:37:43.771728 10845 net.cpp:454] loss <- fc8
I0205 00:37:43.771733 10845 net.cpp:454] loss <- label
I0205 00:37:43.771744 10845 net.cpp:411] loss -> loss
I0205 00:37:43.771764 10845 layer_factory.hpp:77] Creating layer loss
I0205 00:37:43.771790 10845 net.cpp:150] Setting up loss
I0205 00:37:43.771798 10845 net.cpp:157] Top shape: (1)
I0205 00:37:43.771803 10845 net.cpp:160]     with loss weight 1
I0205 00:37:43.771834 10845 net.cpp:165] Memory required for data: 54360004
I0205 00:37:43.771842 10845 net.cpp:226] loss needs backward computation.
I0205 00:37:43.771849 10845 net.cpp:226] fc8 needs backward computation.
I0205 00:37:43.771855 10845 net.cpp:226] drop7 needs backward computation.
I0205 00:37:43.771862 10845 net.cpp:226] relu7 needs backward computation.
I0205 00:37:43.771867 10845 net.cpp:226] fc7 needs backward computation.
I0205 00:37:43.771872 10845 net.cpp:226] drop6 needs backward computation.
I0205 00:37:43.771878 10845 net.cpp:226] relu6 needs backward computation.
I0205 00:37:43.771883 10845 net.cpp:226] fc6 needs backward computation.
I0205 00:37:43.771889 10845 net.cpp:226] pool5 needs backward computation.
I0205 00:37:43.771898 10845 net.cpp:226] relu5 needs backward computation.
I0205 00:37:43.771903 10845 net.cpp:226] conv5 needs backward computation.
I0205 00:37:43.771909 10845 net.cpp:226] relu4 needs backward computation.
I0205 00:37:43.771915 10845 net.cpp:226] conv4 needs backward computation.
I0205 00:37:43.771921 10845 net.cpp:226] relu3 needs backward computation.
I0205 00:37:43.771927 10845 net.cpp:226] conv3 needs backward computation.
I0205 00:37:43.771939 10845 net.cpp:226] norm2 needs backward computation.
I0205 00:37:43.771946 10845 net.cpp:226] pool2 needs backward computation.
I0205 00:37:43.771952 10845 net.cpp:226] relu2 needs backward computation.
I0205 00:37:43.771958 10845 net.cpp:226] conv2 needs backward computation.
I0205 00:37:43.771965 10845 net.cpp:226] norm1 needs backward computation.
I0205 00:37:43.771970 10845 net.cpp:226] pool1 needs backward computation.
I0205 00:37:43.771976 10845 net.cpp:226] relu1 needs backward computation.
I0205 00:37:43.771982 10845 net.cpp:226] conv1 needs backward computation.
I0205 00:37:43.771988 10845 net.cpp:228] data does not need backward computation.
I0205 00:37:43.771994 10845 net.cpp:270] This network produces output loss
I0205 00:37:43.772027 10845 net.cpp:283] Network initialization done.
I0205 00:37:43.772799 10845 solver.cpp:181] Creating test net (#0) specified by net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/train_val.prototxt
I0205 00:37:43.772856 10845 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0205 00:37:43.773164 10845 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 00:37:43.773345 10845 layer_factory.hpp:77] Creating layer data
I0205 00:37:43.773505 10845 net.cpp:106] Creating Layer data
I0205 00:37:43.773519 10845 net.cpp:411] data -> data
I0205 00:37:43.773532 10845 net.cpp:411] data -> label
I0205 00:37:43.773545 10845 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto
I0205 00:37:43.773766 10850 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db
I0205 00:37:43.774540 10845 data_layer.cpp:41] output data size: 100,1,227,227
I0205 00:37:43.804291 10845 net.cpp:150] Setting up data
I0205 00:37:43.804322 10845 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 00:37:43.804330 10845 net.cpp:157] Top shape: 100 (100)
I0205 00:37:43.804337 10845 net.cpp:165] Memory required for data: 20612000
I0205 00:37:43.804347 10845 layer_factory.hpp:77] Creating layer label_data_1_split
I0205 00:37:43.804363 10845 net.cpp:106] Creating Layer label_data_1_split
I0205 00:37:43.804371 10845 net.cpp:454] label_data_1_split <- label
I0205 00:37:43.804383 10845 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0205 00:37:43.804399 10845 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0205 00:37:43.804410 10845 net.cpp:150] Setting up label_data_1_split
I0205 00:37:43.804419 10845 net.cpp:157] Top shape: 100 (100)
I0205 00:37:43.804425 10845 net.cpp:157] Top shape: 100 (100)
I0205 00:37:43.804430 10845 net.cpp:165] Memory required for data: 20612800
I0205 00:37:43.804436 10845 layer_factory.hpp:77] Creating layer conv1
I0205 00:37:43.804452 10845 net.cpp:106] Creating Layer conv1
I0205 00:37:43.804458 10845 net.cpp:454] conv1 <- data
I0205 00:37:43.804469 10845 net.cpp:411] conv1 -> conv1
I0205 00:37:43.804522 10845 net.cpp:150] Setting up conv1
I0205 00:37:43.804532 10845 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 00:37:43.804536 10845 net.cpp:165] Memory required for data: 30292800
I0205 00:37:43.804551 10845 layer_factory.hpp:77] Creating layer relu1
I0205 00:37:43.804561 10845 net.cpp:106] Creating Layer relu1
I0205 00:37:43.804568 10845 net.cpp:454] relu1 <- conv1
I0205 00:37:43.804576 10845 net.cpp:397] relu1 -> conv1 (in-place)
I0205 00:37:43.804585 10845 net.cpp:150] Setting up relu1
I0205 00:37:43.804592 10845 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 00:37:43.804599 10845 net.cpp:165] Memory required for data: 39972800
I0205 00:37:43.804603 10845 layer_factory.hpp:77] Creating layer pool1
I0205 00:37:43.804615 10845 net.cpp:106] Creating Layer pool1
I0205 00:37:43.804621 10845 net.cpp:454] pool1 <- conv1
I0205 00:37:43.804630 10845 net.cpp:411] pool1 -> pool1
I0205 00:37:43.804643 10845 net.cpp:150] Setting up pool1
I0205 00:37:43.804651 10845 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 00:37:43.804656 10845 net.cpp:165] Memory required for data: 42305600
I0205 00:37:43.804661 10845 layer_factory.hpp:77] Creating layer norm1
I0205 00:37:43.804674 10845 net.cpp:106] Creating Layer norm1
I0205 00:37:43.804680 10845 net.cpp:454] norm1 <- pool1
I0205 00:37:43.804687 10845 net.cpp:411] norm1 -> norm1
I0205 00:37:43.804700 10845 net.cpp:150] Setting up norm1
I0205 00:37:43.804708 10845 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 00:37:43.804713 10845 net.cpp:165] Memory required for data: 44638400
I0205 00:37:43.804719 10845 layer_factory.hpp:77] Creating layer conv2
I0205 00:37:43.804730 10845 net.cpp:106] Creating Layer conv2
I0205 00:37:43.804736 10845 net.cpp:454] conv2 <- norm1
I0205 00:37:43.804744 10845 net.cpp:411] conv2 -> conv2
I0205 00:37:43.804774 10845 net.cpp:150] Setting up conv2
I0205 00:37:43.804781 10845 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 00:37:43.804787 10845 net.cpp:165] Memory required for data: 46971200
I0205 00:37:43.804797 10845 layer_factory.hpp:77] Creating layer relu2
I0205 00:37:43.804806 10845 net.cpp:106] Creating Layer relu2
I0205 00:37:43.804812 10845 net.cpp:454] relu2 <- conv2
I0205 00:37:43.804822 10845 net.cpp:397] relu2 -> conv2 (in-place)
I0205 00:37:43.804841 10845 net.cpp:150] Setting up relu2
I0205 00:37:43.804858 10845 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 00:37:43.804864 10845 net.cpp:165] Memory required for data: 49304000
I0205 00:37:43.804872 10845 layer_factory.hpp:77] Creating layer pool2
I0205 00:37:43.804882 10845 net.cpp:106] Creating Layer pool2
I0205 00:37:43.804888 10845 net.cpp:454] pool2 <- conv2
I0205 00:37:43.804896 10845 net.cpp:411] pool2 -> pool2
I0205 00:37:43.804908 10845 net.cpp:150] Setting up pool2
I0205 00:37:43.804914 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.804919 10845 net.cpp:165] Memory required for data: 49844800
I0205 00:37:43.804925 10845 layer_factory.hpp:77] Creating layer norm2
I0205 00:37:43.804934 10845 net.cpp:106] Creating Layer norm2
I0205 00:37:43.804939 10845 net.cpp:454] norm2 <- pool2
I0205 00:37:43.804947 10845 net.cpp:411] norm2 -> norm2
I0205 00:37:43.804956 10845 net.cpp:150] Setting up norm2
I0205 00:37:43.804963 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.804968 10845 net.cpp:165] Memory required for data: 50385600
I0205 00:37:43.804973 10845 layer_factory.hpp:77] Creating layer conv3
I0205 00:37:43.804986 10845 net.cpp:106] Creating Layer conv3
I0205 00:37:43.804992 10845 net.cpp:454] conv3 <- norm2
I0205 00:37:43.805003 10845 net.cpp:411] conv3 -> conv3
I0205 00:37:43.805029 10845 net.cpp:150] Setting up conv3
I0205 00:37:43.805039 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.805045 10845 net.cpp:165] Memory required for data: 50926400
I0205 00:37:43.805057 10845 layer_factory.hpp:77] Creating layer relu3
I0205 00:37:43.805064 10845 net.cpp:106] Creating Layer relu3
I0205 00:37:43.805071 10845 net.cpp:454] relu3 <- conv3
I0205 00:37:43.805078 10845 net.cpp:397] relu3 -> conv3 (in-place)
I0205 00:37:43.805088 10845 net.cpp:150] Setting up relu3
I0205 00:37:43.805109 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.805114 10845 net.cpp:165] Memory required for data: 51467200
I0205 00:37:43.805119 10845 layer_factory.hpp:77] Creating layer conv4
I0205 00:37:43.805130 10845 net.cpp:106] Creating Layer conv4
I0205 00:37:43.805136 10845 net.cpp:454] conv4 <- conv3
I0205 00:37:43.805147 10845 net.cpp:411] conv4 -> conv4
I0205 00:37:43.805172 10845 net.cpp:150] Setting up conv4
I0205 00:37:43.805181 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.805186 10845 net.cpp:165] Memory required for data: 52008000
I0205 00:37:43.805193 10845 layer_factory.hpp:77] Creating layer relu4
I0205 00:37:43.805204 10845 net.cpp:106] Creating Layer relu4
I0205 00:37:43.805209 10845 net.cpp:454] relu4 <- conv4
I0205 00:37:43.805217 10845 net.cpp:397] relu4 -> conv4 (in-place)
I0205 00:37:43.805227 10845 net.cpp:150] Setting up relu4
I0205 00:37:43.805233 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.805238 10845 net.cpp:165] Memory required for data: 52548800
I0205 00:37:43.805244 10845 layer_factory.hpp:77] Creating layer conv5
I0205 00:37:43.805255 10845 net.cpp:106] Creating Layer conv5
I0205 00:37:43.805261 10845 net.cpp:454] conv5 <- conv4
I0205 00:37:43.805269 10845 net.cpp:411] conv5 -> conv5
I0205 00:37:43.805291 10845 net.cpp:150] Setting up conv5
I0205 00:37:43.805300 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.805306 10845 net.cpp:165] Memory required for data: 53089600
I0205 00:37:43.805317 10845 layer_factory.hpp:77] Creating layer relu5
I0205 00:37:43.805325 10845 net.cpp:106] Creating Layer relu5
I0205 00:37:43.805331 10845 net.cpp:454] relu5 <- conv5
I0205 00:37:43.805338 10845 net.cpp:397] relu5 -> conv5 (in-place)
I0205 00:37:43.805348 10845 net.cpp:150] Setting up relu5
I0205 00:37:43.805356 10845 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 00:37:43.805361 10845 net.cpp:165] Memory required for data: 53630400
I0205 00:37:43.805366 10845 layer_factory.hpp:77] Creating layer pool5
I0205 00:37:43.805377 10845 net.cpp:106] Creating Layer pool5
I0205 00:37:43.805382 10845 net.cpp:454] pool5 <- conv5
I0205 00:37:43.805390 10845 net.cpp:411] pool5 -> pool5
I0205 00:37:43.805408 10845 net.cpp:150] Setting up pool5
I0205 00:37:43.805424 10845 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0205 00:37:43.805429 10845 net.cpp:165] Memory required for data: 53745600
I0205 00:37:43.805435 10845 layer_factory.hpp:77] Creating layer fc6
I0205 00:37:43.805449 10845 net.cpp:106] Creating Layer fc6
I0205 00:37:43.805456 10845 net.cpp:454] fc6 <- pool5
I0205 00:37:43.805464 10845 net.cpp:411] fc6 -> fc6
I0205 00:37:43.806203 10845 net.cpp:150] Setting up fc6
I0205 00:37:43.806217 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.806222 10845 net.cpp:165] Memory required for data: 53848000
I0205 00:37:43.806231 10845 layer_factory.hpp:77] Creating layer relu6
I0205 00:37:43.806244 10845 net.cpp:106] Creating Layer relu6
I0205 00:37:43.806251 10845 net.cpp:454] relu6 <- fc6
I0205 00:37:43.806258 10845 net.cpp:397] relu6 -> fc6 (in-place)
I0205 00:37:43.806267 10845 net.cpp:150] Setting up relu6
I0205 00:37:43.806274 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.806279 10845 net.cpp:165] Memory required for data: 53950400
I0205 00:37:43.806285 10845 layer_factory.hpp:77] Creating layer drop6
I0205 00:37:43.806303 10845 net.cpp:106] Creating Layer drop6
I0205 00:37:43.806309 10845 net.cpp:454] drop6 <- fc6
I0205 00:37:43.806323 10845 net.cpp:397] drop6 -> fc6 (in-place)
I0205 00:37:43.806334 10845 net.cpp:150] Setting up drop6
I0205 00:37:43.806341 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.806346 10845 net.cpp:165] Memory required for data: 54052800
I0205 00:37:43.806352 10845 layer_factory.hpp:77] Creating layer fc7
I0205 00:37:43.806363 10845 net.cpp:106] Creating Layer fc7
I0205 00:37:43.806370 10845 net.cpp:454] fc7 <- fc6
I0205 00:37:43.806380 10845 net.cpp:411] fc7 -> fc7
I0205 00:37:43.807132 10845 net.cpp:150] Setting up fc7
I0205 00:37:43.807147 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.807152 10845 net.cpp:165] Memory required for data: 54155200
I0205 00:37:43.807160 10845 layer_factory.hpp:77] Creating layer relu7
I0205 00:37:43.807173 10845 net.cpp:106] Creating Layer relu7
I0205 00:37:43.807180 10845 net.cpp:454] relu7 <- fc7
I0205 00:37:43.807188 10845 net.cpp:397] relu7 -> fc7 (in-place)
I0205 00:37:43.807196 10845 net.cpp:150] Setting up relu7
I0205 00:37:43.807202 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.807210 10845 net.cpp:165] Memory required for data: 54257600
I0205 00:37:43.807216 10845 layer_factory.hpp:77] Creating layer drop7
I0205 00:37:43.807224 10845 net.cpp:106] Creating Layer drop7
I0205 00:37:43.807230 10845 net.cpp:454] drop7 <- fc7
I0205 00:37:43.807237 10845 net.cpp:397] drop7 -> fc7 (in-place)
I0205 00:37:43.807246 10845 net.cpp:150] Setting up drop7
I0205 00:37:43.807253 10845 net.cpp:157] Top shape: 100 256 (25600)
I0205 00:37:43.807258 10845 net.cpp:165] Memory required for data: 54360000
I0205 00:37:43.807265 10845 layer_factory.hpp:77] Creating layer fc8
I0205 00:37:43.807276 10845 net.cpp:106] Creating Layer fc8
I0205 00:37:43.807282 10845 net.cpp:454] fc8 <- fc7
I0205 00:37:43.807296 10845 net.cpp:411] fc8 -> fc8
I0205 00:37:43.807323 10845 net.cpp:150] Setting up fc8
I0205 00:37:43.807332 10845 net.cpp:157] Top shape: 100 2 (200)
I0205 00:37:43.807337 10845 net.cpp:165] Memory required for data: 54360800
I0205 00:37:43.807345 10845 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0205 00:37:43.807353 10845 net.cpp:106] Creating Layer fc8_fc8_0_split
I0205 00:37:43.807361 10845 net.cpp:454] fc8_fc8_0_split <- fc8
I0205 00:37:43.807368 10845 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0205 00:37:43.807377 10845 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0205 00:37:43.807389 10845 net.cpp:150] Setting up fc8_fc8_0_split
I0205 00:37:43.807396 10845 net.cpp:157] Top shape: 100 2 (200)
I0205 00:37:43.807402 10845 net.cpp:157] Top shape: 100 2 (200)
I0205 00:37:43.807407 10845 net.cpp:165] Memory required for data: 54362400
I0205 00:37:43.807413 10845 layer_factory.hpp:77] Creating layer accuracy
I0205 00:37:43.807426 10845 net.cpp:106] Creating Layer accuracy
I0205 00:37:43.807440 10845 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0205 00:37:43.807457 10845 net.cpp:454] accuracy <- label_data_1_split_0
I0205 00:37:43.807469 10845 net.cpp:411] accuracy -> accuracy
I0205 00:37:43.807482 10845 net.cpp:150] Setting up accuracy
I0205 00:37:43.807488 10845 net.cpp:157] Top shape: (1)
I0205 00:37:43.807494 10845 net.cpp:165] Memory required for data: 54362404
I0205 00:37:43.807499 10845 layer_factory.hpp:77] Creating layer loss
I0205 00:37:43.807509 10845 net.cpp:106] Creating Layer loss
I0205 00:37:43.807517 10845 net.cpp:454] loss <- fc8_fc8_0_split_1
I0205 00:37:43.807524 10845 net.cpp:454] loss <- label_data_1_split_1
I0205 00:37:43.807531 10845 net.cpp:411] loss -> loss
I0205 00:37:43.807543 10845 layer_factory.hpp:77] Creating layer loss
I0205 00:37:43.807569 10845 net.cpp:150] Setting up loss
I0205 00:37:43.807576 10845 net.cpp:157] Top shape: (1)
I0205 00:37:43.807581 10845 net.cpp:160]     with loss weight 1
I0205 00:37:43.807600 10845 net.cpp:165] Memory required for data: 54362408
I0205 00:37:43.807606 10845 net.cpp:226] loss needs backward computation.
I0205 00:37:43.807613 10845 net.cpp:228] accuracy does not need backward computation.
I0205 00:37:43.807621 10845 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0205 00:37:43.807627 10845 net.cpp:226] fc8 needs backward computation.
I0205 00:37:43.807633 10845 net.cpp:226] drop7 needs backward computation.
I0205 00:37:43.807639 10845 net.cpp:226] relu7 needs backward computation.
I0205 00:37:43.807644 10845 net.cpp:226] fc7 needs backward computation.
I0205 00:37:43.807651 10845 net.cpp:226] drop6 needs backward computation.
I0205 00:37:43.807657 10845 net.cpp:226] relu6 needs backward computation.
I0205 00:37:43.807663 10845 net.cpp:226] fc6 needs backward computation.
I0205 00:37:43.807669 10845 net.cpp:226] pool5 needs backward computation.
I0205 00:37:43.807675 10845 net.cpp:226] relu5 needs backward computation.
I0205 00:37:43.807682 10845 net.cpp:226] conv5 needs backward computation.
I0205 00:37:43.807687 10845 net.cpp:226] relu4 needs backward computation.
I0205 00:37:43.807693 10845 net.cpp:226] conv4 needs backward computation.
I0205 00:37:43.807698 10845 net.cpp:226] relu3 needs backward computation.
I0205 00:37:43.807704 10845 net.cpp:226] conv3 needs backward computation.
I0205 00:37:43.807710 10845 net.cpp:226] norm2 needs backward computation.
I0205 00:37:43.807716 10845 net.cpp:226] pool2 needs backward computation.
I0205 00:37:43.807723 10845 net.cpp:226] relu2 needs backward computation.
I0205 00:37:43.807729 10845 net.cpp:226] conv2 needs backward computation.
I0205 00:37:43.807734 10845 net.cpp:226] norm1 needs backward computation.
I0205 00:37:43.807739 10845 net.cpp:226] pool1 needs backward computation.
I0205 00:37:43.807747 10845 net.cpp:226] relu1 needs backward computation.
I0205 00:37:43.807754 10845 net.cpp:226] conv1 needs backward computation.
I0205 00:37:43.807760 10845 net.cpp:228] label_data_1_split does not need backward computation.
I0205 00:37:43.807767 10845 net.cpp:228] data does not need backward computation.
I0205 00:37:43.807773 10845 net.cpp:270] This network produces output accuracy
I0205 00:37:43.807780 10845 net.cpp:270] This network produces output loss
I0205 00:37:43.807809 10845 net.cpp:283] Network initialization done.
I0205 00:37:43.807917 10845 solver.cpp:60] Solver scaffolding done.
I0205 00:37:43.808003 10845 caffe.cpp:212] Starting Optimization
I0205 00:37:43.808012 10845 solver.cpp:288] Solving CaffeNet
I0205 00:37:43.808018 10845 solver.cpp:289] Learning Rate Policy: step
I0205 00:37:43.808596 10845 solver.cpp:341] Iteration 0, Testing net (#0)
I0205 00:37:43.808656 10845 blocking_queue.cpp:50] Data layer prefetch queue empty
I0205 00:37:46.262501 10845 solver.cpp:409]     Test net output #0: accuracy = 0.495
I0205 00:37:46.262568 10845 solver.cpp:409]     Test net output #1: loss = 3.11125 (* 1 = 3.11125 loss)
I0205 00:37:46.779042 10845 solver.cpp:237] Iteration 0, loss = 8.41581
I0205 00:37:46.779105 10845 solver.cpp:253]     Train net output #0: loss = 8.41581 (* 1 = 8.41581 loss)
I0205 00:37:46.779134 10845 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0205 00:37:51.597229 10845 solver.cpp:237] Iteration 10, loss = 1.58097
I0205 00:37:51.597297 10845 solver.cpp:253]     Train net output #0: loss = 1.58097 (* 1 = 1.58097 loss)
I0205 00:37:51.597309 10845 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0205 00:37:56.408674 10845 solver.cpp:237] Iteration 20, loss = 0.816421
I0205 00:37:56.408738 10845 solver.cpp:253]     Train net output #0: loss = 0.816421 (* 1 = 0.816421 loss)
I0205 00:37:56.408749 10845 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0205 00:38:01.237828 10845 solver.cpp:237] Iteration 30, loss = 0.755965
I0205 00:38:01.237896 10845 solver.cpp:253]     Train net output #0: loss = 0.755965 (* 1 = 0.755965 loss)
I0205 00:38:01.237907 10845 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0205 00:38:06.079228 10845 solver.cpp:237] Iteration 40, loss = 0.844623
I0205 00:38:06.079295 10845 solver.cpp:253]     Train net output #0: loss = 0.844623 (* 1 = 0.844623 loss)
I0205 00:38:06.079306 10845 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0205 00:38:10.913194 10845 solver.cpp:237] Iteration 50, loss = 0.902425
I0205 00:38:10.913261 10845 solver.cpp:253]     Train net output #0: loss = 0.902425 (* 1 = 0.902425 loss)
I0205 00:38:10.913274 10845 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0205 00:38:15.744557 10845 solver.cpp:237] Iteration 60, loss = 0.827065
I0205 00:38:15.744699 10845 solver.cpp:253]     Train net output #0: loss = 0.827065 (* 1 = 0.827065 loss)
I0205 00:38:15.744712 10845 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0205 00:38:20.575238 10845 solver.cpp:237] Iteration 70, loss = 0.732046
I0205 00:38:20.575314 10845 solver.cpp:253]     Train net output #0: loss = 0.732046 (* 1 = 0.732046 loss)
I0205 00:38:20.575326 10845 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0205 00:38:25.403336 10845 solver.cpp:237] Iteration 80, loss = 0.785861
I0205 00:38:25.403408 10845 solver.cpp:253]     Train net output #0: loss = 0.785861 (* 1 = 0.785861 loss)
I0205 00:38:25.403419 10845 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0205 00:38:30.230813 10845 solver.cpp:237] Iteration 90, loss = 0.770674
I0205 00:38:30.230876 10845 solver.cpp:253]     Train net output #0: loss = 0.770674 (* 1 = 0.770674 loss)
I0205 00:38:30.230888 10845 sgd_solver.cpp:106] Iteration 90, lr = 0.001
I0205 00:38:34.579912 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_100.caffemodel
I0205 00:38:34.582306 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_100.solverstate
I0205 00:38:34.583122 10845 solver.cpp:341] Iteration 100, Testing net (#0)
I0205 00:38:36.915103 10845 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:38:36.915169 10845 solver.cpp:409]     Test net output #1: loss = 0.692731 (* 1 = 0.692731 loss)
I0205 00:38:37.397814 10845 solver.cpp:237] Iteration 100, loss = 0.795357
I0205 00:38:37.397881 10845 solver.cpp:253]     Train net output #0: loss = 0.795357 (* 1 = 0.795357 loss)
I0205 00:38:37.397893 10845 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0205 00:38:42.226133 10845 solver.cpp:237] Iteration 110, loss = 0.750073
I0205 00:38:42.226207 10845 solver.cpp:253]     Train net output #0: loss = 0.750073 (* 1 = 0.750073 loss)
I0205 00:38:42.226218 10845 sgd_solver.cpp:106] Iteration 110, lr = 0.001
I0205 00:38:47.056427 10845 solver.cpp:237] Iteration 120, loss = 0.710645
I0205 00:38:47.056622 10845 solver.cpp:253]     Train net output #0: loss = 0.710645 (* 1 = 0.710645 loss)
I0205 00:38:47.056637 10845 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0205 00:38:51.887135 10845 solver.cpp:237] Iteration 130, loss = 0.742618
I0205 00:38:51.887208 10845 solver.cpp:253]     Train net output #0: loss = 0.742618 (* 1 = 0.742618 loss)
I0205 00:38:51.887219 10845 sgd_solver.cpp:106] Iteration 130, lr = 0.001
I0205 00:38:56.719328 10845 solver.cpp:237] Iteration 140, loss = 0.858099
I0205 00:38:56.719395 10845 solver.cpp:253]     Train net output #0: loss = 0.858099 (* 1 = 0.858099 loss)
I0205 00:38:56.719408 10845 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0205 00:39:01.549031 10845 solver.cpp:237] Iteration 150, loss = 0.715269
I0205 00:39:01.549104 10845 solver.cpp:253]     Train net output #0: loss = 0.715269 (* 1 = 0.715269 loss)
I0205 00:39:01.549118 10845 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0205 00:39:06.379915 10845 solver.cpp:237] Iteration 160, loss = 0.719222
I0205 00:39:06.379983 10845 solver.cpp:253]     Train net output #0: loss = 0.719222 (* 1 = 0.719222 loss)
I0205 00:39:06.379994 10845 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0205 00:39:11.207958 10845 solver.cpp:237] Iteration 170, loss = 0.754694
I0205 00:39:11.208019 10845 solver.cpp:253]     Train net output #0: loss = 0.754694 (* 1 = 0.754694 loss)
I0205 00:39:11.208030 10845 sgd_solver.cpp:106] Iteration 170, lr = 0.001
I0205 00:39:16.038081 10845 solver.cpp:237] Iteration 180, loss = 0.746769
I0205 00:39:16.038159 10845 solver.cpp:253]     Train net output #0: loss = 0.746769 (* 1 = 0.746769 loss)
I0205 00:39:16.038171 10845 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0205 00:39:20.867574 10845 solver.cpp:237] Iteration 190, loss = 0.757006
I0205 00:39:20.867848 10845 solver.cpp:253]     Train net output #0: loss = 0.757006 (* 1 = 0.757006 loss)
I0205 00:39:20.867862 10845 sgd_solver.cpp:106] Iteration 190, lr = 0.001
I0205 00:39:25.215611 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_200.caffemodel
I0205 00:39:25.217686 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_200.solverstate
I0205 00:39:25.218490 10845 solver.cpp:341] Iteration 200, Testing net (#0)
I0205 00:39:27.551187 10845 solver.cpp:409]     Test net output #0: accuracy = 0.504
I0205 00:39:27.551251 10845 solver.cpp:409]     Test net output #1: loss = 0.692742 (* 1 = 0.692742 loss)
I0205 00:39:28.033623 10845 solver.cpp:237] Iteration 200, loss = 0.663301
I0205 00:39:28.033684 10845 solver.cpp:253]     Train net output #0: loss = 0.663301 (* 1 = 0.663301 loss)
I0205 00:39:28.033695 10845 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0205 00:39:32.863976 10845 solver.cpp:237] Iteration 210, loss = 0.722489
I0205 00:39:32.864048 10845 solver.cpp:253]     Train net output #0: loss = 0.722489 (* 1 = 0.722489 loss)
I0205 00:39:32.864060 10845 sgd_solver.cpp:106] Iteration 210, lr = 0.001
I0205 00:39:37.694998 10845 solver.cpp:237] Iteration 220, loss = 0.743365
I0205 00:39:37.695065 10845 solver.cpp:253]     Train net output #0: loss = 0.743365 (* 1 = 0.743365 loss)
I0205 00:39:37.695076 10845 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0205 00:39:42.525660 10845 solver.cpp:237] Iteration 230, loss = 0.743177
I0205 00:39:42.525732 10845 solver.cpp:253]     Train net output #0: loss = 0.743177 (* 1 = 0.743177 loss)
I0205 00:39:42.525744 10845 sgd_solver.cpp:106] Iteration 230, lr = 0.001
I0205 00:39:47.356371 10845 solver.cpp:237] Iteration 240, loss = 0.684242
I0205 00:39:47.356439 10845 solver.cpp:253]     Train net output #0: loss = 0.684242 (* 1 = 0.684242 loss)
I0205 00:39:47.356451 10845 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0205 00:39:52.187049 10845 solver.cpp:237] Iteration 250, loss = 0.747386
I0205 00:39:52.187247 10845 solver.cpp:253]     Train net output #0: loss = 0.747386 (* 1 = 0.747386 loss)
I0205 00:39:52.187263 10845 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0205 00:39:57.017652 10845 solver.cpp:237] Iteration 260, loss = 0.700802
I0205 00:39:57.017715 10845 solver.cpp:253]     Train net output #0: loss = 0.700802 (* 1 = 0.700802 loss)
I0205 00:39:57.017727 10845 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0205 00:40:01.847442 10845 solver.cpp:237] Iteration 270, loss = 0.749787
I0205 00:40:01.847522 10845 solver.cpp:253]     Train net output #0: loss = 0.749787 (* 1 = 0.749787 loss)
I0205 00:40:01.847534 10845 sgd_solver.cpp:106] Iteration 270, lr = 0.001
I0205 00:40:06.681352 10845 solver.cpp:237] Iteration 280, loss = 0.753925
I0205 00:40:06.681422 10845 solver.cpp:253]     Train net output #0: loss = 0.753925 (* 1 = 0.753925 loss)
I0205 00:40:06.681434 10845 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0205 00:40:11.510468 10845 solver.cpp:237] Iteration 290, loss = 0.703137
I0205 00:40:11.510535 10845 solver.cpp:253]     Train net output #0: loss = 0.703137 (* 1 = 0.703137 loss)
I0205 00:40:11.510547 10845 sgd_solver.cpp:106] Iteration 290, lr = 0.001
I0205 00:40:15.859637 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_300.caffemodel
I0205 00:40:15.861724 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_300.solverstate
I0205 00:40:15.862524 10845 solver.cpp:341] Iteration 300, Testing net (#0)
I0205 00:40:18.194840 10845 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:40:18.194903 10845 solver.cpp:409]     Test net output #1: loss = 0.702236 (* 1 = 0.702236 loss)
I0205 00:40:18.677953 10845 solver.cpp:237] Iteration 300, loss = 0.770218
I0205 00:40:18.678014 10845 solver.cpp:253]     Train net output #0: loss = 0.770218 (* 1 = 0.770218 loss)
I0205 00:40:18.678026 10845 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0205 00:40:23.508884 10845 solver.cpp:237] Iteration 310, loss = 0.707553
I0205 00:40:23.509148 10845 solver.cpp:253]     Train net output #0: loss = 0.707553 (* 1 = 0.707553 loss)
I0205 00:40:23.509163 10845 sgd_solver.cpp:106] Iteration 310, lr = 0.001
I0205 00:40:28.339962 10845 solver.cpp:237] Iteration 320, loss = 0.69571
I0205 00:40:28.340035 10845 solver.cpp:253]     Train net output #0: loss = 0.69571 (* 1 = 0.69571 loss)
I0205 00:40:28.340047 10845 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0205 00:40:33.172399 10845 solver.cpp:237] Iteration 330, loss = 0.742666
I0205 00:40:33.172466 10845 solver.cpp:253]     Train net output #0: loss = 0.742666 (* 1 = 0.742666 loss)
I0205 00:40:33.172477 10845 sgd_solver.cpp:106] Iteration 330, lr = 0.001
I0205 00:40:38.002931 10845 solver.cpp:237] Iteration 340, loss = 0.677954
I0205 00:40:38.002993 10845 solver.cpp:253]     Train net output #0: loss = 0.677954 (* 1 = 0.677954 loss)
I0205 00:40:38.003005 10845 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0205 00:40:42.833324 10845 solver.cpp:237] Iteration 350, loss = 0.71061
I0205 00:40:42.833397 10845 solver.cpp:253]     Train net output #0: loss = 0.71061 (* 1 = 0.71061 loss)
I0205 00:40:42.833410 10845 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I0205 00:40:47.666482 10845 solver.cpp:237] Iteration 360, loss = 0.71484
I0205 00:40:47.666548 10845 solver.cpp:253]     Train net output #0: loss = 0.71484 (* 1 = 0.71484 loss)
I0205 00:40:47.666561 10845 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0205 00:40:52.496459 10845 solver.cpp:237] Iteration 370, loss = 0.691375
I0205 00:40:52.496528 10845 solver.cpp:253]     Train net output #0: loss = 0.691375 (* 1 = 0.691375 loss)
I0205 00:40:52.496541 10845 sgd_solver.cpp:106] Iteration 370, lr = 0.001
I0205 00:40:57.324396 10845 solver.cpp:237] Iteration 380, loss = 0.748794
I0205 00:40:57.324615 10845 solver.cpp:253]     Train net output #0: loss = 0.748794 (* 1 = 0.748794 loss)
I0205 00:40:57.324630 10845 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0205 00:41:02.156437 10845 solver.cpp:237] Iteration 390, loss = 0.717561
I0205 00:41:02.156497 10845 solver.cpp:253]     Train net output #0: loss = 0.717561 (* 1 = 0.717561 loss)
I0205 00:41:02.156509 10845 sgd_solver.cpp:106] Iteration 390, lr = 0.001
I0205 00:41:06.505908 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_400.caffemodel
I0205 00:41:06.508014 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_400.solverstate
I0205 00:41:06.508805 10845 solver.cpp:341] Iteration 400, Testing net (#0)
I0205 00:41:08.847934 10845 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:41:08.847996 10845 solver.cpp:409]     Test net output #1: loss = 0.698856 (* 1 = 0.698856 loss)
I0205 00:41:09.330531 10845 solver.cpp:237] Iteration 400, loss = 0.691905
I0205 00:41:09.330598 10845 solver.cpp:253]     Train net output #0: loss = 0.691905 (* 1 = 0.691905 loss)
I0205 00:41:09.330610 10845 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0205 00:41:14.165360 10845 solver.cpp:237] Iteration 410, loss = 0.765365
I0205 00:41:14.165429 10845 solver.cpp:253]     Train net output #0: loss = 0.765365 (* 1 = 0.765365 loss)
I0205 00:41:14.165441 10845 sgd_solver.cpp:106] Iteration 410, lr = 0.001
I0205 00:41:18.996161 10845 solver.cpp:237] Iteration 420, loss = 0.713446
I0205 00:41:18.996233 10845 solver.cpp:253]     Train net output #0: loss = 0.713446 (* 1 = 0.713446 loss)
I0205 00:41:18.996245 10845 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0205 00:41:23.826021 10845 solver.cpp:237] Iteration 430, loss = 0.720713
I0205 00:41:23.826086 10845 solver.cpp:253]     Train net output #0: loss = 0.720713 (* 1 = 0.720713 loss)
I0205 00:41:23.826107 10845 sgd_solver.cpp:106] Iteration 430, lr = 0.001
I0205 00:41:28.659453 10845 solver.cpp:237] Iteration 440, loss = 0.707997
I0205 00:41:28.659678 10845 solver.cpp:253]     Train net output #0: loss = 0.707997 (* 1 = 0.707997 loss)
I0205 00:41:28.659693 10845 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0205 00:41:33.495167 10845 solver.cpp:237] Iteration 450, loss = 0.730261
I0205 00:41:33.495235 10845 solver.cpp:253]     Train net output #0: loss = 0.730261 (* 1 = 0.730261 loss)
I0205 00:41:33.495249 10845 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I0205 00:41:38.328930 10845 solver.cpp:237] Iteration 460, loss = 0.759905
I0205 00:41:38.328996 10845 solver.cpp:253]     Train net output #0: loss = 0.759905 (* 1 = 0.759905 loss)
I0205 00:41:38.329010 10845 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0205 00:41:43.166180 10845 solver.cpp:237] Iteration 470, loss = 0.706772
I0205 00:41:43.166250 10845 solver.cpp:253]     Train net output #0: loss = 0.706772 (* 1 = 0.706772 loss)
I0205 00:41:43.166262 10845 sgd_solver.cpp:106] Iteration 470, lr = 0.001
I0205 00:41:47.999049 10845 solver.cpp:237] Iteration 480, loss = 0.684981
I0205 00:41:47.999130 10845 solver.cpp:253]     Train net output #0: loss = 0.684981 (* 1 = 0.684981 loss)
I0205 00:41:47.999143 10845 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0205 00:41:52.833753 10845 solver.cpp:237] Iteration 490, loss = 0.716895
I0205 00:41:52.833819 10845 solver.cpp:253]     Train net output #0: loss = 0.716895 (* 1 = 0.716895 loss)
I0205 00:41:52.833832 10845 sgd_solver.cpp:106] Iteration 490, lr = 0.001
I0205 00:41:57.183624 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_500.caffemodel
I0205 00:41:57.185752 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_500.solverstate
I0205 00:41:57.186561 10845 solver.cpp:341] Iteration 500, Testing net (#0)
I0205 00:41:59.519474 10845 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:41:59.519661 10845 solver.cpp:409]     Test net output #1: loss = 0.700756 (* 1 = 0.700756 loss)
I0205 00:42:00.002027 10845 solver.cpp:237] Iteration 500, loss = 0.703692
I0205 00:42:00.002097 10845 solver.cpp:253]     Train net output #0: loss = 0.703692 (* 1 = 0.703692 loss)
I0205 00:42:00.002111 10845 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0205 00:42:04.832881 10845 solver.cpp:237] Iteration 510, loss = 0.74322
I0205 00:42:04.832950 10845 solver.cpp:253]     Train net output #0: loss = 0.74322 (* 1 = 0.74322 loss)
I0205 00:42:04.832963 10845 sgd_solver.cpp:106] Iteration 510, lr = 0.001
I0205 00:42:09.661530 10845 solver.cpp:237] Iteration 520, loss = 0.706759
I0205 00:42:09.661598 10845 solver.cpp:253]     Train net output #0: loss = 0.706759 (* 1 = 0.706759 loss)
I0205 00:42:09.661610 10845 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I0205 00:42:14.498306 10845 solver.cpp:237] Iteration 530, loss = 0.713847
I0205 00:42:14.498380 10845 solver.cpp:253]     Train net output #0: loss = 0.713847 (* 1 = 0.713847 loss)
I0205 00:42:14.498392 10845 sgd_solver.cpp:106] Iteration 530, lr = 0.001
I0205 00:42:19.326539 10845 solver.cpp:237] Iteration 540, loss = 0.755006
I0205 00:42:19.326608 10845 solver.cpp:253]     Train net output #0: loss = 0.755006 (* 1 = 0.755006 loss)
I0205 00:42:19.326620 10845 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I0205 00:42:24.156484 10845 solver.cpp:237] Iteration 550, loss = 0.747606
I0205 00:42:24.156551 10845 solver.cpp:253]     Train net output #0: loss = 0.747606 (* 1 = 0.747606 loss)
I0205 00:42:24.156563 10845 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I0205 00:42:28.989778 10845 solver.cpp:237] Iteration 560, loss = 0.734639
I0205 00:42:28.989848 10845 solver.cpp:253]     Train net output #0: loss = 0.734639 (* 1 = 0.734639 loss)
I0205 00:42:28.989861 10845 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I0205 00:42:33.820150 10845 solver.cpp:237] Iteration 570, loss = 0.747038
I0205 00:42:33.820404 10845 solver.cpp:253]     Train net output #0: loss = 0.747038 (* 1 = 0.747038 loss)
I0205 00:42:33.820417 10845 sgd_solver.cpp:106] Iteration 570, lr = 0.001
I0205 00:42:38.655150 10845 solver.cpp:237] Iteration 580, loss = 0.686773
I0205 00:42:38.655220 10845 solver.cpp:253]     Train net output #0: loss = 0.686773 (* 1 = 0.686773 loss)
I0205 00:42:38.655231 10845 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I0205 00:42:43.489151 10845 solver.cpp:237] Iteration 590, loss = 0.708902
I0205 00:42:43.489223 10845 solver.cpp:253]     Train net output #0: loss = 0.708902 (* 1 = 0.708902 loss)
I0205 00:42:43.489235 10845 sgd_solver.cpp:106] Iteration 590, lr = 0.001
I0205 00:42:47.837738 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_600.caffemodel
I0205 00:42:47.839828 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_600.solverstate
I0205 00:42:47.840637 10845 solver.cpp:341] Iteration 600, Testing net (#0)
I0205 00:42:50.174231 10845 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:42:50.174300 10845 solver.cpp:409]     Test net output #1: loss = 0.693574 (* 1 = 0.693574 loss)
I0205 00:42:50.657373 10845 solver.cpp:237] Iteration 600, loss = 0.710148
I0205 00:42:50.657438 10845 solver.cpp:253]     Train net output #0: loss = 0.710148 (* 1 = 0.710148 loss)
I0205 00:42:50.657450 10845 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0205 00:42:55.489243 10845 solver.cpp:237] Iteration 610, loss = 0.682606
I0205 00:42:55.489313 10845 solver.cpp:253]     Train net output #0: loss = 0.682606 (* 1 = 0.682606 loss)
I0205 00:42:55.489326 10845 sgd_solver.cpp:106] Iteration 610, lr = 0.001
I0205 00:43:00.320266 10845 solver.cpp:237] Iteration 620, loss = 0.721945
I0205 00:43:00.320333 10845 solver.cpp:253]     Train net output #0: loss = 0.721945 (* 1 = 0.721945 loss)
I0205 00:43:00.320345 10845 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I0205 00:43:05.149734 10845 solver.cpp:237] Iteration 630, loss = 0.716583
I0205 00:43:05.149966 10845 solver.cpp:253]     Train net output #0: loss = 0.716583 (* 1 = 0.716583 loss)
I0205 00:43:05.149981 10845 sgd_solver.cpp:106] Iteration 630, lr = 0.001
I0205 00:43:09.984391 10845 solver.cpp:237] Iteration 640, loss = 0.686661
I0205 00:43:09.984477 10845 solver.cpp:253]     Train net output #0: loss = 0.686661 (* 1 = 0.686661 loss)
I0205 00:43:09.984488 10845 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I0205 00:43:14.815922 10845 solver.cpp:237] Iteration 650, loss = 0.70797
I0205 00:43:14.815986 10845 solver.cpp:253]     Train net output #0: loss = 0.70797 (* 1 = 0.70797 loss)
I0205 00:43:14.815997 10845 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I0205 00:43:19.646617 10845 solver.cpp:237] Iteration 660, loss = 0.685082
I0205 00:43:19.646680 10845 solver.cpp:253]     Train net output #0: loss = 0.685082 (* 1 = 0.685082 loss)
I0205 00:43:19.646692 10845 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I0205 00:43:24.504061 10845 solver.cpp:237] Iteration 670, loss = 0.717802
I0205 00:43:24.504132 10845 solver.cpp:253]     Train net output #0: loss = 0.717802 (* 1 = 0.717802 loss)
I0205 00:43:24.504145 10845 sgd_solver.cpp:106] Iteration 670, lr = 0.001
I0205 00:43:29.342027 10845 solver.cpp:237] Iteration 680, loss = 0.69996
I0205 00:43:29.342098 10845 solver.cpp:253]     Train net output #0: loss = 0.69996 (* 1 = 0.69996 loss)
I0205 00:43:29.342111 10845 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I0205 00:43:34.174021 10845 solver.cpp:237] Iteration 690, loss = 0.698857
I0205 00:43:34.174085 10845 solver.cpp:253]     Train net output #0: loss = 0.698857 (* 1 = 0.698857 loss)
I0205 00:43:34.174103 10845 sgd_solver.cpp:106] Iteration 690, lr = 0.001
I0205 00:43:38.521679 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_700.caffemodel
I0205 00:43:38.523960 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_700.solverstate
I0205 00:43:38.524758 10845 solver.cpp:341] Iteration 700, Testing net (#0)
I0205 00:43:40.856688 10845 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:43:40.856753 10845 solver.cpp:409]     Test net output #1: loss = 0.691325 (* 1 = 0.691325 loss)
I0205 00:43:41.339823 10845 solver.cpp:237] Iteration 700, loss = 0.725107
I0205 00:43:41.339886 10845 solver.cpp:253]     Train net output #0: loss = 0.725107 (* 1 = 0.725107 loss)
I0205 00:43:41.339898 10845 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0205 00:43:46.173429 10845 solver.cpp:237] Iteration 710, loss = 0.716952
I0205 00:43:46.173499 10845 solver.cpp:253]     Train net output #0: loss = 0.716952 (* 1 = 0.716952 loss)
I0205 00:43:46.173511 10845 sgd_solver.cpp:106] Iteration 710, lr = 0.001
I0205 00:43:51.003995 10845 solver.cpp:237] Iteration 720, loss = 0.671691
I0205 00:43:51.004060 10845 solver.cpp:253]     Train net output #0: loss = 0.671691 (* 1 = 0.671691 loss)
I0205 00:43:51.004072 10845 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I0205 00:43:55.847302 10845 solver.cpp:237] Iteration 730, loss = 0.726567
I0205 00:43:55.847373 10845 solver.cpp:253]     Train net output #0: loss = 0.726567 (* 1 = 0.726567 loss)
I0205 00:43:55.847384 10845 sgd_solver.cpp:106] Iteration 730, lr = 0.001
I0205 00:44:00.684011 10845 solver.cpp:237] Iteration 740, loss = 0.687117
I0205 00:44:00.684087 10845 solver.cpp:253]     Train net output #0: loss = 0.687117 (* 1 = 0.687117 loss)
I0205 00:44:00.684106 10845 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I0205 00:44:05.518568 10845 solver.cpp:237] Iteration 750, loss = 0.706765
I0205 00:44:05.518632 10845 solver.cpp:253]     Train net output #0: loss = 0.706765 (* 1 = 0.706765 loss)
I0205 00:44:05.518645 10845 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I0205 00:44:10.351359 10845 solver.cpp:237] Iteration 760, loss = 0.684264
I0205 00:44:10.351568 10845 solver.cpp:253]     Train net output #0: loss = 0.684264 (* 1 = 0.684264 loss)
I0205 00:44:10.351584 10845 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I0205 00:44:15.185926 10845 solver.cpp:237] Iteration 770, loss = 0.697243
I0205 00:44:15.185997 10845 solver.cpp:253]     Train net output #0: loss = 0.697243 (* 1 = 0.697243 loss)
I0205 00:44:15.186022 10845 sgd_solver.cpp:106] Iteration 770, lr = 0.001
I0205 00:44:20.023216 10845 solver.cpp:237] Iteration 780, loss = 0.683443
I0205 00:44:20.023284 10845 solver.cpp:253]     Train net output #0: loss = 0.683443 (* 1 = 0.683443 loss)
I0205 00:44:20.023296 10845 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I0205 00:44:24.860587 10845 solver.cpp:237] Iteration 790, loss = 0.612325
I0205 00:44:24.860656 10845 solver.cpp:253]     Train net output #0: loss = 0.612325 (* 1 = 0.612325 loss)
I0205 00:44:24.860666 10845 sgd_solver.cpp:106] Iteration 790, lr = 0.001
I0205 00:44:29.208464 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_800.caffemodel
I0205 00:44:29.210541 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_800.solverstate
I0205 00:44:29.211338 10845 solver.cpp:341] Iteration 800, Testing net (#0)
I0205 00:44:31.544348 10845 solver.cpp:409]     Test net output #0: accuracy = 0.757
I0205 00:44:31.544406 10845 solver.cpp:409]     Test net output #1: loss = 0.556109 (* 1 = 0.556109 loss)
I0205 00:44:32.027200 10845 solver.cpp:237] Iteration 800, loss = 0.672433
I0205 00:44:32.027262 10845 solver.cpp:253]     Train net output #0: loss = 0.672433 (* 1 = 0.672433 loss)
I0205 00:44:32.027274 10845 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0205 00:44:36.873456 10845 solver.cpp:237] Iteration 810, loss = 0.494471
I0205 00:44:36.873525 10845 solver.cpp:253]     Train net output #0: loss = 0.494471 (* 1 = 0.494471 loss)
I0205 00:44:36.873538 10845 sgd_solver.cpp:106] Iteration 810, lr = 0.001
I0205 00:44:41.706030 10845 solver.cpp:237] Iteration 820, loss = 0.480955
I0205 00:44:41.706305 10845 solver.cpp:253]     Train net output #0: loss = 0.480955 (* 1 = 0.480955 loss)
I0205 00:44:41.706321 10845 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I0205 00:44:46.538614 10845 solver.cpp:237] Iteration 830, loss = 0.509813
I0205 00:44:46.538681 10845 solver.cpp:253]     Train net output #0: loss = 0.509813 (* 1 = 0.509813 loss)
I0205 00:44:46.538692 10845 sgd_solver.cpp:106] Iteration 830, lr = 0.001
I0205 00:44:51.375783 10845 solver.cpp:237] Iteration 840, loss = 0.536428
I0205 00:44:51.375854 10845 solver.cpp:253]     Train net output #0: loss = 0.536428 (* 1 = 0.536428 loss)
I0205 00:44:51.375865 10845 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I0205 00:44:56.205941 10845 solver.cpp:237] Iteration 850, loss = 0.385987
I0205 00:44:56.206008 10845 solver.cpp:253]     Train net output #0: loss = 0.385987 (* 1 = 0.385987 loss)
I0205 00:44:56.206020 10845 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I0205 00:45:01.035915 10845 solver.cpp:237] Iteration 860, loss = 0.28138
I0205 00:45:01.035982 10845 solver.cpp:253]     Train net output #0: loss = 0.28138 (* 1 = 0.28138 loss)
I0205 00:45:01.035995 10845 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I0205 00:45:05.869907 10845 solver.cpp:237] Iteration 870, loss = 0.173071
I0205 00:45:05.869978 10845 solver.cpp:253]     Train net output #0: loss = 0.173071 (* 1 = 0.173071 loss)
I0205 00:45:05.869992 10845 sgd_solver.cpp:106] Iteration 870, lr = 0.001
I0205 00:45:10.702430 10845 solver.cpp:237] Iteration 880, loss = 0.211294
I0205 00:45:10.702493 10845 solver.cpp:253]     Train net output #0: loss = 0.211294 (* 1 = 0.211294 loss)
I0205 00:45:10.702505 10845 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I0205 00:45:15.529947 10845 solver.cpp:237] Iteration 890, loss = 0.285854
I0205 00:45:15.530164 10845 solver.cpp:253]     Train net output #0: loss = 0.285854 (* 1 = 0.285854 loss)
I0205 00:45:15.530179 10845 sgd_solver.cpp:106] Iteration 890, lr = 0.001
I0205 00:45:19.879801 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_900.caffemodel
I0205 00:45:19.882627 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_900.solverstate
I0205 00:45:19.883431 10845 solver.cpp:341] Iteration 900, Testing net (#0)
I0205 00:45:22.215035 10845 solver.cpp:409]     Test net output #0: accuracy = 0.895
I0205 00:45:22.215103 10845 solver.cpp:409]     Test net output #1: loss = 0.265355 (* 1 = 0.265355 loss)
I0205 00:45:22.699213 10845 solver.cpp:237] Iteration 900, loss = 0.342471
I0205 00:45:22.699278 10845 solver.cpp:253]     Train net output #0: loss = 0.342471 (* 1 = 0.342471 loss)
I0205 00:45:22.699290 10845 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0205 00:45:27.542984 10845 solver.cpp:237] Iteration 910, loss = 0.209314
I0205 00:45:27.543052 10845 solver.cpp:253]     Train net output #0: loss = 0.209314 (* 1 = 0.209314 loss)
I0205 00:45:27.543064 10845 sgd_solver.cpp:106] Iteration 910, lr = 0.001
I0205 00:45:32.368762 10845 solver.cpp:237] Iteration 920, loss = 0.193749
I0205 00:45:32.368831 10845 solver.cpp:253]     Train net output #0: loss = 0.193749 (* 1 = 0.193749 loss)
I0205 00:45:32.368844 10845 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I0205 00:45:37.196595 10845 solver.cpp:237] Iteration 930, loss = 0.0936323
I0205 00:45:37.196656 10845 solver.cpp:253]     Train net output #0: loss = 0.0936323 (* 1 = 0.0936323 loss)
I0205 00:45:37.196668 10845 sgd_solver.cpp:106] Iteration 930, lr = 0.001
I0205 00:45:42.024538 10845 solver.cpp:237] Iteration 940, loss = 0.0581494
I0205 00:45:42.024605 10845 solver.cpp:253]     Train net output #0: loss = 0.0581494 (* 1 = 0.0581494 loss)
I0205 00:45:42.024616 10845 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I0205 00:45:46.866547 10845 solver.cpp:237] Iteration 950, loss = 0.0879115
I0205 00:45:47.416299 10845 solver.cpp:253]     Train net output #0: loss = 0.0879115 (* 1 = 0.0879115 loss)
I0205 00:45:47.416333 10845 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I0205 00:45:52.263010 10845 solver.cpp:237] Iteration 960, loss = 0.2038
I0205 00:45:52.263078 10845 solver.cpp:253]     Train net output #0: loss = 0.2038 (* 1 = 0.2038 loss)
I0205 00:45:52.263097 10845 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I0205 00:45:57.095517 10845 solver.cpp:237] Iteration 970, loss = 0.253455
I0205 00:45:57.095582 10845 solver.cpp:253]     Train net output #0: loss = 0.253455 (* 1 = 0.253455 loss)
I0205 00:45:57.095592 10845 sgd_solver.cpp:106] Iteration 970, lr = 0.001
I0205 00:46:01.964998 10845 solver.cpp:237] Iteration 980, loss = 0.0905726
I0205 00:46:01.965070 10845 solver.cpp:253]     Train net output #0: loss = 0.0905726 (* 1 = 0.0905726 loss)
I0205 00:46:01.965081 10845 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I0205 00:46:06.799399 10845 solver.cpp:237] Iteration 990, loss = 0.105122
I0205 00:46:06.799471 10845 solver.cpp:253]     Train net output #0: loss = 0.105122 (* 1 = 0.105122 loss)
I0205 00:46:06.799484 10845 sgd_solver.cpp:106] Iteration 990, lr = 0.001
I0205 00:46:11.148418 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1000.caffemodel
I0205 00:46:11.150490 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1000.solverstate
I0205 00:46:11.151286 10845 solver.cpp:341] Iteration 1000, Testing net (#0)
I0205 00:46:13.482753 10845 solver.cpp:409]     Test net output #0: accuracy = 0.984
I0205 00:46:13.482815 10845 solver.cpp:409]     Test net output #1: loss = 0.0436898 (* 1 = 0.0436898 loss)
I0205 00:46:13.965762 10845 solver.cpp:237] Iteration 1000, loss = 0.124542
I0205 00:46:13.965828 10845 solver.cpp:253]     Train net output #0: loss = 0.124542 (* 1 = 0.124542 loss)
I0205 00:46:13.965842 10845 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0205 00:46:18.792925 10845 solver.cpp:237] Iteration 1010, loss = 1.01883
I0205 00:46:18.801290 10845 solver.cpp:253]     Train net output #0: loss = 1.01883 (* 1 = 1.01883 loss)
I0205 00:46:18.801322 10845 sgd_solver.cpp:106] Iteration 1010, lr = 0.001
I0205 00:46:23.627723 10845 solver.cpp:237] Iteration 1020, loss = 0.520469
I0205 00:46:23.627794 10845 solver.cpp:253]     Train net output #0: loss = 0.520469 (* 1 = 0.520469 loss)
I0205 00:46:23.627806 10845 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I0205 00:46:28.449713 10845 solver.cpp:237] Iteration 1030, loss = 0.177975
I0205 00:46:28.449784 10845 solver.cpp:253]     Train net output #0: loss = 0.177975 (* 1 = 0.177975 loss)
I0205 00:46:28.449795 10845 sgd_solver.cpp:106] Iteration 1030, lr = 0.001
I0205 00:46:33.273154 10845 solver.cpp:237] Iteration 1040, loss = 0.147013
I0205 00:46:33.273222 10845 solver.cpp:253]     Train net output #0: loss = 0.147013 (* 1 = 0.147013 loss)
I0205 00:46:33.273234 10845 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I0205 00:46:38.099355 10845 solver.cpp:237] Iteration 1050, loss = 0.252962
I0205 00:46:38.099423 10845 solver.cpp:253]     Train net output #0: loss = 0.252962 (* 1 = 0.252962 loss)
I0205 00:46:38.099434 10845 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I0205 00:46:42.926625 10845 solver.cpp:237] Iteration 1060, loss = 0.0695528
I0205 00:46:42.926693 10845 solver.cpp:253]     Train net output #0: loss = 0.0695529 (* 1 = 0.0695529 loss)
I0205 00:46:42.926705 10845 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I0205 00:46:47.756196 10845 solver.cpp:237] Iteration 1070, loss = 0.128496
I0205 00:46:47.756263 10845 solver.cpp:253]     Train net output #0: loss = 0.128496 (* 1 = 0.128496 loss)
I0205 00:46:47.756275 10845 sgd_solver.cpp:106] Iteration 1070, lr = 0.001
I0205 00:46:52.583009 10845 solver.cpp:237] Iteration 1080, loss = 0.0328437
I0205 00:46:52.583225 10845 solver.cpp:253]     Train net output #0: loss = 0.0328438 (* 1 = 0.0328438 loss)
I0205 00:46:52.583240 10845 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I0205 00:46:57.411631 10845 solver.cpp:237] Iteration 1090, loss = 0.0356061
I0205 00:46:57.411696 10845 solver.cpp:253]     Train net output #0: loss = 0.0356062 (* 1 = 0.0356062 loss)
I0205 00:46:57.411710 10845 sgd_solver.cpp:106] Iteration 1090, lr = 0.001
I0205 00:47:01.788007 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1100.caffemodel
I0205 00:47:01.790083 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1100.solverstate
I0205 00:47:01.790892 10845 solver.cpp:341] Iteration 1100, Testing net (#0)
I0205 00:47:04.148491 10845 solver.cpp:409]     Test net output #0: accuracy = 0.991
I0205 00:47:04.148552 10845 solver.cpp:409]     Test net output #1: loss = 0.0243876 (* 1 = 0.0243876 loss)
I0205 00:47:04.632588 10845 solver.cpp:237] Iteration 1100, loss = 0.0427201
I0205 00:47:04.632652 10845 solver.cpp:253]     Train net output #0: loss = 0.0427202 (* 1 = 0.0427202 loss)
I0205 00:47:04.632664 10845 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I0205 00:47:09.461946 10845 solver.cpp:237] Iteration 1110, loss = 0.0480559
I0205 00:47:09.462013 10845 solver.cpp:253]     Train net output #0: loss = 0.0480561 (* 1 = 0.0480561 loss)
I0205 00:47:09.462026 10845 sgd_solver.cpp:106] Iteration 1110, lr = 0.001
I0205 00:47:14.292146 10845 solver.cpp:237] Iteration 1120, loss = 0.0366121
I0205 00:47:14.292215 10845 solver.cpp:253]     Train net output #0: loss = 0.0366122 (* 1 = 0.0366122 loss)
I0205 00:47:14.292227 10845 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I0205 00:47:19.122467 10845 solver.cpp:237] Iteration 1130, loss = 0.0401704
I0205 00:47:19.122539 10845 solver.cpp:253]     Train net output #0: loss = 0.0401705 (* 1 = 0.0401705 loss)
I0205 00:47:19.122551 10845 sgd_solver.cpp:106] Iteration 1130, lr = 0.001
I0205 00:47:23.946707 10845 solver.cpp:237] Iteration 1140, loss = 0.0220759
I0205 00:47:23.946954 10845 solver.cpp:253]     Train net output #0: loss = 0.0220761 (* 1 = 0.0220761 loss)
I0205 00:47:23.946969 10845 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I0205 00:47:28.775111 10845 solver.cpp:237] Iteration 1150, loss = 0.0439652
I0205 00:47:28.775182 10845 solver.cpp:253]     Train net output #0: loss = 0.0439653 (* 1 = 0.0439653 loss)
I0205 00:47:28.775194 10845 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I0205 00:47:33.600993 10845 solver.cpp:237] Iteration 1160, loss = 0.0535537
I0205 00:47:33.601063 10845 solver.cpp:253]     Train net output #0: loss = 0.0535538 (* 1 = 0.0535538 loss)
I0205 00:47:33.601074 10845 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I0205 00:47:38.427650 10845 solver.cpp:237] Iteration 1170, loss = 0.0452524
I0205 00:47:38.427717 10845 solver.cpp:253]     Train net output #0: loss = 0.0452525 (* 1 = 0.0452525 loss)
I0205 00:47:38.427729 10845 sgd_solver.cpp:106] Iteration 1170, lr = 0.001
I0205 00:47:43.254367 10845 solver.cpp:237] Iteration 1180, loss = 0.05469
I0205 00:47:43.254439 10845 solver.cpp:253]     Train net output #0: loss = 0.0546901 (* 1 = 0.0546901 loss)
I0205 00:47:43.254451 10845 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I0205 00:47:48.081573 10845 solver.cpp:237] Iteration 1190, loss = 0.00414441
I0205 00:47:48.081640 10845 solver.cpp:253]     Train net output #0: loss = 0.00414453 (* 1 = 0.00414453 loss)
I0205 00:47:48.081652 10845 sgd_solver.cpp:106] Iteration 1190, lr = 0.001
I0205 00:47:52.427314 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1200.caffemodel
I0205 00:47:52.429544 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1200.solverstate
I0205 00:47:52.430356 10845 solver.cpp:341] Iteration 1200, Testing net (#0)
I0205 00:47:54.766937 10845 solver.cpp:409]     Test net output #0: accuracy = 0.993
I0205 00:47:54.767166 10845 solver.cpp:409]     Test net output #1: loss = 0.0244523 (* 1 = 0.0244523 loss)
I0205 00:47:55.250635 10845 solver.cpp:237] Iteration 1200, loss = 0.00687899
I0205 00:47:55.250705 10845 solver.cpp:253]     Train net output #0: loss = 0.00687912 (* 1 = 0.00687912 loss)
I0205 00:47:55.250716 10845 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0205 00:48:00.085393 10845 solver.cpp:237] Iteration 1210, loss = 0.0990002
I0205 00:48:00.085459 10845 solver.cpp:253]     Train net output #0: loss = 0.0990003 (* 1 = 0.0990003 loss)
I0205 00:48:00.085471 10845 sgd_solver.cpp:106] Iteration 1210, lr = 0.001
I0205 00:48:04.915787 10845 solver.cpp:237] Iteration 1220, loss = 0.00531088
I0205 00:48:04.915859 10845 solver.cpp:253]     Train net output #0: loss = 0.00531101 (* 1 = 0.00531101 loss)
I0205 00:48:04.915871 10845 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I0205 00:48:09.746419 10845 solver.cpp:237] Iteration 1230, loss = 0.105106
I0205 00:48:09.746485 10845 solver.cpp:253]     Train net output #0: loss = 0.105106 (* 1 = 0.105106 loss)
I0205 00:48:09.746498 10845 sgd_solver.cpp:106] Iteration 1230, lr = 0.001
I0205 00:48:14.575508 10845 solver.cpp:237] Iteration 1240, loss = 0.107591
I0205 00:48:14.575574 10845 solver.cpp:253]     Train net output #0: loss = 0.107591 (* 1 = 0.107591 loss)
I0205 00:48:14.575587 10845 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I0205 00:48:19.404463 10845 solver.cpp:237] Iteration 1250, loss = 0.0746523
I0205 00:48:19.404536 10845 solver.cpp:253]     Train net output #0: loss = 0.0746525 (* 1 = 0.0746525 loss)
I0205 00:48:19.404548 10845 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I0205 00:48:24.235043 10845 solver.cpp:237] Iteration 1260, loss = 0.0230285
I0205 00:48:24.235121 10845 solver.cpp:253]     Train net output #0: loss = 0.0230286 (* 1 = 0.0230286 loss)
I0205 00:48:24.235133 10845 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I0205 00:48:29.062718 10845 solver.cpp:237] Iteration 1270, loss = 0.00732433
I0205 00:48:29.062953 10845 solver.cpp:253]     Train net output #0: loss = 0.00732447 (* 1 = 0.00732447 loss)
I0205 00:48:29.062968 10845 sgd_solver.cpp:106] Iteration 1270, lr = 0.001
I0205 00:48:33.890303 10845 solver.cpp:237] Iteration 1280, loss = 0.0290298
I0205 00:48:33.890372 10845 solver.cpp:253]     Train net output #0: loss = 0.0290299 (* 1 = 0.0290299 loss)
I0205 00:48:33.890385 10845 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I0205 00:48:38.718461 10845 solver.cpp:237] Iteration 1290, loss = 0.0607828
I0205 00:48:38.718526 10845 solver.cpp:253]     Train net output #0: loss = 0.060783 (* 1 = 0.060783 loss)
I0205 00:48:38.718538 10845 sgd_solver.cpp:106] Iteration 1290, lr = 0.001
I0205 00:48:43.069439 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1300.caffemodel
I0205 00:48:43.071527 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1300.solverstate
I0205 00:48:43.072329 10845 solver.cpp:341] Iteration 1300, Testing net (#0)
I0205 00:48:45.404904 10845 solver.cpp:409]     Test net output #0: accuracy = 0.994
I0205 00:48:45.404964 10845 solver.cpp:409]     Test net output #1: loss = 0.0178884 (* 1 = 0.0178884 loss)
I0205 00:48:45.887300 10845 solver.cpp:237] Iteration 1300, loss = 0.0493087
I0205 00:48:45.887367 10845 solver.cpp:253]     Train net output #0: loss = 0.0493089 (* 1 = 0.0493089 loss)
I0205 00:48:45.887378 10845 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I0205 00:48:50.718271 10845 solver.cpp:237] Iteration 1310, loss = 0.0104441
I0205 00:48:50.718344 10845 solver.cpp:253]     Train net output #0: loss = 0.0104443 (* 1 = 0.0104443 loss)
I0205 00:48:50.718356 10845 sgd_solver.cpp:106] Iteration 1310, lr = 0.001
I0205 00:48:55.549373 10845 solver.cpp:237] Iteration 1320, loss = 0.0188958
I0205 00:48:55.549439 10845 solver.cpp:253]     Train net output #0: loss = 0.018896 (* 1 = 0.018896 loss)
I0205 00:48:55.549453 10845 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I0205 00:49:00.383874 10845 solver.cpp:237] Iteration 1330, loss = 0.0939015
I0205 00:49:00.384088 10845 solver.cpp:253]     Train net output #0: loss = 0.0939016 (* 1 = 0.0939016 loss)
I0205 00:49:00.384112 10845 sgd_solver.cpp:106] Iteration 1330, lr = 0.001
I0205 00:49:05.225595 10845 solver.cpp:237] Iteration 1340, loss = 0.0214122
I0205 00:49:05.225666 10845 solver.cpp:253]     Train net output #0: loss = 0.0214123 (* 1 = 0.0214123 loss)
I0205 00:49:05.225678 10845 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I0205 00:49:10.060631 10845 solver.cpp:237] Iteration 1350, loss = 0.0235438
I0205 00:49:10.060704 10845 solver.cpp:253]     Train net output #0: loss = 0.0235439 (* 1 = 0.0235439 loss)
I0205 00:49:10.060716 10845 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I0205 00:49:14.894891 10845 solver.cpp:237] Iteration 1360, loss = 0.0134661
I0205 00:49:14.894955 10845 solver.cpp:253]     Train net output #0: loss = 0.0134662 (* 1 = 0.0134662 loss)
I0205 00:49:14.894970 10845 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I0205 00:49:19.731024 10845 solver.cpp:237] Iteration 1370, loss = 0.0388113
I0205 00:49:19.731099 10845 solver.cpp:253]     Train net output #0: loss = 0.0388114 (* 1 = 0.0388114 loss)
I0205 00:49:19.731112 10845 sgd_solver.cpp:106] Iteration 1370, lr = 0.001
I0205 00:49:24.569226 10845 solver.cpp:237] Iteration 1380, loss = 0.0482514
I0205 00:49:24.569294 10845 solver.cpp:253]     Train net output #0: loss = 0.0482515 (* 1 = 0.0482515 loss)
I0205 00:49:24.569306 10845 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I0205 00:49:29.405716 10845 solver.cpp:237] Iteration 1390, loss = 0.0733749
I0205 00:49:29.405792 10845 solver.cpp:253]     Train net output #0: loss = 0.073375 (* 1 = 0.073375 loss)
I0205 00:49:29.405804 10845 sgd_solver.cpp:106] Iteration 1390, lr = 0.001
I0205 00:49:33.762658 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1400.caffemodel
I0205 00:49:33.764941 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1400.solverstate
I0205 00:49:33.765782 10845 solver.cpp:341] Iteration 1400, Testing net (#0)
I0205 00:49:36.102706 10845 solver.cpp:409]     Test net output #0: accuracy = 0.994
I0205 00:49:36.102771 10845 solver.cpp:409]     Test net output #1: loss = 0.0130443 (* 1 = 0.0130443 loss)
I0205 00:49:36.587151 10845 solver.cpp:237] Iteration 1400, loss = 0.0255787
I0205 00:49:36.587213 10845 solver.cpp:253]     Train net output #0: loss = 0.0255788 (* 1 = 0.0255788 loss)
I0205 00:49:36.587224 10845 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0205 00:49:41.426616 10845 solver.cpp:237] Iteration 1410, loss = 0.015558
I0205 00:49:41.426690 10845 solver.cpp:253]     Train net output #0: loss = 0.0155581 (* 1 = 0.0155581 loss)
I0205 00:49:41.426702 10845 sgd_solver.cpp:106] Iteration 1410, lr = 0.001
I0205 00:49:46.268481 10845 solver.cpp:237] Iteration 1420, loss = 0.0033858
I0205 00:49:46.268545 10845 solver.cpp:253]     Train net output #0: loss = 0.00338595 (* 1 = 0.00338595 loss)
I0205 00:49:46.268558 10845 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I0205 00:49:51.115953 10845 solver.cpp:237] Iteration 1430, loss = 0.0350454
I0205 00:49:51.116026 10845 solver.cpp:253]     Train net output #0: loss = 0.0350455 (* 1 = 0.0350455 loss)
I0205 00:49:51.116039 10845 sgd_solver.cpp:106] Iteration 1430, lr = 0.001
I0205 00:49:55.963490 10845 solver.cpp:237] Iteration 1440, loss = 0.0116796
I0205 00:49:55.963558 10845 solver.cpp:253]     Train net output #0: loss = 0.0116797 (* 1 = 0.0116797 loss)
I0205 00:49:55.963570 10845 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I0205 00:50:00.806113 10845 solver.cpp:237] Iteration 1450, loss = 0.0142933
I0205 00:50:00.806183 10845 solver.cpp:253]     Train net output #0: loss = 0.0142935 (* 1 = 0.0142935 loss)
I0205 00:50:00.806195 10845 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I0205 00:50:05.654693 10845 solver.cpp:237] Iteration 1460, loss = 0.0076673
I0205 00:50:05.654917 10845 solver.cpp:253]     Train net output #0: loss = 0.00766747 (* 1 = 0.00766747 loss)
I0205 00:50:05.654932 10845 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I0205 00:50:10.490128 10845 solver.cpp:237] Iteration 1470, loss = 0.0155305
I0205 00:50:10.490203 10845 solver.cpp:253]     Train net output #0: loss = 0.0155307 (* 1 = 0.0155307 loss)
I0205 00:50:10.490216 10845 sgd_solver.cpp:106] Iteration 1470, lr = 0.001
I0205 00:50:15.326232 10845 solver.cpp:237] Iteration 1480, loss = 0.0830116
I0205 00:50:15.326300 10845 solver.cpp:253]     Train net output #0: loss = 0.0830118 (* 1 = 0.0830118 loss)
I0205 00:50:15.326313 10845 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I0205 00:50:20.162823 10845 solver.cpp:237] Iteration 1490, loss = 0.0542327
I0205 00:50:20.162895 10845 solver.cpp:253]     Train net output #0: loss = 0.0542329 (* 1 = 0.0542329 loss)
I0205 00:50:20.162907 10845 sgd_solver.cpp:106] Iteration 1490, lr = 0.001
I0205 00:50:24.516141 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1500.caffemodel
I0205 00:50:24.518208 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1500.solverstate
I0205 00:50:24.518996 10845 solver.cpp:341] Iteration 1500, Testing net (#0)
I0205 00:50:26.854266 10845 solver.cpp:409]     Test net output #0: accuracy = 0.95
I0205 00:50:26.854332 10845 solver.cpp:409]     Test net output #1: loss = 0.148405 (* 1 = 0.148405 loss)
I0205 00:50:27.337625 10845 solver.cpp:237] Iteration 1500, loss = 0.127081
I0205 00:50:27.337704 10845 solver.cpp:253]     Train net output #0: loss = 0.127082 (* 1 = 0.127082 loss)
I0205 00:50:27.337716 10845 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I0205 00:50:32.177676 10845 solver.cpp:237] Iteration 1510, loss = 0.0180177
I0205 00:50:32.177741 10845 solver.cpp:253]     Train net output #0: loss = 0.0180179 (* 1 = 0.0180179 loss)
I0205 00:50:32.177752 10845 sgd_solver.cpp:106] Iteration 1510, lr = 0.001
I0205 00:50:37.018084 10845 solver.cpp:237] Iteration 1520, loss = 0.0120868
I0205 00:50:37.018311 10845 solver.cpp:253]     Train net output #0: loss = 0.012087 (* 1 = 0.012087 loss)
I0205 00:50:37.018326 10845 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I0205 00:50:41.856914 10845 solver.cpp:237] Iteration 1530, loss = 0.0160065
I0205 00:50:41.856982 10845 solver.cpp:253]     Train net output #0: loss = 0.0160067 (* 1 = 0.0160067 loss)
I0205 00:50:41.856994 10845 sgd_solver.cpp:106] Iteration 1530, lr = 0.001
I0205 00:50:46.695633 10845 solver.cpp:237] Iteration 1540, loss = 0.020253
I0205 00:50:46.695705 10845 solver.cpp:253]     Train net output #0: loss = 0.0202532 (* 1 = 0.0202532 loss)
I0205 00:50:46.695719 10845 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I0205 00:50:51.533493 10845 solver.cpp:237] Iteration 1550, loss = 0.0991347
I0205 00:50:51.533563 10845 solver.cpp:253]     Train net output #0: loss = 0.0991349 (* 1 = 0.0991349 loss)
I0205 00:50:51.533576 10845 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I0205 00:50:56.373111 10845 solver.cpp:237] Iteration 1560, loss = 0.0553892
I0205 00:50:56.373181 10845 solver.cpp:253]     Train net output #0: loss = 0.0553894 (* 1 = 0.0553894 loss)
I0205 00:50:56.373194 10845 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I0205 00:51:01.209442 10845 solver.cpp:237] Iteration 1570, loss = 0.223778
I0205 00:51:01.209507 10845 solver.cpp:253]     Train net output #0: loss = 0.223778 (* 1 = 0.223778 loss)
I0205 00:51:01.209520 10845 sgd_solver.cpp:106] Iteration 1570, lr = 0.001
I0205 00:51:06.042364 10845 solver.cpp:237] Iteration 1580, loss = 0.00478209
I0205 00:51:06.042438 10845 solver.cpp:253]     Train net output #0: loss = 0.0047823 (* 1 = 0.0047823 loss)
I0205 00:51:06.042448 10845 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I0205 00:51:10.879050 10845 solver.cpp:237] Iteration 1590, loss = 0.0121236
I0205 00:51:10.879254 10845 solver.cpp:253]     Train net output #0: loss = 0.0121238 (* 1 = 0.0121238 loss)
I0205 00:51:10.879268 10845 sgd_solver.cpp:106] Iteration 1590, lr = 0.001
I0205 00:51:15.229043 10845 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1600.caffemodel
I0205 00:51:15.231097 10845 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed14/snaps/snap__iter_1600.solverstate
I0205 00:51:15.468842 10845 solver.cpp:321] Iteration 1600, loss = 0.0155438
I0205 00:51:15.468900 10845 solver.cpp:341] Iteration 1600, Testing net (#0)
I0205 00:51:17.800657 10845 solver.cpp:409]     Test net output #0: accuracy = 0.996
I0205 00:51:17.800717 10845 solver.cpp:409]     Test net output #1: loss = 0.012579 (* 1 = 0.012579 loss)
I0205 00:51:17.800726 10845 solver.cpp:326] Optimization Done.
I0205 00:51:17.800732 10845 caffe.cpp:215] Optimization Done.
