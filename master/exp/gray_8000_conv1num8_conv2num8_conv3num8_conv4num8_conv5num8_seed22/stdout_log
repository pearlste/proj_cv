I0205 06:27:44.494722 12688 caffe.cpp:177] Use CPU.
I0205 06:27:44.495558 12688 solver.cpp:48] Initializing solver from parameters: 
test_iter: 10
test_interval: 100
base_lr: 0.001
display: 10
max_iter: 1600
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 100
snapshot_prefix: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap_"
solver_mode: CPU
random_seed: 22
net: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/train_val.prototxt"
I0205 06:27:44.957283 12688 solver.cpp:91] Creating training net from net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/train_val.prototxt
I0205 06:27:44.958492 12688 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0205 06:27:44.958551 12688 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0205 06:27:44.959044 12688 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 06:27:44.959293 12688 layer_factory.hpp:77] Creating layer data
I0205 06:27:44.960379 12688 net.cpp:106] Creating Layer data
I0205 06:27:44.960415 12688 net.cpp:411] data -> data
I0205 06:27:44.960546 12688 net.cpp:411] data -> label
I0205 06:27:44.960587 12688 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto
I0205 06:27:44.960752 12689 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db
I0205 06:27:44.962385 12688 data_layer.cpp:41] output data size: 100,1,227,227
I0205 06:27:44.997931 12688 net.cpp:150] Setting up data
I0205 06:27:44.998037 12688 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 06:27:44.998046 12688 net.cpp:157] Top shape: 100 (100)
I0205 06:27:44.998054 12688 net.cpp:165] Memory required for data: 20612000
I0205 06:27:44.998075 12688 layer_factory.hpp:77] Creating layer conv1
I0205 06:27:44.998126 12688 net.cpp:106] Creating Layer conv1
I0205 06:27:44.998137 12688 net.cpp:454] conv1 <- data
I0205 06:27:44.998160 12688 net.cpp:411] conv1 -> conv1
I0205 06:27:44.998312 12688 net.cpp:150] Setting up conv1
I0205 06:27:44.998325 12688 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 06:27:44.998332 12688 net.cpp:165] Memory required for data: 30292000
I0205 06:27:44.998350 12688 layer_factory.hpp:77] Creating layer relu1
I0205 06:27:44.998363 12688 net.cpp:106] Creating Layer relu1
I0205 06:27:44.998373 12688 net.cpp:454] relu1 <- conv1
I0205 06:27:44.998383 12688 net.cpp:397] relu1 -> conv1 (in-place)
I0205 06:27:44.998396 12688 net.cpp:150] Setting up relu1
I0205 06:27:44.998404 12688 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 06:27:44.998409 12688 net.cpp:165] Memory required for data: 39972000
I0205 06:27:44.998415 12688 layer_factory.hpp:77] Creating layer pool1
I0205 06:27:44.998426 12688 net.cpp:106] Creating Layer pool1
I0205 06:27:44.998432 12688 net.cpp:454] pool1 <- conv1
I0205 06:27:44.998440 12688 net.cpp:411] pool1 -> pool1
I0205 06:27:44.998468 12688 net.cpp:150] Setting up pool1
I0205 06:27:44.998476 12688 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:27:44.998481 12688 net.cpp:165] Memory required for data: 42304800
I0205 06:27:44.998487 12688 layer_factory.hpp:77] Creating layer norm1
I0205 06:27:44.998512 12688 net.cpp:106] Creating Layer norm1
I0205 06:27:44.998533 12688 net.cpp:454] norm1 <- pool1
I0205 06:27:44.998543 12688 net.cpp:411] norm1 -> norm1
I0205 06:27:44.998563 12688 net.cpp:150] Setting up norm1
I0205 06:27:44.998570 12688 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:27:44.998576 12688 net.cpp:165] Memory required for data: 44637600
I0205 06:27:44.998581 12688 layer_factory.hpp:77] Creating layer conv2
I0205 06:27:44.998594 12688 net.cpp:106] Creating Layer conv2
I0205 06:27:44.998600 12688 net.cpp:454] conv2 <- norm1
I0205 06:27:44.998610 12688 net.cpp:411] conv2 -> conv2
I0205 06:27:44.998643 12688 net.cpp:150] Setting up conv2
I0205 06:27:44.998652 12688 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:27:44.998657 12688 net.cpp:165] Memory required for data: 46970400
I0205 06:27:44.998668 12688 layer_factory.hpp:77] Creating layer relu2
I0205 06:27:44.998677 12688 net.cpp:106] Creating Layer relu2
I0205 06:27:44.998683 12688 net.cpp:454] relu2 <- conv2
I0205 06:27:44.998692 12688 net.cpp:397] relu2 -> conv2 (in-place)
I0205 06:27:44.998700 12688 net.cpp:150] Setting up relu2
I0205 06:27:44.998708 12688 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:27:44.998713 12688 net.cpp:165] Memory required for data: 49303200
I0205 06:27:44.998718 12688 layer_factory.hpp:77] Creating layer pool2
I0205 06:27:44.998728 12688 net.cpp:106] Creating Layer pool2
I0205 06:27:44.998733 12688 net.cpp:454] pool2 <- conv2
I0205 06:27:44.998742 12688 net.cpp:411] pool2 -> pool2
I0205 06:27:44.998752 12688 net.cpp:150] Setting up pool2
I0205 06:27:44.998761 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:44.998766 12688 net.cpp:165] Memory required for data: 49844000
I0205 06:27:44.998772 12688 layer_factory.hpp:77] Creating layer norm2
I0205 06:27:44.998782 12688 net.cpp:106] Creating Layer norm2
I0205 06:27:44.998787 12688 net.cpp:454] norm2 <- pool2
I0205 06:27:44.998796 12688 net.cpp:411] norm2 -> norm2
I0205 06:27:44.998805 12688 net.cpp:150] Setting up norm2
I0205 06:27:44.998812 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:44.998817 12688 net.cpp:165] Memory required for data: 50384800
I0205 06:27:44.998822 12688 layer_factory.hpp:77] Creating layer conv3
I0205 06:27:44.998834 12688 net.cpp:106] Creating Layer conv3
I0205 06:27:44.998839 12688 net.cpp:454] conv3 <- norm2
I0205 06:27:44.998848 12688 net.cpp:411] conv3 -> conv3
I0205 06:27:44.998878 12688 net.cpp:150] Setting up conv3
I0205 06:27:44.998885 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:44.998893 12688 net.cpp:165] Memory required for data: 50925600
I0205 06:27:44.998903 12688 layer_factory.hpp:77] Creating layer relu3
I0205 06:27:44.998913 12688 net.cpp:106] Creating Layer relu3
I0205 06:27:44.998917 12688 net.cpp:454] relu3 <- conv3
I0205 06:27:44.998925 12688 net.cpp:397] relu3 -> conv3 (in-place)
I0205 06:27:44.998934 12688 net.cpp:150] Setting up relu3
I0205 06:27:44.998940 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:44.998945 12688 net.cpp:165] Memory required for data: 51466400
I0205 06:27:44.998953 12688 layer_factory.hpp:77] Creating layer conv4
I0205 06:27:44.998970 12688 net.cpp:106] Creating Layer conv4
I0205 06:27:44.998975 12688 net.cpp:454] conv4 <- conv3
I0205 06:27:44.998986 12688 net.cpp:411] conv4 -> conv4
I0205 06:27:44.999011 12688 net.cpp:150] Setting up conv4
I0205 06:27:44.999018 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:44.999027 12688 net.cpp:165] Memory required for data: 52007200
I0205 06:27:44.999034 12688 layer_factory.hpp:77] Creating layer relu4
I0205 06:27:44.999042 12688 net.cpp:106] Creating Layer relu4
I0205 06:27:44.999047 12688 net.cpp:454] relu4 <- conv4
I0205 06:27:44.999055 12688 net.cpp:397] relu4 -> conv4 (in-place)
I0205 06:27:44.999063 12688 net.cpp:150] Setting up relu4
I0205 06:27:44.999071 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:44.999076 12688 net.cpp:165] Memory required for data: 52548000
I0205 06:27:44.999083 12688 layer_factory.hpp:77] Creating layer conv5
I0205 06:27:44.999099 12688 net.cpp:106] Creating Layer conv5
I0205 06:27:44.999111 12688 net.cpp:454] conv5 <- conv4
I0205 06:27:44.999122 12688 net.cpp:411] conv5 -> conv5
I0205 06:27:44.999145 12688 net.cpp:150] Setting up conv5
I0205 06:27:44.999152 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:44.999160 12688 net.cpp:165] Memory required for data: 53088800
I0205 06:27:44.999171 12688 layer_factory.hpp:77] Creating layer relu5
I0205 06:27:44.999181 12688 net.cpp:106] Creating Layer relu5
I0205 06:27:44.999186 12688 net.cpp:454] relu5 <- conv5
I0205 06:27:44.999194 12688 net.cpp:397] relu5 -> conv5 (in-place)
I0205 06:27:44.999202 12688 net.cpp:150] Setting up relu5
I0205 06:27:44.999208 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:44.999214 12688 net.cpp:165] Memory required for data: 53629600
I0205 06:27:44.999219 12688 layer_factory.hpp:77] Creating layer pool5
I0205 06:27:44.999228 12688 net.cpp:106] Creating Layer pool5
I0205 06:27:44.999233 12688 net.cpp:454] pool5 <- conv5
I0205 06:27:44.999241 12688 net.cpp:411] pool5 -> pool5
I0205 06:27:44.999253 12688 net.cpp:150] Setting up pool5
I0205 06:27:44.999259 12688 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0205 06:27:44.999264 12688 net.cpp:165] Memory required for data: 53744800
I0205 06:27:44.999269 12688 layer_factory.hpp:77] Creating layer fc6
I0205 06:27:44.999286 12688 net.cpp:106] Creating Layer fc6
I0205 06:27:44.999294 12688 net.cpp:454] fc6 <- pool5
I0205 06:27:44.999305 12688 net.cpp:411] fc6 -> fc6
I0205 06:27:45.000130 12688 net.cpp:150] Setting up fc6
I0205 06:27:45.000144 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.000149 12688 net.cpp:165] Memory required for data: 53847200
I0205 06:27:45.000157 12688 layer_factory.hpp:77] Creating layer relu6
I0205 06:27:45.000167 12688 net.cpp:106] Creating Layer relu6
I0205 06:27:45.000174 12688 net.cpp:454] relu6 <- fc6
I0205 06:27:45.000182 12688 net.cpp:397] relu6 -> fc6 (in-place)
I0205 06:27:45.000192 12688 net.cpp:150] Setting up relu6
I0205 06:27:45.000198 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.000205 12688 net.cpp:165] Memory required for data: 53949600
I0205 06:27:45.000210 12688 layer_factory.hpp:77] Creating layer drop6
I0205 06:27:45.000221 12688 net.cpp:106] Creating Layer drop6
I0205 06:27:45.000227 12688 net.cpp:454] drop6 <- fc6
I0205 06:27:45.000234 12688 net.cpp:397] drop6 -> fc6 (in-place)
I0205 06:27:45.000262 12688 net.cpp:150] Setting up drop6
I0205 06:27:45.000272 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.000277 12688 net.cpp:165] Memory required for data: 54052000
I0205 06:27:45.000283 12688 layer_factory.hpp:77] Creating layer fc7
I0205 06:27:45.000293 12688 net.cpp:106] Creating Layer fc7
I0205 06:27:45.000298 12688 net.cpp:454] fc7 <- fc6
I0205 06:27:45.000306 12688 net.cpp:411] fc7 -> fc7
I0205 06:27:45.001003 12688 net.cpp:150] Setting up fc7
I0205 06:27:45.001018 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.001022 12688 net.cpp:165] Memory required for data: 54154400
I0205 06:27:45.001031 12688 layer_factory.hpp:77] Creating layer relu7
I0205 06:27:45.001039 12688 net.cpp:106] Creating Layer relu7
I0205 06:27:45.001045 12688 net.cpp:454] relu7 <- fc7
I0205 06:27:45.001055 12688 net.cpp:397] relu7 -> fc7 (in-place)
I0205 06:27:45.001065 12688 net.cpp:150] Setting up relu7
I0205 06:27:45.001070 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.001076 12688 net.cpp:165] Memory required for data: 54256800
I0205 06:27:45.001081 12688 layer_factory.hpp:77] Creating layer drop7
I0205 06:27:45.001092 12688 net.cpp:106] Creating Layer drop7
I0205 06:27:45.001098 12688 net.cpp:454] drop7 <- fc7
I0205 06:27:45.001106 12688 net.cpp:397] drop7 -> fc7 (in-place)
I0205 06:27:45.001116 12688 net.cpp:150] Setting up drop7
I0205 06:27:45.001121 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.001127 12688 net.cpp:165] Memory required for data: 54359200
I0205 06:27:45.001132 12688 layer_factory.hpp:77] Creating layer fc8
I0205 06:27:45.001144 12688 net.cpp:106] Creating Layer fc8
I0205 06:27:45.001154 12688 net.cpp:454] fc8 <- fc7
I0205 06:27:45.001168 12688 net.cpp:411] fc8 -> fc8
I0205 06:27:45.001196 12688 net.cpp:150] Setting up fc8
I0205 06:27:45.001204 12688 net.cpp:157] Top shape: 100 2 (200)
I0205 06:27:45.001209 12688 net.cpp:165] Memory required for data: 54360000
I0205 06:27:45.001220 12688 layer_factory.hpp:77] Creating layer loss
I0205 06:27:45.001230 12688 net.cpp:106] Creating Layer loss
I0205 06:27:45.001235 12688 net.cpp:454] loss <- fc8
I0205 06:27:45.001241 12688 net.cpp:454] loss <- label
I0205 06:27:45.001252 12688 net.cpp:411] loss -> loss
I0205 06:27:45.001268 12688 layer_factory.hpp:77] Creating layer loss
I0205 06:27:45.001292 12688 net.cpp:150] Setting up loss
I0205 06:27:45.001301 12688 net.cpp:157] Top shape: (1)
I0205 06:27:45.001305 12688 net.cpp:160]     with loss weight 1
I0205 06:27:45.001337 12688 net.cpp:165] Memory required for data: 54360004
I0205 06:27:45.001344 12688 net.cpp:226] loss needs backward computation.
I0205 06:27:45.001351 12688 net.cpp:226] fc8 needs backward computation.
I0205 06:27:45.001356 12688 net.cpp:226] drop7 needs backward computation.
I0205 06:27:45.001363 12688 net.cpp:226] relu7 needs backward computation.
I0205 06:27:45.001368 12688 net.cpp:226] fc7 needs backward computation.
I0205 06:27:45.001374 12688 net.cpp:226] drop6 needs backward computation.
I0205 06:27:45.001379 12688 net.cpp:226] relu6 needs backward computation.
I0205 06:27:45.001384 12688 net.cpp:226] fc6 needs backward computation.
I0205 06:27:45.001389 12688 net.cpp:226] pool5 needs backward computation.
I0205 06:27:45.001395 12688 net.cpp:226] relu5 needs backward computation.
I0205 06:27:45.001400 12688 net.cpp:226] conv5 needs backward computation.
I0205 06:27:45.001405 12688 net.cpp:226] relu4 needs backward computation.
I0205 06:27:45.001411 12688 net.cpp:226] conv4 needs backward computation.
I0205 06:27:45.001416 12688 net.cpp:226] relu3 needs backward computation.
I0205 06:27:45.001422 12688 net.cpp:226] conv3 needs backward computation.
I0205 06:27:45.001431 12688 net.cpp:226] norm2 needs backward computation.
I0205 06:27:45.001440 12688 net.cpp:226] pool2 needs backward computation.
I0205 06:27:45.001446 12688 net.cpp:226] relu2 needs backward computation.
I0205 06:27:45.001451 12688 net.cpp:226] conv2 needs backward computation.
I0205 06:27:45.001456 12688 net.cpp:226] norm1 needs backward computation.
I0205 06:27:45.001462 12688 net.cpp:226] pool1 needs backward computation.
I0205 06:27:45.001467 12688 net.cpp:226] relu1 needs backward computation.
I0205 06:27:45.001473 12688 net.cpp:226] conv1 needs backward computation.
I0205 06:27:45.001479 12688 net.cpp:228] data does not need backward computation.
I0205 06:27:45.001485 12688 net.cpp:270] This network produces output loss
I0205 06:27:45.001515 12688 net.cpp:283] Network initialization done.
I0205 06:27:45.003448 12688 solver.cpp:181] Creating test net (#0) specified by net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/train_val.prototxt
I0205 06:27:45.003509 12688 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0205 06:27:45.003813 12688 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 06:27:45.003998 12688 layer_factory.hpp:77] Creating layer data
I0205 06:27:45.004158 12688 net.cpp:106] Creating Layer data
I0205 06:27:45.004173 12688 net.cpp:411] data -> data
I0205 06:27:45.004189 12688 net.cpp:411] data -> label
I0205 06:27:45.004202 12688 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto
I0205 06:27:45.004406 12692 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db
I0205 06:27:45.005142 12688 data_layer.cpp:41] output data size: 100,1,227,227
I0205 06:27:45.037737 12688 net.cpp:150] Setting up data
I0205 06:27:45.037765 12688 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 06:27:45.037773 12688 net.cpp:157] Top shape: 100 (100)
I0205 06:27:45.037778 12688 net.cpp:165] Memory required for data: 20612000
I0205 06:27:45.037788 12688 layer_factory.hpp:77] Creating layer label_data_1_split
I0205 06:27:45.037803 12688 net.cpp:106] Creating Layer label_data_1_split
I0205 06:27:45.037813 12688 net.cpp:454] label_data_1_split <- label
I0205 06:27:45.037824 12688 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0205 06:27:45.037839 12688 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0205 06:27:45.037852 12688 net.cpp:150] Setting up label_data_1_split
I0205 06:27:45.037859 12688 net.cpp:157] Top shape: 100 (100)
I0205 06:27:45.037865 12688 net.cpp:157] Top shape: 100 (100)
I0205 06:27:45.037870 12688 net.cpp:165] Memory required for data: 20612800
I0205 06:27:45.037876 12688 layer_factory.hpp:77] Creating layer conv1
I0205 06:27:45.037890 12688 net.cpp:106] Creating Layer conv1
I0205 06:27:45.037896 12688 net.cpp:454] conv1 <- data
I0205 06:27:45.037905 12688 net.cpp:411] conv1 -> conv1
I0205 06:27:45.037955 12688 net.cpp:150] Setting up conv1
I0205 06:27:45.037978 12688 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 06:27:45.037988 12688 net.cpp:165] Memory required for data: 30292800
I0205 06:27:45.038002 12688 layer_factory.hpp:77] Creating layer relu1
I0205 06:27:45.038012 12688 net.cpp:106] Creating Layer relu1
I0205 06:27:45.038018 12688 net.cpp:454] relu1 <- conv1
I0205 06:27:45.038027 12688 net.cpp:397] relu1 -> conv1 (in-place)
I0205 06:27:45.038035 12688 net.cpp:150] Setting up relu1
I0205 06:27:45.038043 12688 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 06:27:45.038048 12688 net.cpp:165] Memory required for data: 39972800
I0205 06:27:45.038054 12688 layer_factory.hpp:77] Creating layer pool1
I0205 06:27:45.038064 12688 net.cpp:106] Creating Layer pool1
I0205 06:27:45.038069 12688 net.cpp:454] pool1 <- conv1
I0205 06:27:45.038077 12688 net.cpp:411] pool1 -> pool1
I0205 06:27:45.038091 12688 net.cpp:150] Setting up pool1
I0205 06:27:45.038100 12688 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:27:45.038106 12688 net.cpp:165] Memory required for data: 42305600
I0205 06:27:45.038112 12688 layer_factory.hpp:77] Creating layer norm1
I0205 06:27:45.038122 12688 net.cpp:106] Creating Layer norm1
I0205 06:27:45.038128 12688 net.cpp:454] norm1 <- pool1
I0205 06:27:45.038136 12688 net.cpp:411] norm1 -> norm1
I0205 06:27:45.038147 12688 net.cpp:150] Setting up norm1
I0205 06:27:45.038156 12688 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:27:45.038162 12688 net.cpp:165] Memory required for data: 44638400
I0205 06:27:45.038168 12688 layer_factory.hpp:77] Creating layer conv2
I0205 06:27:45.038179 12688 net.cpp:106] Creating Layer conv2
I0205 06:27:45.038185 12688 net.cpp:454] conv2 <- norm1
I0205 06:27:45.038193 12688 net.cpp:411] conv2 -> conv2
I0205 06:27:45.038223 12688 net.cpp:150] Setting up conv2
I0205 06:27:45.038230 12688 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:27:45.038235 12688 net.cpp:165] Memory required for data: 46971200
I0205 06:27:45.038246 12688 layer_factory.hpp:77] Creating layer relu2
I0205 06:27:45.038256 12688 net.cpp:106] Creating Layer relu2
I0205 06:27:45.038262 12688 net.cpp:454] relu2 <- conv2
I0205 06:27:45.038270 12688 net.cpp:397] relu2 -> conv2 (in-place)
I0205 06:27:45.038287 12688 net.cpp:150] Setting up relu2
I0205 06:27:45.038306 12688 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 06:27:45.038312 12688 net.cpp:165] Memory required for data: 49304000
I0205 06:27:45.038318 12688 layer_factory.hpp:77] Creating layer pool2
I0205 06:27:45.038331 12688 net.cpp:106] Creating Layer pool2
I0205 06:27:45.038336 12688 net.cpp:454] pool2 <- conv2
I0205 06:27:45.038344 12688 net.cpp:411] pool2 -> pool2
I0205 06:27:45.038355 12688 net.cpp:150] Setting up pool2
I0205 06:27:45.038363 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:45.038368 12688 net.cpp:165] Memory required for data: 49844800
I0205 06:27:45.038373 12688 layer_factory.hpp:77] Creating layer norm2
I0205 06:27:45.038383 12688 net.cpp:106] Creating Layer norm2
I0205 06:27:45.038388 12688 net.cpp:454] norm2 <- pool2
I0205 06:27:45.038395 12688 net.cpp:411] norm2 -> norm2
I0205 06:27:45.038404 12688 net.cpp:150] Setting up norm2
I0205 06:27:45.038411 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:45.038416 12688 net.cpp:165] Memory required for data: 50385600
I0205 06:27:45.038421 12688 layer_factory.hpp:77] Creating layer conv3
I0205 06:27:45.038434 12688 net.cpp:106] Creating Layer conv3
I0205 06:27:45.038439 12688 net.cpp:454] conv3 <- norm2
I0205 06:27:45.038447 12688 net.cpp:411] conv3 -> conv3
I0205 06:27:45.038473 12688 net.cpp:150] Setting up conv3
I0205 06:27:45.038481 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:45.038486 12688 net.cpp:165] Memory required for data: 50926400
I0205 06:27:45.038499 12688 layer_factory.hpp:77] Creating layer relu3
I0205 06:27:45.038523 12688 net.cpp:106] Creating Layer relu3
I0205 06:27:45.038529 12688 net.cpp:454] relu3 <- conv3
I0205 06:27:45.038537 12688 net.cpp:397] relu3 -> conv3 (in-place)
I0205 06:27:45.038545 12688 net.cpp:150] Setting up relu3
I0205 06:27:45.038552 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:45.038558 12688 net.cpp:165] Memory required for data: 51467200
I0205 06:27:45.038563 12688 layer_factory.hpp:77] Creating layer conv4
I0205 06:27:45.038571 12688 net.cpp:106] Creating Layer conv4
I0205 06:27:45.038578 12688 net.cpp:454] conv4 <- conv3
I0205 06:27:45.038584 12688 net.cpp:411] conv4 -> conv4
I0205 06:27:45.038607 12688 net.cpp:150] Setting up conv4
I0205 06:27:45.038615 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:45.038620 12688 net.cpp:165] Memory required for data: 52008000
I0205 06:27:45.038628 12688 layer_factory.hpp:77] Creating layer relu4
I0205 06:27:45.038635 12688 net.cpp:106] Creating Layer relu4
I0205 06:27:45.038641 12688 net.cpp:454] relu4 <- conv4
I0205 06:27:45.038648 12688 net.cpp:397] relu4 -> conv4 (in-place)
I0205 06:27:45.038658 12688 net.cpp:150] Setting up relu4
I0205 06:27:45.038666 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:45.038671 12688 net.cpp:165] Memory required for data: 52548800
I0205 06:27:45.038676 12688 layer_factory.hpp:77] Creating layer conv5
I0205 06:27:45.038687 12688 net.cpp:106] Creating Layer conv5
I0205 06:27:45.038692 12688 net.cpp:454] conv5 <- conv4
I0205 06:27:45.038702 12688 net.cpp:411] conv5 -> conv5
I0205 06:27:45.038722 12688 net.cpp:150] Setting up conv5
I0205 06:27:45.038730 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:45.038735 12688 net.cpp:165] Memory required for data: 53089600
I0205 06:27:45.038745 12688 layer_factory.hpp:77] Creating layer relu5
I0205 06:27:45.038755 12688 net.cpp:106] Creating Layer relu5
I0205 06:27:45.038761 12688 net.cpp:454] relu5 <- conv5
I0205 06:27:45.038769 12688 net.cpp:397] relu5 -> conv5 (in-place)
I0205 06:27:45.038776 12688 net.cpp:150] Setting up relu5
I0205 06:27:45.038784 12688 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 06:27:45.038789 12688 net.cpp:165] Memory required for data: 53630400
I0205 06:27:45.038794 12688 layer_factory.hpp:77] Creating layer pool5
I0205 06:27:45.038803 12688 net.cpp:106] Creating Layer pool5
I0205 06:27:45.038810 12688 net.cpp:454] pool5 <- conv5
I0205 06:27:45.038817 12688 net.cpp:411] pool5 -> pool5
I0205 06:27:45.038836 12688 net.cpp:150] Setting up pool5
I0205 06:27:45.038852 12688 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0205 06:27:45.038857 12688 net.cpp:165] Memory required for data: 53745600
I0205 06:27:45.038863 12688 layer_factory.hpp:77] Creating layer fc6
I0205 06:27:45.038874 12688 net.cpp:106] Creating Layer fc6
I0205 06:27:45.038880 12688 net.cpp:454] fc6 <- pool5
I0205 06:27:45.038888 12688 net.cpp:411] fc6 -> fc6
I0205 06:27:45.039623 12688 net.cpp:150] Setting up fc6
I0205 06:27:45.039636 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.039641 12688 net.cpp:165] Memory required for data: 53848000
I0205 06:27:45.039650 12688 layer_factory.hpp:77] Creating layer relu6
I0205 06:27:45.039662 12688 net.cpp:106] Creating Layer relu6
I0205 06:27:45.039669 12688 net.cpp:454] relu6 <- fc6
I0205 06:27:45.039677 12688 net.cpp:397] relu6 -> fc6 (in-place)
I0205 06:27:45.039686 12688 net.cpp:150] Setting up relu6
I0205 06:27:45.039693 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.039698 12688 net.cpp:165] Memory required for data: 53950400
I0205 06:27:45.039703 12688 layer_factory.hpp:77] Creating layer drop6
I0205 06:27:45.039713 12688 net.cpp:106] Creating Layer drop6
I0205 06:27:45.039718 12688 net.cpp:454] drop6 <- fc6
I0205 06:27:45.039726 12688 net.cpp:397] drop6 -> fc6 (in-place)
I0205 06:27:45.039736 12688 net.cpp:150] Setting up drop6
I0205 06:27:45.039742 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.039749 12688 net.cpp:165] Memory required for data: 54052800
I0205 06:27:45.039755 12688 layer_factory.hpp:77] Creating layer fc7
I0205 06:27:45.039767 12688 net.cpp:106] Creating Layer fc7
I0205 06:27:45.039773 12688 net.cpp:454] fc7 <- fc6
I0205 06:27:45.039783 12688 net.cpp:411] fc7 -> fc7
I0205 06:27:45.040549 12688 net.cpp:150] Setting up fc7
I0205 06:27:45.040562 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.040567 12688 net.cpp:165] Memory required for data: 54155200
I0205 06:27:45.040576 12688 layer_factory.hpp:77] Creating layer relu7
I0205 06:27:45.040585 12688 net.cpp:106] Creating Layer relu7
I0205 06:27:45.040591 12688 net.cpp:454] relu7 <- fc7
I0205 06:27:45.040602 12688 net.cpp:397] relu7 -> fc7 (in-place)
I0205 06:27:45.040611 12688 net.cpp:150] Setting up relu7
I0205 06:27:45.040617 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.040623 12688 net.cpp:165] Memory required for data: 54257600
I0205 06:27:45.040628 12688 layer_factory.hpp:77] Creating layer drop7
I0205 06:27:45.040637 12688 net.cpp:106] Creating Layer drop7
I0205 06:27:45.040642 12688 net.cpp:454] drop7 <- fc7
I0205 06:27:45.040648 12688 net.cpp:397] drop7 -> fc7 (in-place)
I0205 06:27:45.040657 12688 net.cpp:150] Setting up drop7
I0205 06:27:45.040666 12688 net.cpp:157] Top shape: 100 256 (25600)
I0205 06:27:45.040671 12688 net.cpp:165] Memory required for data: 54360000
I0205 06:27:45.040678 12688 layer_factory.hpp:77] Creating layer fc8
I0205 06:27:45.040688 12688 net.cpp:106] Creating Layer fc8
I0205 06:27:45.040694 12688 net.cpp:454] fc8 <- fc7
I0205 06:27:45.040704 12688 net.cpp:411] fc8 -> fc8
I0205 06:27:45.040729 12688 net.cpp:150] Setting up fc8
I0205 06:27:45.040740 12688 net.cpp:157] Top shape: 100 2 (200)
I0205 06:27:45.040746 12688 net.cpp:165] Memory required for data: 54360800
I0205 06:27:45.040755 12688 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0205 06:27:45.040762 12688 net.cpp:106] Creating Layer fc8_fc8_0_split
I0205 06:27:45.040767 12688 net.cpp:454] fc8_fc8_0_split <- fc8
I0205 06:27:45.040776 12688 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0205 06:27:45.040784 12688 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0205 06:27:45.040793 12688 net.cpp:150] Setting up fc8_fc8_0_split
I0205 06:27:45.040799 12688 net.cpp:157] Top shape: 100 2 (200)
I0205 06:27:45.040807 12688 net.cpp:157] Top shape: 100 2 (200)
I0205 06:27:45.040810 12688 net.cpp:165] Memory required for data: 54362400
I0205 06:27:45.040817 12688 layer_factory.hpp:77] Creating layer accuracy
I0205 06:27:45.040848 12688 net.cpp:106] Creating Layer accuracy
I0205 06:27:45.040858 12688 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0205 06:27:45.040873 12688 net.cpp:454] accuracy <- label_data_1_split_0
I0205 06:27:45.040884 12688 net.cpp:411] accuracy -> accuracy
I0205 06:27:45.040896 12688 net.cpp:150] Setting up accuracy
I0205 06:27:45.040904 12688 net.cpp:157] Top shape: (1)
I0205 06:27:45.040909 12688 net.cpp:165] Memory required for data: 54362404
I0205 06:27:45.040915 12688 layer_factory.hpp:77] Creating layer loss
I0205 06:27:45.040925 12688 net.cpp:106] Creating Layer loss
I0205 06:27:45.040930 12688 net.cpp:454] loss <- fc8_fc8_0_split_1
I0205 06:27:45.040937 12688 net.cpp:454] loss <- label_data_1_split_1
I0205 06:27:45.040945 12688 net.cpp:411] loss -> loss
I0205 06:27:45.040954 12688 layer_factory.hpp:77] Creating layer loss
I0205 06:27:45.041010 12688 net.cpp:150] Setting up loss
I0205 06:27:45.041020 12688 net.cpp:157] Top shape: (1)
I0205 06:27:45.041025 12688 net.cpp:160]     with loss weight 1
I0205 06:27:45.041038 12688 net.cpp:165] Memory required for data: 54362408
I0205 06:27:45.041045 12688 net.cpp:226] loss needs backward computation.
I0205 06:27:45.041051 12688 net.cpp:228] accuracy does not need backward computation.
I0205 06:27:45.041059 12688 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0205 06:27:45.041064 12688 net.cpp:226] fc8 needs backward computation.
I0205 06:27:45.041069 12688 net.cpp:226] drop7 needs backward computation.
I0205 06:27:45.041074 12688 net.cpp:226] relu7 needs backward computation.
I0205 06:27:45.041080 12688 net.cpp:226] fc7 needs backward computation.
I0205 06:27:45.041087 12688 net.cpp:226] drop6 needs backward computation.
I0205 06:27:45.041093 12688 net.cpp:226] relu6 needs backward computation.
I0205 06:27:45.041098 12688 net.cpp:226] fc6 needs backward computation.
I0205 06:27:45.041105 12688 net.cpp:226] pool5 needs backward computation.
I0205 06:27:45.041110 12688 net.cpp:226] relu5 needs backward computation.
I0205 06:27:45.041115 12688 net.cpp:226] conv5 needs backward computation.
I0205 06:27:45.041121 12688 net.cpp:226] relu4 needs backward computation.
I0205 06:27:45.041126 12688 net.cpp:226] conv4 needs backward computation.
I0205 06:27:45.041131 12688 net.cpp:226] relu3 needs backward computation.
I0205 06:27:45.041137 12688 net.cpp:226] conv3 needs backward computation.
I0205 06:27:45.041143 12688 net.cpp:226] norm2 needs backward computation.
I0205 06:27:45.041148 12688 net.cpp:226] pool2 needs backward computation.
I0205 06:27:45.041154 12688 net.cpp:226] relu2 needs backward computation.
I0205 06:27:45.041162 12688 net.cpp:226] conv2 needs backward computation.
I0205 06:27:45.041167 12688 net.cpp:226] norm1 needs backward computation.
I0205 06:27:45.041173 12688 net.cpp:226] pool1 needs backward computation.
I0205 06:27:45.041178 12688 net.cpp:226] relu1 needs backward computation.
I0205 06:27:45.041184 12688 net.cpp:226] conv1 needs backward computation.
I0205 06:27:45.041190 12688 net.cpp:228] label_data_1_split does not need backward computation.
I0205 06:27:45.041196 12688 net.cpp:228] data does not need backward computation.
I0205 06:27:45.041203 12688 net.cpp:270] This network produces output accuracy
I0205 06:27:45.041208 12688 net.cpp:270] This network produces output loss
I0205 06:27:45.041247 12688 net.cpp:283] Network initialization done.
I0205 06:27:45.041357 12688 solver.cpp:60] Solver scaffolding done.
I0205 06:27:45.041427 12688 caffe.cpp:212] Starting Optimization
I0205 06:27:45.041435 12688 solver.cpp:288] Solving CaffeNet
I0205 06:27:45.041441 12688 solver.cpp:289] Learning Rate Policy: step
I0205 06:27:45.041975 12688 solver.cpp:341] Iteration 0, Testing net (#0)
I0205 06:27:45.042044 12688 blocking_queue.cpp:50] Data layer prefetch queue empty
I0205 06:27:47.506065 12688 solver.cpp:409]     Test net output #0: accuracy = 0.505
I0205 06:27:47.506117 12688 solver.cpp:409]     Test net output #1: loss = 1.90521 (* 1 = 1.90521 loss)
I0205 06:27:48.035084 12688 solver.cpp:237] Iteration 0, loss = 7.31437
I0205 06:27:48.035136 12688 solver.cpp:253]     Train net output #0: loss = 7.31437 (* 1 = 7.31437 loss)
I0205 06:27:48.035162 12688 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0205 06:27:52.897112 12688 solver.cpp:237] Iteration 10, loss = 1.04744
I0205 06:27:52.897164 12688 solver.cpp:253]     Train net output #0: loss = 1.04744 (* 1 = 1.04744 loss)
I0205 06:27:52.897174 12688 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0205 06:27:57.750079 12688 solver.cpp:237] Iteration 20, loss = 1.00314
I0205 06:27:57.750129 12688 solver.cpp:253]     Train net output #0: loss = 1.00314 (* 1 = 1.00314 loss)
I0205 06:27:57.750139 12688 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0205 06:28:02.599409 12688 solver.cpp:237] Iteration 30, loss = 0.818892
I0205 06:28:02.599462 12688 solver.cpp:253]     Train net output #0: loss = 0.818893 (* 1 = 0.818893 loss)
I0205 06:28:02.599472 12688 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0205 06:28:07.449187 12688 solver.cpp:237] Iteration 40, loss = 0.912251
I0205 06:28:07.449241 12688 solver.cpp:253]     Train net output #0: loss = 0.912252 (* 1 = 0.912252 loss)
I0205 06:28:07.449252 12688 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0205 06:28:12.301549 12688 solver.cpp:237] Iteration 50, loss = 0.775275
I0205 06:28:12.301602 12688 solver.cpp:253]     Train net output #0: loss = 0.775276 (* 1 = 0.775276 loss)
I0205 06:28:12.301614 12688 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0205 06:28:17.157061 12688 solver.cpp:237] Iteration 60, loss = 0.775481
I0205 06:28:17.157169 12688 solver.cpp:253]     Train net output #0: loss = 0.775481 (* 1 = 0.775481 loss)
I0205 06:28:17.157181 12688 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0205 06:28:22.018070 12688 solver.cpp:237] Iteration 70, loss = 0.698086
I0205 06:28:22.018126 12688 solver.cpp:253]     Train net output #0: loss = 0.698086 (* 1 = 0.698086 loss)
I0205 06:28:22.018137 12688 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0205 06:28:26.889987 12688 solver.cpp:237] Iteration 80, loss = 0.681167
I0205 06:28:26.890038 12688 solver.cpp:253]     Train net output #0: loss = 0.681167 (* 1 = 0.681167 loss)
I0205 06:28:26.890049 12688 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0205 06:28:31.766057 12688 solver.cpp:237] Iteration 90, loss = 0.689137
I0205 06:28:31.766108 12688 solver.cpp:253]     Train net output #0: loss = 0.689137 (* 1 = 0.689137 loss)
I0205 06:28:31.766119 12688 sgd_solver.cpp:106] Iteration 90, lr = 0.001
I0205 06:28:36.174899 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_100.caffemodel
I0205 06:28:36.177652 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_100.solverstate
I0205 06:28:36.178472 12688 solver.cpp:341] Iteration 100, Testing net (#0)
I0205 06:28:38.587271 12688 solver.cpp:409]     Test net output #0: accuracy = 0.746
I0205 06:28:38.587318 12688 solver.cpp:409]     Test net output #1: loss = 0.545271 (* 1 = 0.545271 loss)
I0205 06:28:39.080245 12688 solver.cpp:237] Iteration 100, loss = 0.623574
I0205 06:28:39.080286 12688 solver.cpp:253]     Train net output #0: loss = 0.623574 (* 1 = 0.623574 loss)
I0205 06:28:39.080297 12688 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0205 06:28:44.002869 12688 solver.cpp:237] Iteration 110, loss = 0.703402
I0205 06:28:44.002928 12688 solver.cpp:253]     Train net output #0: loss = 0.703402 (* 1 = 0.703402 loss)
I0205 06:28:44.002938 12688 sgd_solver.cpp:106] Iteration 110, lr = 0.001
I0205 06:28:48.985422 12688 solver.cpp:237] Iteration 120, loss = 0.695362
I0205 06:28:48.985597 12688 solver.cpp:253]     Train net output #0: loss = 0.695362 (* 1 = 0.695362 loss)
I0205 06:28:48.985610 12688 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0205 06:28:54.048900 12688 solver.cpp:237] Iteration 130, loss = 0.540535
I0205 06:28:54.048960 12688 solver.cpp:253]     Train net output #0: loss = 0.540536 (* 1 = 0.540536 loss)
I0205 06:28:54.048977 12688 sgd_solver.cpp:106] Iteration 130, lr = 0.001
I0205 06:28:59.203747 12688 solver.cpp:237] Iteration 140, loss = 0.4817
I0205 06:28:59.203811 12688 solver.cpp:253]     Train net output #0: loss = 0.4817 (* 1 = 0.4817 loss)
I0205 06:28:59.203824 12688 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0205 06:29:04.473583 12688 solver.cpp:237] Iteration 150, loss = 0.42553
I0205 06:29:04.473637 12688 solver.cpp:253]     Train net output #0: loss = 0.42553 (* 1 = 0.42553 loss)
I0205 06:29:04.473649 12688 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0205 06:29:09.855798 12688 solver.cpp:237] Iteration 160, loss = 0.432117
I0205 06:29:09.855851 12688 solver.cpp:253]     Train net output #0: loss = 0.432118 (* 1 = 0.432118 loss)
I0205 06:29:09.855862 12688 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0205 06:29:15.343412 12688 solver.cpp:237] Iteration 170, loss = 0.400766
I0205 06:29:15.343453 12688 solver.cpp:253]     Train net output #0: loss = 0.400766 (* 1 = 0.400766 loss)
I0205 06:29:15.343463 12688 sgd_solver.cpp:106] Iteration 170, lr = 0.001
I0205 06:29:20.973203 12688 solver.cpp:237] Iteration 180, loss = 0.452436
I0205 06:29:20.973426 12688 solver.cpp:253]     Train net output #0: loss = 0.452436 (* 1 = 0.452436 loss)
I0205 06:29:20.973440 12688 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0205 06:29:26.719260 12688 solver.cpp:237] Iteration 190, loss = 0.531057
I0205 06:29:26.719318 12688 solver.cpp:253]     Train net output #0: loss = 0.531058 (* 1 = 0.531058 loss)
I0205 06:29:26.719331 12688 sgd_solver.cpp:106] Iteration 190, lr = 0.001
I0205 06:29:31.932201 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_200.caffemodel
I0205 06:29:31.934198 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_200.solverstate
I0205 06:29:31.935009 12688 solver.cpp:341] Iteration 200, Testing net (#0)
I0205 06:29:34.798668 12688 solver.cpp:409]     Test net output #0: accuracy = 0.9
I0205 06:29:34.798715 12688 solver.cpp:409]     Test net output #1: loss = 0.267897 (* 1 = 0.267897 loss)
I0205 06:29:35.385192 12688 solver.cpp:237] Iteration 200, loss = 0.370352
I0205 06:29:35.385241 12688 solver.cpp:253]     Train net output #0: loss = 0.370352 (* 1 = 0.370352 loss)
I0205 06:29:35.385252 12688 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0205 06:29:41.191822 12688 solver.cpp:237] Iteration 210, loss = 0.302589
I0205 06:29:41.191874 12688 solver.cpp:253]     Train net output #0: loss = 0.302589 (* 1 = 0.302589 loss)
I0205 06:29:41.191884 12688 sgd_solver.cpp:106] Iteration 210, lr = 0.001
I0205 06:29:47.013120 12688 solver.cpp:237] Iteration 220, loss = 0.263186
I0205 06:29:47.013173 12688 solver.cpp:253]     Train net output #0: loss = 0.263186 (* 1 = 0.263186 loss)
I0205 06:29:47.013185 12688 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0205 06:29:52.917481 12688 solver.cpp:237] Iteration 230, loss = 0.246498
I0205 06:29:52.917680 12688 solver.cpp:253]     Train net output #0: loss = 0.246499 (* 1 = 0.246499 loss)
I0205 06:29:52.917693 12688 sgd_solver.cpp:106] Iteration 230, lr = 0.001
I0205 06:29:58.932478 12688 solver.cpp:237] Iteration 240, loss = 0.252415
I0205 06:29:58.932533 12688 solver.cpp:253]     Train net output #0: loss = 0.252415 (* 1 = 0.252415 loss)
I0205 06:29:58.932544 12688 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0205 06:30:04.953845 12688 solver.cpp:237] Iteration 250, loss = 0.339506
I0205 06:30:04.953901 12688 solver.cpp:253]     Train net output #0: loss = 0.339506 (* 1 = 0.339506 loss)
I0205 06:30:04.953912 12688 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0205 06:30:11.045286 12688 solver.cpp:237] Iteration 260, loss = 0.34022
I0205 06:30:11.045341 12688 solver.cpp:253]     Train net output #0: loss = 0.34022 (* 1 = 0.34022 loss)
I0205 06:30:11.045352 12688 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0205 06:30:17.146945 12688 solver.cpp:237] Iteration 270, loss = 0.292398
I0205 06:30:17.147017 12688 solver.cpp:253]     Train net output #0: loss = 0.292398 (* 1 = 0.292398 loss)
I0205 06:30:17.147029 12688 sgd_solver.cpp:106] Iteration 270, lr = 0.001
I0205 06:30:23.084470 12688 solver.cpp:237] Iteration 280, loss = 0.317241
I0205 06:30:23.084688 12688 solver.cpp:253]     Train net output #0: loss = 0.317241 (* 1 = 0.317241 loss)
I0205 06:30:23.084702 12688 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0205 06:30:28.979288 12688 solver.cpp:237] Iteration 290, loss = 0.255291
I0205 06:30:28.979346 12688 solver.cpp:253]     Train net output #0: loss = 0.255291 (* 1 = 0.255291 loss)
I0205 06:30:28.979357 12688 sgd_solver.cpp:106] Iteration 290, lr = 0.001
I0205 06:30:34.304821 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_300.caffemodel
I0205 06:30:34.307018 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_300.solverstate
I0205 06:30:34.307812 12688 solver.cpp:341] Iteration 300, Testing net (#0)
I0205 06:30:37.259667 12688 solver.cpp:409]     Test net output #0: accuracy = 0.928
I0205 06:30:37.259716 12688 solver.cpp:409]     Test net output #1: loss = 0.200818 (* 1 = 0.200818 loss)
I0205 06:30:37.858224 12688 solver.cpp:237] Iteration 300, loss = 0.198084
I0205 06:30:37.858273 12688 solver.cpp:253]     Train net output #0: loss = 0.198084 (* 1 = 0.198084 loss)
I0205 06:30:37.858284 12688 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0205 06:30:43.900712 12688 solver.cpp:237] Iteration 310, loss = 0.203081
I0205 06:30:43.900768 12688 solver.cpp:253]     Train net output #0: loss = 0.203081 (* 1 = 0.203081 loss)
I0205 06:30:43.900779 12688 sgd_solver.cpp:106] Iteration 310, lr = 0.001
I0205 06:30:49.977018 12688 solver.cpp:237] Iteration 320, loss = 0.217098
I0205 06:30:49.977080 12688 solver.cpp:253]     Train net output #0: loss = 0.217098 (* 1 = 0.217098 loss)
I0205 06:30:49.977092 12688 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0205 06:30:56.026350 12688 solver.cpp:237] Iteration 330, loss = 0.22703
I0205 06:30:56.026543 12688 solver.cpp:253]     Train net output #0: loss = 0.22703 (* 1 = 0.22703 loss)
I0205 06:30:56.026556 12688 sgd_solver.cpp:106] Iteration 330, lr = 0.001
I0205 06:31:02.107841 12688 solver.cpp:237] Iteration 340, loss = 0.146412
I0205 06:31:02.107899 12688 solver.cpp:253]     Train net output #0: loss = 0.146412 (* 1 = 0.146412 loss)
I0205 06:31:02.107911 12688 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0205 06:31:08.196269 12688 solver.cpp:237] Iteration 350, loss = 0.316277
I0205 06:31:08.196326 12688 solver.cpp:253]     Train net output #0: loss = 0.316277 (* 1 = 0.316277 loss)
I0205 06:31:08.196338 12688 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I0205 06:31:14.282763 12688 solver.cpp:237] Iteration 360, loss = 0.249935
I0205 06:31:14.282819 12688 solver.cpp:253]     Train net output #0: loss = 0.249935 (* 1 = 0.249935 loss)
I0205 06:31:14.282830 12688 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0205 06:31:20.353987 12688 solver.cpp:237] Iteration 370, loss = 0.242804
I0205 06:31:20.354043 12688 solver.cpp:253]     Train net output #0: loss = 0.242804 (* 1 = 0.242804 loss)
I0205 06:31:20.354054 12688 sgd_solver.cpp:106] Iteration 370, lr = 0.001
I0205 06:31:26.414908 12688 solver.cpp:237] Iteration 380, loss = 0.0895525
I0205 06:31:26.415105 12688 solver.cpp:253]     Train net output #0: loss = 0.0895527 (* 1 = 0.0895527 loss)
I0205 06:31:26.415119 12688 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0205 06:31:32.478960 12688 solver.cpp:237] Iteration 390, loss = 0.134631
I0205 06:31:32.479022 12688 solver.cpp:253]     Train net output #0: loss = 0.134631 (* 1 = 0.134631 loss)
I0205 06:31:32.479032 12688 sgd_solver.cpp:106] Iteration 390, lr = 0.001
I0205 06:31:37.969450 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_400.caffemodel
I0205 06:31:37.971464 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_400.solverstate
I0205 06:31:37.972281 12688 solver.cpp:341] Iteration 400, Testing net (#0)
I0205 06:31:40.989322 12688 solver.cpp:409]     Test net output #0: accuracy = 0.944
I0205 06:31:40.989369 12688 solver.cpp:409]     Test net output #1: loss = 0.176655 (* 1 = 0.176655 loss)
I0205 06:31:41.601723 12688 solver.cpp:237] Iteration 400, loss = 0.313659
I0205 06:31:41.601773 12688 solver.cpp:253]     Train net output #0: loss = 0.313659 (* 1 = 0.313659 loss)
I0205 06:31:41.601783 12688 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0205 06:31:47.722080 12688 solver.cpp:237] Iteration 410, loss = 0.175015
I0205 06:31:47.722136 12688 solver.cpp:253]     Train net output #0: loss = 0.175016 (* 1 = 0.175016 loss)
I0205 06:31:47.722146 12688 sgd_solver.cpp:106] Iteration 410, lr = 0.001
I0205 06:31:53.799523 12688 solver.cpp:237] Iteration 420, loss = 0.0836414
I0205 06:31:53.799578 12688 solver.cpp:253]     Train net output #0: loss = 0.0836417 (* 1 = 0.0836417 loss)
I0205 06:31:53.799589 12688 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0205 06:31:59.817488 12688 solver.cpp:237] Iteration 430, loss = 0.119715
I0205 06:31:59.817705 12688 solver.cpp:253]     Train net output #0: loss = 0.119715 (* 1 = 0.119715 loss)
I0205 06:31:59.817719 12688 sgd_solver.cpp:106] Iteration 430, lr = 0.001
I0205 06:32:05.779997 12688 solver.cpp:237] Iteration 440, loss = 0.179931
I0205 06:32:05.780055 12688 solver.cpp:253]     Train net output #0: loss = 0.179932 (* 1 = 0.179932 loss)
I0205 06:32:05.780066 12688 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0205 06:32:11.713577 12688 solver.cpp:237] Iteration 450, loss = 0.114803
I0205 06:32:11.713630 12688 solver.cpp:253]     Train net output #0: loss = 0.114803 (* 1 = 0.114803 loss)
I0205 06:32:11.713641 12688 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I0205 06:32:17.710554 12688 solver.cpp:237] Iteration 460, loss = 0.122351
I0205 06:32:17.710613 12688 solver.cpp:253]     Train net output #0: loss = 0.122351 (* 1 = 0.122351 loss)
I0205 06:32:17.710623 12688 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0205 06:32:23.768457 12688 solver.cpp:237] Iteration 470, loss = 0.153604
I0205 06:32:23.768512 12688 solver.cpp:253]     Train net output #0: loss = 0.153604 (* 1 = 0.153604 loss)
I0205 06:32:23.768523 12688 sgd_solver.cpp:106] Iteration 470, lr = 0.001
I0205 06:32:29.860976 12688 solver.cpp:237] Iteration 480, loss = 0.142779
I0205 06:32:29.861207 12688 solver.cpp:253]     Train net output #0: loss = 0.142779 (* 1 = 0.142779 loss)
I0205 06:32:29.861220 12688 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0205 06:32:35.936662 12688 solver.cpp:237] Iteration 490, loss = 0.173748
I0205 06:32:35.936719 12688 solver.cpp:253]     Train net output #0: loss = 0.173749 (* 1 = 0.173749 loss)
I0205 06:32:35.936730 12688 sgd_solver.cpp:106] Iteration 490, lr = 0.001
I0205 06:32:41.433346 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_500.caffemodel
I0205 06:32:41.435395 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_500.solverstate
I0205 06:32:41.436190 12688 solver.cpp:341] Iteration 500, Testing net (#0)
I0205 06:32:44.432122 12688 solver.cpp:409]     Test net output #0: accuracy = 0.951
I0205 06:32:44.432169 12688 solver.cpp:409]     Test net output #1: loss = 0.156085 (* 1 = 0.156085 loss)
I0205 06:32:45.042490 12688 solver.cpp:237] Iteration 500, loss = 0.171806
I0205 06:32:45.042542 12688 solver.cpp:253]     Train net output #0: loss = 0.171807 (* 1 = 0.171807 loss)
I0205 06:32:45.042553 12688 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0205 06:32:51.138170 12688 solver.cpp:237] Iteration 510, loss = 0.203712
I0205 06:32:51.138223 12688 solver.cpp:253]     Train net output #0: loss = 0.203712 (* 1 = 0.203712 loss)
I0205 06:32:51.138234 12688 sgd_solver.cpp:106] Iteration 510, lr = 0.001
I0205 06:32:57.144413 12688 solver.cpp:237] Iteration 520, loss = 0.171104
I0205 06:32:57.415387 12688 solver.cpp:253]     Train net output #0: loss = 0.171104 (* 1 = 0.171104 loss)
I0205 06:32:57.415402 12688 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I0205 06:33:03.463068 12688 solver.cpp:237] Iteration 530, loss = 0.13654
I0205 06:33:03.463276 12688 solver.cpp:253]     Train net output #0: loss = 0.136541 (* 1 = 0.136541 loss)
I0205 06:33:03.463289 12688 sgd_solver.cpp:106] Iteration 530, lr = 0.001
I0205 06:33:09.430649 12688 solver.cpp:237] Iteration 540, loss = 0.177163
I0205 06:33:09.430703 12688 solver.cpp:253]     Train net output #0: loss = 0.177163 (* 1 = 0.177163 loss)
I0205 06:33:09.430714 12688 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I0205 06:33:15.418258 12688 solver.cpp:237] Iteration 550, loss = 0.126538
I0205 06:33:15.418318 12688 solver.cpp:253]     Train net output #0: loss = 0.126539 (* 1 = 0.126539 loss)
I0205 06:33:15.418329 12688 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I0205 06:33:21.517539 12688 solver.cpp:237] Iteration 560, loss = 0.164576
I0205 06:33:21.517593 12688 solver.cpp:253]     Train net output #0: loss = 0.164576 (* 1 = 0.164576 loss)
I0205 06:33:21.517606 12688 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I0205 06:33:27.584545 12688 solver.cpp:237] Iteration 570, loss = 0.187635
I0205 06:33:27.584599 12688 solver.cpp:253]     Train net output #0: loss = 0.187636 (* 1 = 0.187636 loss)
I0205 06:33:27.584610 12688 sgd_solver.cpp:106] Iteration 570, lr = 0.001
I0205 06:33:33.669978 12688 solver.cpp:237] Iteration 580, loss = 0.144387
I0205 06:33:33.670174 12688 solver.cpp:253]     Train net output #0: loss = 0.144387 (* 1 = 0.144387 loss)
I0205 06:33:33.670188 12688 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I0205 06:33:39.793762 12688 solver.cpp:237] Iteration 590, loss = 0.231645
I0205 06:33:39.793818 12688 solver.cpp:253]     Train net output #0: loss = 0.231646 (* 1 = 0.231646 loss)
I0205 06:33:39.793829 12688 sgd_solver.cpp:106] Iteration 590, lr = 0.001
I0205 06:33:45.311655 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_600.caffemodel
I0205 06:33:45.313659 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_600.solverstate
I0205 06:33:45.314481 12688 solver.cpp:341] Iteration 600, Testing net (#0)
I0205 06:33:48.370780 12688 solver.cpp:409]     Test net output #0: accuracy = 0.95
I0205 06:33:48.370828 12688 solver.cpp:409]     Test net output #1: loss = 0.159851 (* 1 = 0.159851 loss)
I0205 06:33:48.991351 12688 solver.cpp:237] Iteration 600, loss = 0.294152
I0205 06:33:48.991401 12688 solver.cpp:253]     Train net output #0: loss = 0.294152 (* 1 = 0.294152 loss)
I0205 06:33:48.991412 12688 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0205 06:33:55.105902 12688 solver.cpp:237] Iteration 610, loss = 0.10878
I0205 06:33:55.105959 12688 solver.cpp:253]     Train net output #0: loss = 0.10878 (* 1 = 0.10878 loss)
I0205 06:33:55.105978 12688 sgd_solver.cpp:106] Iteration 610, lr = 0.001
I0205 06:34:01.220084 12688 solver.cpp:237] Iteration 620, loss = 0.102928
I0205 06:34:01.220137 12688 solver.cpp:253]     Train net output #0: loss = 0.102928 (* 1 = 0.102928 loss)
I0205 06:34:01.220149 12688 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I0205 06:34:07.314903 12688 solver.cpp:237] Iteration 630, loss = 0.207439
I0205 06:34:07.315105 12688 solver.cpp:253]     Train net output #0: loss = 0.20744 (* 1 = 0.20744 loss)
I0205 06:34:07.315119 12688 sgd_solver.cpp:106] Iteration 630, lr = 0.001
I0205 06:34:13.438351 12688 solver.cpp:237] Iteration 640, loss = 0.176394
I0205 06:34:13.438426 12688 solver.cpp:253]     Train net output #0: loss = 0.176394 (* 1 = 0.176394 loss)
I0205 06:34:13.438436 12688 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I0205 06:34:19.585875 12688 solver.cpp:237] Iteration 650, loss = 0.210033
I0205 06:34:19.585930 12688 solver.cpp:253]     Train net output #0: loss = 0.210033 (* 1 = 0.210033 loss)
I0205 06:34:19.585942 12688 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I0205 06:34:25.834293 12688 solver.cpp:237] Iteration 660, loss = 0.209407
I0205 06:34:25.834347 12688 solver.cpp:253]     Train net output #0: loss = 0.209407 (* 1 = 0.209407 loss)
I0205 06:34:25.834358 12688 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I0205 06:34:32.100723 12688 solver.cpp:237] Iteration 670, loss = 0.348017
I0205 06:34:32.100780 12688 solver.cpp:253]     Train net output #0: loss = 0.348017 (* 1 = 0.348017 loss)
I0205 06:34:32.100791 12688 sgd_solver.cpp:106] Iteration 670, lr = 0.001
I0205 06:34:38.346436 12688 solver.cpp:237] Iteration 680, loss = 0.240354
I0205 06:34:38.346643 12688 solver.cpp:253]     Train net output #0: loss = 0.240354 (* 1 = 0.240354 loss)
I0205 06:34:38.346657 12688 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I0205 06:34:44.616529 12688 solver.cpp:237] Iteration 690, loss = 0.127166
I0205 06:34:44.616586 12688 solver.cpp:253]     Train net output #0: loss = 0.127167 (* 1 = 0.127167 loss)
I0205 06:34:44.616598 12688 sgd_solver.cpp:106] Iteration 690, lr = 0.001
I0205 06:34:50.271862 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_700.caffemodel
I0205 06:34:50.273871 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_700.solverstate
I0205 06:34:50.274659 12688 solver.cpp:341] Iteration 700, Testing net (#0)
I0205 06:34:53.406874 12688 solver.cpp:409]     Test net output #0: accuracy = 0.954
I0205 06:34:53.406921 12688 solver.cpp:409]     Test net output #1: loss = 0.107993 (* 1 = 0.107993 loss)
I0205 06:34:54.046341 12688 solver.cpp:237] Iteration 700, loss = 0.169291
I0205 06:34:54.046392 12688 solver.cpp:253]     Train net output #0: loss = 0.169291 (* 1 = 0.169291 loss)
I0205 06:34:54.046403 12688 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0205 06:35:00.432334 12688 solver.cpp:237] Iteration 710, loss = 0.101807
I0205 06:35:00.432394 12688 solver.cpp:253]     Train net output #0: loss = 0.101808 (* 1 = 0.101808 loss)
I0205 06:35:00.432405 12688 sgd_solver.cpp:106] Iteration 710, lr = 0.001
I0205 06:35:06.969414 12688 solver.cpp:237] Iteration 720, loss = 0.0831723
I0205 06:35:06.969470 12688 solver.cpp:253]     Train net output #0: loss = 0.0831725 (* 1 = 0.0831725 loss)
I0205 06:35:06.969480 12688 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I0205 06:35:13.539192 12688 solver.cpp:237] Iteration 730, loss = 0.168839
I0205 06:35:13.539389 12688 solver.cpp:253]     Train net output #0: loss = 0.168839 (* 1 = 0.168839 loss)
I0205 06:35:13.539403 12688 sgd_solver.cpp:106] Iteration 730, lr = 0.001
I0205 06:35:20.167001 12688 solver.cpp:237] Iteration 740, loss = 0.0801717
I0205 06:35:20.167059 12688 solver.cpp:253]     Train net output #0: loss = 0.080172 (* 1 = 0.080172 loss)
I0205 06:35:20.167070 12688 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I0205 06:35:26.841718 12688 solver.cpp:237] Iteration 750, loss = 0.0647314
I0205 06:35:26.841776 12688 solver.cpp:253]     Train net output #0: loss = 0.0647316 (* 1 = 0.0647316 loss)
I0205 06:35:26.841787 12688 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I0205 06:35:33.517299 12688 solver.cpp:237] Iteration 760, loss = 0.0577709
I0205 06:35:33.517354 12688 solver.cpp:253]     Train net output #0: loss = 0.0577712 (* 1 = 0.0577712 loss)
I0205 06:35:33.517365 12688 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I0205 06:35:40.204010 12688 solver.cpp:237] Iteration 770, loss = 0.0453771
I0205 06:35:40.204066 12688 solver.cpp:253]     Train net output #0: loss = 0.0453774 (* 1 = 0.0453774 loss)
I0205 06:35:40.204089 12688 sgd_solver.cpp:106] Iteration 770, lr = 0.001
I0205 06:35:46.984932 12688 solver.cpp:237] Iteration 780, loss = 0.0276716
I0205 06:35:46.985164 12688 solver.cpp:253]     Train net output #0: loss = 0.0276719 (* 1 = 0.0276719 loss)
I0205 06:35:46.985178 12688 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I0205 06:35:53.746243 12688 solver.cpp:237] Iteration 790, loss = 0.0945593
I0205 06:35:53.746299 12688 solver.cpp:253]     Train net output #0: loss = 0.0945596 (* 1 = 0.0945596 loss)
I0205 06:35:53.746310 12688 sgd_solver.cpp:106] Iteration 790, lr = 0.001
I0205 06:35:59.846395 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_800.caffemodel
I0205 06:35:59.848387 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_800.solverstate
I0205 06:35:59.849179 12688 solver.cpp:341] Iteration 800, Testing net (#0)
I0205 06:36:03.184777 12688 solver.cpp:409]     Test net output #0: accuracy = 0.994
I0205 06:36:03.184826 12688 solver.cpp:409]     Test net output #1: loss = 0.0256235 (* 1 = 0.0256235 loss)
I0205 06:36:03.868875 12688 solver.cpp:237] Iteration 800, loss = 0.02377
I0205 06:36:03.868927 12688 solver.cpp:253]     Train net output #0: loss = 0.0237702 (* 1 = 0.0237702 loss)
I0205 06:36:03.868937 12688 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0205 06:36:10.596150 12688 solver.cpp:237] Iteration 810, loss = 0.0954225
I0205 06:36:10.596210 12688 solver.cpp:253]     Train net output #0: loss = 0.0954227 (* 1 = 0.0954227 loss)
I0205 06:36:10.596220 12688 sgd_solver.cpp:106] Iteration 810, lr = 0.001
I0205 06:36:17.301529 12688 solver.cpp:237] Iteration 820, loss = 0.070476
I0205 06:36:17.301724 12688 solver.cpp:253]     Train net output #0: loss = 0.0704762 (* 1 = 0.0704762 loss)
I0205 06:36:17.301738 12688 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I0205 06:36:24.057425 12688 solver.cpp:237] Iteration 830, loss = 0.122494
I0205 06:36:24.057484 12688 solver.cpp:253]     Train net output #0: loss = 0.122494 (* 1 = 0.122494 loss)
I0205 06:36:24.057495 12688 sgd_solver.cpp:106] Iteration 830, lr = 0.001
I0205 06:36:30.810524 12688 solver.cpp:237] Iteration 840, loss = 0.0200693
I0205 06:36:30.810580 12688 solver.cpp:253]     Train net output #0: loss = 0.0200696 (* 1 = 0.0200696 loss)
I0205 06:36:30.810591 12688 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I0205 06:36:37.583935 12688 solver.cpp:237] Iteration 850, loss = 0.0310545
I0205 06:36:37.583997 12688 solver.cpp:253]     Train net output #0: loss = 0.0310548 (* 1 = 0.0310548 loss)
I0205 06:36:37.584007 12688 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I0205 06:36:44.389917 12688 solver.cpp:237] Iteration 860, loss = 0.0205287
I0205 06:36:44.389979 12688 solver.cpp:253]     Train net output #0: loss = 0.0205289 (* 1 = 0.0205289 loss)
I0205 06:36:44.389991 12688 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I0205 06:36:51.255899 12688 solver.cpp:237] Iteration 870, loss = 0.0243706
I0205 06:36:51.256093 12688 solver.cpp:253]     Train net output #0: loss = 0.0243709 (* 1 = 0.0243709 loss)
I0205 06:36:51.256108 12688 sgd_solver.cpp:106] Iteration 870, lr = 0.001
I0205 06:36:58.123493 12688 solver.cpp:237] Iteration 880, loss = 0.015242
I0205 06:36:58.123550 12688 solver.cpp:253]     Train net output #0: loss = 0.0152422 (* 1 = 0.0152422 loss)
I0205 06:36:58.123563 12688 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I0205 06:37:04.905617 12688 solver.cpp:237] Iteration 890, loss = 0.0107296
I0205 06:37:04.905675 12688 solver.cpp:253]     Train net output #0: loss = 0.0107299 (* 1 = 0.0107299 loss)
I0205 06:37:04.905686 12688 sgd_solver.cpp:106] Iteration 890, lr = 0.001
I0205 06:37:11.050302 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_900.caffemodel
I0205 06:37:11.052278 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_900.solverstate
I0205 06:37:11.053081 12688 solver.cpp:341] Iteration 900, Testing net (#0)
I0205 06:37:14.453563 12688 solver.cpp:409]     Test net output #0: accuracy = 0.995
I0205 06:37:14.453608 12688 solver.cpp:409]     Test net output #1: loss = 0.0250514 (* 1 = 0.0250514 loss)
I0205 06:37:15.155956 12688 solver.cpp:237] Iteration 900, loss = 0.0436762
I0205 06:37:15.156010 12688 solver.cpp:253]     Train net output #0: loss = 0.0436765 (* 1 = 0.0436765 loss)
I0205 06:37:15.156023 12688 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0205 06:37:22.045780 12688 solver.cpp:237] Iteration 910, loss = 0.0448464
I0205 06:37:22.046007 12688 solver.cpp:253]     Train net output #0: loss = 0.0448467 (* 1 = 0.0448467 loss)
I0205 06:37:22.046021 12688 sgd_solver.cpp:106] Iteration 910, lr = 0.001
I0205 06:37:28.931929 12688 solver.cpp:237] Iteration 920, loss = 0.022417
I0205 06:37:28.931988 12688 solver.cpp:253]     Train net output #0: loss = 0.0224172 (* 1 = 0.0224172 loss)
I0205 06:37:28.931999 12688 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I0205 06:37:35.772359 12688 solver.cpp:237] Iteration 930, loss = 0.0337543
I0205 06:37:35.772413 12688 solver.cpp:253]     Train net output #0: loss = 0.0337546 (* 1 = 0.0337546 loss)
I0205 06:37:35.772424 12688 sgd_solver.cpp:106] Iteration 930, lr = 0.001
I0205 06:37:42.558372 12688 solver.cpp:237] Iteration 940, loss = 0.00902651
I0205 06:37:42.558424 12688 solver.cpp:253]     Train net output #0: loss = 0.00902677 (* 1 = 0.00902677 loss)
I0205 06:37:42.558434 12688 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I0205 06:37:49.371920 12688 solver.cpp:237] Iteration 950, loss = 0.00657252
I0205 06:37:49.371984 12688 solver.cpp:253]     Train net output #0: loss = 0.00657278 (* 1 = 0.00657278 loss)
I0205 06:37:49.371994 12688 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I0205 06:37:56.169529 12688 solver.cpp:237] Iteration 960, loss = 0.0566958
I0205 06:37:56.169716 12688 solver.cpp:253]     Train net output #0: loss = 0.0566961 (* 1 = 0.0566961 loss)
I0205 06:37:56.169729 12688 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I0205 06:38:02.888362 12688 solver.cpp:237] Iteration 970, loss = 0.040702
I0205 06:38:02.888420 12688 solver.cpp:253]     Train net output #0: loss = 0.0407023 (* 1 = 0.0407023 loss)
I0205 06:38:02.888432 12688 sgd_solver.cpp:106] Iteration 970, lr = 0.001
I0205 06:38:09.601338 12688 solver.cpp:237] Iteration 980, loss = 0.0612301
I0205 06:38:09.601395 12688 solver.cpp:253]     Train net output #0: loss = 0.0612303 (* 1 = 0.0612303 loss)
I0205 06:38:09.601408 12688 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I0205 06:38:16.393780 12688 solver.cpp:237] Iteration 990, loss = 0.0148544
I0205 06:38:16.393837 12688 solver.cpp:253]     Train net output #0: loss = 0.0148547 (* 1 = 0.0148547 loss)
I0205 06:38:16.393848 12688 sgd_solver.cpp:106] Iteration 990, lr = 0.001
I0205 06:38:22.522027 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1000.caffemodel
I0205 06:38:22.524016 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1000.solverstate
I0205 06:38:22.524806 12688 solver.cpp:341] Iteration 1000, Testing net (#0)
I0205 06:38:25.916074 12688 solver.cpp:409]     Test net output #0: accuracy = 0.994
I0205 06:38:25.916126 12688 solver.cpp:409]     Test net output #1: loss = 0.0131438 (* 1 = 0.0131438 loss)
I0205 06:38:26.605660 12688 solver.cpp:237] Iteration 1000, loss = 0.00513159
I0205 06:38:26.605875 12688 solver.cpp:253]     Train net output #0: loss = 0.00513185 (* 1 = 0.00513185 loss)
I0205 06:38:26.605887 12688 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0205 06:38:33.479889 12688 solver.cpp:237] Iteration 1010, loss = 0.00504734
I0205 06:38:33.479943 12688 solver.cpp:253]     Train net output #0: loss = 0.00504759 (* 1 = 0.00504759 loss)
I0205 06:38:33.479954 12688 sgd_solver.cpp:106] Iteration 1010, lr = 0.001
I0205 06:38:40.326766 12688 solver.cpp:237] Iteration 1020, loss = 0.00343173
I0205 06:38:40.326819 12688 solver.cpp:253]     Train net output #0: loss = 0.00343199 (* 1 = 0.00343199 loss)
I0205 06:38:40.326830 12688 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I0205 06:38:47.182770 12688 solver.cpp:237] Iteration 1030, loss = 0.0230963
I0205 06:38:47.182826 12688 solver.cpp:253]     Train net output #0: loss = 0.0230965 (* 1 = 0.0230965 loss)
I0205 06:38:47.182837 12688 sgd_solver.cpp:106] Iteration 1030, lr = 0.001
I0205 06:38:54.039839 12688 solver.cpp:237] Iteration 1040, loss = 0.0130104
I0205 06:38:54.039896 12688 solver.cpp:253]     Train net output #0: loss = 0.0130106 (* 1 = 0.0130106 loss)
I0205 06:38:54.039906 12688 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I0205 06:39:00.900398 12688 solver.cpp:237] Iteration 1050, loss = 0.0231454
I0205 06:39:00.900580 12688 solver.cpp:253]     Train net output #0: loss = 0.0231457 (* 1 = 0.0231457 loss)
I0205 06:39:00.900593 12688 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I0205 06:39:07.775923 12688 solver.cpp:237] Iteration 1060, loss = 0.0281731
I0205 06:39:07.775990 12688 solver.cpp:253]     Train net output #0: loss = 0.0281734 (* 1 = 0.0281734 loss)
I0205 06:39:07.776002 12688 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I0205 06:39:14.628712 12688 solver.cpp:237] Iteration 1070, loss = 0.0416825
I0205 06:39:14.628768 12688 solver.cpp:253]     Train net output #0: loss = 0.0416828 (* 1 = 0.0416828 loss)
I0205 06:39:14.628779 12688 sgd_solver.cpp:106] Iteration 1070, lr = 0.001
I0205 06:39:21.459893 12688 solver.cpp:237] Iteration 1080, loss = 0.0157729
I0205 06:39:21.459951 12688 solver.cpp:253]     Train net output #0: loss = 0.0157732 (* 1 = 0.0157732 loss)
I0205 06:39:21.459962 12688 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I0205 06:39:28.233875 12688 solver.cpp:237] Iteration 1090, loss = 0.00530226
I0205 06:39:28.233928 12688 solver.cpp:253]     Train net output #0: loss = 0.00530254 (* 1 = 0.00530254 loss)
I0205 06:39:28.233939 12688 sgd_solver.cpp:106] Iteration 1090, lr = 0.001
I0205 06:39:34.334334 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1100.caffemodel
I0205 06:39:34.336524 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1100.solverstate
I0205 06:39:34.337334 12688 solver.cpp:341] Iteration 1100, Testing net (#0)
I0205 06:39:37.683794 12688 solver.cpp:409]     Test net output #0: accuracy = 0.997
I0205 06:39:37.683845 12688 solver.cpp:409]     Test net output #1: loss = 0.0127267 (* 1 = 0.0127267 loss)
I0205 06:39:38.379230 12688 solver.cpp:237] Iteration 1100, loss = 0.00398588
I0205 06:39:38.379281 12688 solver.cpp:253]     Train net output #0: loss = 0.00398615 (* 1 = 0.00398615 loss)
I0205 06:39:38.379292 12688 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I0205 06:39:45.237078 12688 solver.cpp:237] Iteration 1110, loss = 0.00723525
I0205 06:39:45.237131 12688 solver.cpp:253]     Train net output #0: loss = 0.00723552 (* 1 = 0.00723552 loss)
I0205 06:39:45.237143 12688 sgd_solver.cpp:106] Iteration 1110, lr = 0.001
I0205 06:39:52.103763 12688 solver.cpp:237] Iteration 1120, loss = 0.0477661
I0205 06:39:52.103821 12688 solver.cpp:253]     Train net output #0: loss = 0.0477664 (* 1 = 0.0477664 loss)
I0205 06:39:52.103832 12688 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I0205 06:39:58.948544 12688 solver.cpp:237] Iteration 1130, loss = 0.0331947
I0205 06:39:58.948601 12688 solver.cpp:253]     Train net output #0: loss = 0.033195 (* 1 = 0.033195 loss)
I0205 06:39:58.948624 12688 sgd_solver.cpp:106] Iteration 1130, lr = 0.001
I0205 06:40:05.790459 12688 solver.cpp:237] Iteration 1140, loss = 0.00328222
I0205 06:40:05.790688 12688 solver.cpp:253]     Train net output #0: loss = 0.0032825 (* 1 = 0.0032825 loss)
I0205 06:40:05.790701 12688 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I0205 06:40:12.589642 12688 solver.cpp:237] Iteration 1150, loss = 0.0194904
I0205 06:40:12.589697 12688 solver.cpp:253]     Train net output #0: loss = 0.0194907 (* 1 = 0.0194907 loss)
I0205 06:40:12.589709 12688 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I0205 06:40:19.336658 12688 solver.cpp:237] Iteration 1160, loss = 0.0410107
I0205 06:40:19.336714 12688 solver.cpp:253]     Train net output #0: loss = 0.041011 (* 1 = 0.041011 loss)
I0205 06:40:19.336725 12688 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I0205 06:40:26.081287 12688 solver.cpp:237] Iteration 1170, loss = 0.0175784
I0205 06:40:26.081342 12688 solver.cpp:253]     Train net output #0: loss = 0.0175786 (* 1 = 0.0175786 loss)
I0205 06:40:26.081352 12688 sgd_solver.cpp:106] Iteration 1170, lr = 0.001
I0205 06:40:32.812742 12688 solver.cpp:237] Iteration 1180, loss = 0.0405344
I0205 06:40:32.812799 12688 solver.cpp:253]     Train net output #0: loss = 0.0405347 (* 1 = 0.0405347 loss)
I0205 06:40:32.812810 12688 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I0205 06:40:39.570958 12688 solver.cpp:237] Iteration 1190, loss = 0.0038245
I0205 06:40:39.571157 12688 solver.cpp:253]     Train net output #0: loss = 0.00382477 (* 1 = 0.00382477 loss)
I0205 06:40:39.571171 12688 sgd_solver.cpp:106] Iteration 1190, lr = 0.001
I0205 06:40:45.718852 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1200.caffemodel
I0205 06:40:45.720849 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1200.solverstate
I0205 06:40:45.721642 12688 solver.cpp:341] Iteration 1200, Testing net (#0)
I0205 06:40:49.047118 12688 solver.cpp:409]     Test net output #0: accuracy = 0.995
I0205 06:40:49.047166 12688 solver.cpp:409]     Test net output #1: loss = 0.00921298 (* 1 = 0.00921298 loss)
I0205 06:40:49.721143 12688 solver.cpp:237] Iteration 1200, loss = 0.00184607
I0205 06:40:49.721194 12688 solver.cpp:253]     Train net output #0: loss = 0.00184635 (* 1 = 0.00184635 loss)
I0205 06:40:49.721204 12688 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0205 06:40:56.505089 12688 solver.cpp:237] Iteration 1210, loss = 0.0374381
I0205 06:40:56.505149 12688 solver.cpp:253]     Train net output #0: loss = 0.0374383 (* 1 = 0.0374383 loss)
I0205 06:40:56.505162 12688 sgd_solver.cpp:106] Iteration 1210, lr = 0.001
I0205 06:41:03.290957 12688 solver.cpp:237] Iteration 1220, loss = 0.0151575
I0205 06:41:03.291020 12688 solver.cpp:253]     Train net output #0: loss = 0.0151578 (* 1 = 0.0151578 loss)
I0205 06:41:03.291031 12688 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I0205 06:41:10.092700 12688 solver.cpp:237] Iteration 1230, loss = 0.00477063
I0205 06:41:10.092893 12688 solver.cpp:253]     Train net output #0: loss = 0.00477091 (* 1 = 0.00477091 loss)
I0205 06:41:10.092907 12688 sgd_solver.cpp:106] Iteration 1230, lr = 0.001
I0205 06:41:16.907382 12688 solver.cpp:237] Iteration 1240, loss = 0.0256504
I0205 06:41:16.907438 12688 solver.cpp:253]     Train net output #0: loss = 0.0256506 (* 1 = 0.0256506 loss)
I0205 06:41:16.907449 12688 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I0205 06:41:23.620666 12688 solver.cpp:237] Iteration 1250, loss = 0.00126491
I0205 06:41:23.620723 12688 solver.cpp:253]     Train net output #0: loss = 0.00126518 (* 1 = 0.00126518 loss)
I0205 06:41:23.620735 12688 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I0205 06:41:30.354609 12688 solver.cpp:237] Iteration 1260, loss = 0.00129954
I0205 06:41:30.354667 12688 solver.cpp:253]     Train net output #0: loss = 0.00129981 (* 1 = 0.00129981 loss)
I0205 06:41:30.354691 12688 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I0205 06:41:37.110188 12688 solver.cpp:237] Iteration 1270, loss = 0.0088288
I0205 06:41:37.110244 12688 solver.cpp:253]     Train net output #0: loss = 0.00882906 (* 1 = 0.00882906 loss)
I0205 06:41:37.110255 12688 sgd_solver.cpp:106] Iteration 1270, lr = 0.001
I0205 06:41:43.886833 12688 solver.cpp:237] Iteration 1280, loss = 0.0211926
I0205 06:41:43.887043 12688 solver.cpp:253]     Train net output #0: loss = 0.0211929 (* 1 = 0.0211929 loss)
I0205 06:41:43.887058 12688 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I0205 06:41:50.643734 12688 solver.cpp:237] Iteration 1290, loss = 0.0638773
I0205 06:41:50.643792 12688 solver.cpp:253]     Train net output #0: loss = 0.0638776 (* 1 = 0.0638776 loss)
I0205 06:41:50.643803 12688 sgd_solver.cpp:106] Iteration 1290, lr = 0.001
I0205 06:41:56.766566 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1300.caffemodel
I0205 06:41:56.768553 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1300.solverstate
I0205 06:41:56.769342 12688 solver.cpp:341] Iteration 1300, Testing net (#0)
I0205 06:42:00.113700 12688 solver.cpp:409]     Test net output #0: accuracy = 0.997
I0205 06:42:00.113747 12688 solver.cpp:409]     Test net output #1: loss = 0.00923616 (* 1 = 0.00923616 loss)
I0205 06:42:00.789327 12688 solver.cpp:237] Iteration 1300, loss = 0.00320147
I0205 06:42:00.789379 12688 solver.cpp:253]     Train net output #0: loss = 0.00320172 (* 1 = 0.00320172 loss)
I0205 06:42:00.789391 12688 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I0205 06:42:07.565642 12688 solver.cpp:237] Iteration 1310, loss = 0.0070707
I0205 06:42:07.565699 12688 solver.cpp:253]     Train net output #0: loss = 0.00707095 (* 1 = 0.00707095 loss)
I0205 06:42:07.565711 12688 sgd_solver.cpp:106] Iteration 1310, lr = 0.001
I0205 06:42:14.328851 12688 solver.cpp:237] Iteration 1320, loss = 0.00251616
I0205 06:42:14.329051 12688 solver.cpp:253]     Train net output #0: loss = 0.00251641 (* 1 = 0.00251641 loss)
I0205 06:42:14.329064 12688 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I0205 06:42:21.131686 12688 solver.cpp:237] Iteration 1330, loss = 0.0112273
I0205 06:42:21.131747 12688 solver.cpp:253]     Train net output #0: loss = 0.0112275 (* 1 = 0.0112275 loss)
I0205 06:42:21.131757 12688 sgd_solver.cpp:106] Iteration 1330, lr = 0.001
I0205 06:42:27.975643 12688 solver.cpp:237] Iteration 1340, loss = 0.00863568
I0205 06:42:27.975699 12688 solver.cpp:253]     Train net output #0: loss = 0.00863593 (* 1 = 0.00863593 loss)
I0205 06:42:27.975710 12688 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I0205 06:42:34.776463 12688 solver.cpp:237] Iteration 1350, loss = 0.00182646
I0205 06:42:34.776518 12688 solver.cpp:253]     Train net output #0: loss = 0.00182671 (* 1 = 0.00182671 loss)
I0205 06:42:34.776530 12688 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I0205 06:42:41.587412 12688 solver.cpp:237] Iteration 1360, loss = 0.0102678
I0205 06:42:41.587469 12688 solver.cpp:253]     Train net output #0: loss = 0.010268 (* 1 = 0.010268 loss)
I0205 06:42:41.587481 12688 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I0205 06:42:48.461051 12688 solver.cpp:237] Iteration 1370, loss = 0.0160526
I0205 06:42:48.461236 12688 solver.cpp:253]     Train net output #0: loss = 0.0160529 (* 1 = 0.0160529 loss)
I0205 06:42:48.461248 12688 sgd_solver.cpp:106] Iteration 1370, lr = 0.001
I0205 06:42:55.336976 12688 solver.cpp:237] Iteration 1380, loss = 0.0236281
I0205 06:42:55.337034 12688 solver.cpp:253]     Train net output #0: loss = 0.0236283 (* 1 = 0.0236283 loss)
I0205 06:42:55.337045 12688 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I0205 06:43:02.147212 12688 solver.cpp:237] Iteration 1390, loss = 0.00865302
I0205 06:43:02.147270 12688 solver.cpp:253]     Train net output #0: loss = 0.00865327 (* 1 = 0.00865327 loss)
I0205 06:43:02.147294 12688 sgd_solver.cpp:106] Iteration 1390, lr = 0.001
I0205 06:43:08.306671 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1400.caffemodel
I0205 06:43:08.308650 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1400.solverstate
I0205 06:43:08.309439 12688 solver.cpp:341] Iteration 1400, Testing net (#0)
I0205 06:43:11.632010 12688 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 06:43:11.632056 12688 solver.cpp:409]     Test net output #1: loss = 0.00735 (* 1 = 0.00735 loss)
I0205 06:43:12.305918 12688 solver.cpp:237] Iteration 1400, loss = 0.00584539
I0205 06:43:12.305975 12688 solver.cpp:253]     Train net output #0: loss = 0.00584564 (* 1 = 0.00584564 loss)
I0205 06:43:12.305985 12688 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0205 06:43:19.050412 12688 solver.cpp:237] Iteration 1410, loss = 0.00371312
I0205 06:43:19.050623 12688 solver.cpp:253]     Train net output #0: loss = 0.00371338 (* 1 = 0.00371338 loss)
I0205 06:43:19.050637 12688 sgd_solver.cpp:106] Iteration 1410, lr = 0.001
I0205 06:43:25.858185 12688 solver.cpp:237] Iteration 1420, loss = 0.0229777
I0205 06:43:25.858240 12688 solver.cpp:253]     Train net output #0: loss = 0.022978 (* 1 = 0.022978 loss)
I0205 06:43:25.858252 12688 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I0205 06:43:32.731541 12688 solver.cpp:237] Iteration 1430, loss = 0.00320468
I0205 06:43:32.731600 12688 solver.cpp:253]     Train net output #0: loss = 0.00320494 (* 1 = 0.00320494 loss)
I0205 06:43:32.731611 12688 sgd_solver.cpp:106] Iteration 1430, lr = 0.001
I0205 06:43:39.581791 12688 solver.cpp:237] Iteration 1440, loss = 0.0129024
I0205 06:43:39.581850 12688 solver.cpp:253]     Train net output #0: loss = 0.0129026 (* 1 = 0.0129026 loss)
I0205 06:43:39.581861 12688 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I0205 06:43:46.443524 12688 solver.cpp:237] Iteration 1450, loss = 0.00145947
I0205 06:43:46.443580 12688 solver.cpp:253]     Train net output #0: loss = 0.00145973 (* 1 = 0.00145973 loss)
I0205 06:43:46.443591 12688 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I0205 06:43:53.355362 12688 solver.cpp:237] Iteration 1460, loss = 0.134356
I0205 06:43:53.355547 12688 solver.cpp:253]     Train net output #0: loss = 0.134356 (* 1 = 0.134356 loss)
I0205 06:43:53.355561 12688 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I0205 06:44:00.286845 12688 solver.cpp:237] Iteration 1470, loss = 0.143008
I0205 06:44:00.286906 12688 solver.cpp:253]     Train net output #0: loss = 0.143009 (* 1 = 0.143009 loss)
I0205 06:44:00.286917 12688 sgd_solver.cpp:106] Iteration 1470, lr = 0.001
I0205 06:44:07.207391 12688 solver.cpp:237] Iteration 1480, loss = 0.00446012
I0205 06:44:07.207447 12688 solver.cpp:253]     Train net output #0: loss = 0.00446038 (* 1 = 0.00446038 loss)
I0205 06:44:07.207458 12688 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I0205 06:44:14.061126 12688 solver.cpp:237] Iteration 1490, loss = 0.0127287
I0205 06:44:14.061182 12688 solver.cpp:253]     Train net output #0: loss = 0.0127289 (* 1 = 0.0127289 loss)
I0205 06:44:14.061193 12688 sgd_solver.cpp:106] Iteration 1490, lr = 0.001
I0205 06:44:20.334651 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1500.caffemodel
I0205 06:44:20.336645 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1500.solverstate
I0205 06:44:20.337430 12688 solver.cpp:341] Iteration 1500, Testing net (#0)
I0205 06:44:23.761030 12688 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 06:44:23.761241 12688 solver.cpp:409]     Test net output #1: loss = 0.00911622 (* 1 = 0.00911622 loss)
I0205 06:44:24.461086 12688 solver.cpp:237] Iteration 1500, loss = 0.00457092
I0205 06:44:24.461138 12688 solver.cpp:253]     Train net output #0: loss = 0.00457117 (* 1 = 0.00457117 loss)
I0205 06:44:24.461148 12688 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I0205 06:44:31.430950 12688 solver.cpp:237] Iteration 1510, loss = 0.00419933
I0205 06:44:31.431008 12688 solver.cpp:253]     Train net output #0: loss = 0.00419959 (* 1 = 0.00419959 loss)
I0205 06:44:31.431018 12688 sgd_solver.cpp:106] Iteration 1510, lr = 0.001
I0205 06:44:38.441633 12688 solver.cpp:237] Iteration 1520, loss = 0.019492
I0205 06:44:38.441689 12688 solver.cpp:253]     Train net output #0: loss = 0.0194922 (* 1 = 0.0194922 loss)
I0205 06:44:38.441700 12688 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I0205 06:44:45.372501 12688 solver.cpp:237] Iteration 1530, loss = 0.0254894
I0205 06:44:45.372555 12688 solver.cpp:253]     Train net output #0: loss = 0.0254896 (* 1 = 0.0254896 loss)
I0205 06:44:45.372565 12688 sgd_solver.cpp:106] Iteration 1530, lr = 0.001
I0205 06:44:52.331435 12688 solver.cpp:237] Iteration 1540, loss = 0.0118269
I0205 06:44:52.331490 12688 solver.cpp:253]     Train net output #0: loss = 0.0118271 (* 1 = 0.0118271 loss)
I0205 06:44:52.331501 12688 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I0205 06:44:59.203799 12688 solver.cpp:237] Iteration 1550, loss = 0.0543797
I0205 06:44:59.203994 12688 solver.cpp:253]     Train net output #0: loss = 0.0543799 (* 1 = 0.0543799 loss)
I0205 06:44:59.204007 12688 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I0205 06:45:06.015810 12688 solver.cpp:237] Iteration 1560, loss = 0.00504221
I0205 06:45:06.015867 12688 solver.cpp:253]     Train net output #0: loss = 0.00504246 (* 1 = 0.00504246 loss)
I0205 06:45:06.015878 12688 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I0205 06:45:12.860664 12688 solver.cpp:237] Iteration 1570, loss = 0.0460488
I0205 06:45:12.860719 12688 solver.cpp:253]     Train net output #0: loss = 0.0460491 (* 1 = 0.0460491 loss)
I0205 06:45:12.860731 12688 sgd_solver.cpp:106] Iteration 1570, lr = 0.001
I0205 06:45:19.701248 12688 solver.cpp:237] Iteration 1580, loss = 0.00366167
I0205 06:45:19.701306 12688 solver.cpp:253]     Train net output #0: loss = 0.00366193 (* 1 = 0.00366193 loss)
I0205 06:45:19.701318 12688 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I0205 06:45:26.546926 12688 solver.cpp:237] Iteration 1590, loss = 0.00271017
I0205 06:45:26.546988 12688 solver.cpp:253]     Train net output #0: loss = 0.00271042 (* 1 = 0.00271042 loss)
I0205 06:45:26.546999 12688 sgd_solver.cpp:106] Iteration 1590, lr = 0.001
I0205 06:45:32.727932 12688 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1600.caffemodel
I0205 06:45:32.730100 12688 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed22/snaps/snap__iter_1600.solverstate
I0205 06:45:33.068063 12688 solver.cpp:321] Iteration 1600, loss = 0.00367542
I0205 06:45:33.068110 12688 solver.cpp:341] Iteration 1600, Testing net (#0)
I0205 06:45:36.456188 12688 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 06:45:36.456235 12688 solver.cpp:409]     Test net output #1: loss = 0.00957963 (* 1 = 0.00957963 loss)
I0205 06:45:36.456243 12688 solver.cpp:326] Optimization Done.
I0205 06:45:36.456249 12688 caffe.cpp:215] Optimization Done.
