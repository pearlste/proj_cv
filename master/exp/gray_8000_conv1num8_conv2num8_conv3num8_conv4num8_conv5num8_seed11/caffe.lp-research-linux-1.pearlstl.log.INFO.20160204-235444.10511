Log file created at: 2016/02/04 23:54:44
Running on machine: lp-research-linux-1
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0204 23:54:44.435441 10511 caffe.cpp:177] Use CPU.
I0204 23:54:44.436342 10511 solver.cpp:48] Initializing solver from parameters: 
test_iter: 10
test_interval: 100
base_lr: 0.001
display: 10
max_iter: 1600
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 100
snapshot_prefix: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap_"
solver_mode: CPU
random_seed: 11
net: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/train_val.prototxt"
I0204 23:54:44.436511 10511 solver.cpp:91] Creating training net from net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/train_val.prototxt
I0204 23:54:44.437001 10511 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0204 23:54:44.437033 10511 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0204 23:54:44.437247 10511 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0204 23:54:44.437371 10511 layer_factory.hpp:77] Creating layer data
I0204 23:54:44.437544 10511 net.cpp:106] Creating Layer data
I0204 23:54:44.437561 10511 net.cpp:411] data -> data
I0204 23:54:44.437625 10511 net.cpp:411] data -> label
I0204 23:54:44.437652 10511 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto
I0204 23:54:44.437867 10512 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db
I0204 23:54:44.438755 10511 data_layer.cpp:41] output data size: 100,1,227,227
I0204 23:54:44.470360 10511 net.cpp:150] Setting up data
I0204 23:54:44.470443 10511 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0204 23:54:44.470450 10511 net.cpp:157] Top shape: 100 (100)
I0204 23:54:44.470456 10511 net.cpp:165] Memory required for data: 20612000
I0204 23:54:44.470479 10511 layer_factory.hpp:77] Creating layer conv1
I0204 23:54:44.470506 10511 net.cpp:106] Creating Layer conv1
I0204 23:54:44.470515 10511 net.cpp:454] conv1 <- data
I0204 23:54:44.470538 10511 net.cpp:411] conv1 -> conv1
I0204 23:54:44.470654 10511 net.cpp:150] Setting up conv1
I0204 23:54:44.470664 10511 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0204 23:54:44.470669 10511 net.cpp:165] Memory required for data: 30292000
I0204 23:54:44.470685 10511 layer_factory.hpp:77] Creating layer relu1
I0204 23:54:44.470697 10511 net.cpp:106] Creating Layer relu1
I0204 23:54:44.470703 10511 net.cpp:454] relu1 <- conv1
I0204 23:54:44.470711 10511 net.cpp:397] relu1 -> conv1 (in-place)
I0204 23:54:44.470733 10511 net.cpp:150] Setting up relu1
I0204 23:54:44.470741 10511 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0204 23:54:44.470746 10511 net.cpp:165] Memory required for data: 39972000
I0204 23:54:44.470752 10511 layer_factory.hpp:77] Creating layer pool1
I0204 23:54:44.470762 10511 net.cpp:106] Creating Layer pool1
I0204 23:54:44.470767 10511 net.cpp:454] pool1 <- conv1
I0204 23:54:44.470774 10511 net.cpp:411] pool1 -> pool1
I0204 23:54:44.470801 10511 net.cpp:150] Setting up pool1
I0204 23:54:44.470809 10511 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0204 23:54:44.470813 10511 net.cpp:165] Memory required for data: 42304800
I0204 23:54:44.470819 10511 layer_factory.hpp:77] Creating layer norm1
I0204 23:54:44.470839 10511 net.cpp:106] Creating Layer norm1
I0204 23:54:44.470856 10511 net.cpp:454] norm1 <- pool1
I0204 23:54:44.470865 10511 net.cpp:411] norm1 -> norm1
I0204 23:54:44.470882 10511 net.cpp:150] Setting up norm1
I0204 23:54:44.470890 10511 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0204 23:54:44.470895 10511 net.cpp:165] Memory required for data: 44637600
I0204 23:54:44.470899 10511 layer_factory.hpp:77] Creating layer conv2
I0204 23:54:44.470911 10511 net.cpp:106] Creating Layer conv2
I0204 23:54:44.470916 10511 net.cpp:454] conv2 <- norm1
I0204 23:54:44.470923 10511 net.cpp:411] conv2 -> conv2
I0204 23:54:44.470949 10511 net.cpp:150] Setting up conv2
I0204 23:54:44.470957 10511 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0204 23:54:44.470963 10511 net.cpp:165] Memory required for data: 46970400
I0204 23:54:44.470971 10511 layer_factory.hpp:77] Creating layer relu2
I0204 23:54:44.470979 10511 net.cpp:106] Creating Layer relu2
I0204 23:54:44.470984 10511 net.cpp:454] relu2 <- conv2
I0204 23:54:44.470994 10511 net.cpp:397] relu2 -> conv2 (in-place)
I0204 23:54:44.471002 10511 net.cpp:150] Setting up relu2
I0204 23:54:44.471009 10511 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0204 23:54:44.471012 10511 net.cpp:165] Memory required for data: 49303200
I0204 23:54:44.471017 10511 layer_factory.hpp:77] Creating layer pool2
I0204 23:54:44.471025 10511 net.cpp:106] Creating Layer pool2
I0204 23:54:44.471030 10511 net.cpp:454] pool2 <- conv2
I0204 23:54:44.471037 10511 net.cpp:411] pool2 -> pool2
I0204 23:54:44.471045 10511 net.cpp:150] Setting up pool2
I0204 23:54:44.471051 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.471068 10511 net.cpp:165] Memory required for data: 49844000
I0204 23:54:44.471073 10511 layer_factory.hpp:77] Creating layer norm2
I0204 23:54:44.471082 10511 net.cpp:106] Creating Layer norm2
I0204 23:54:44.471087 10511 net.cpp:454] norm2 <- pool2
I0204 23:54:44.471102 10511 net.cpp:411] norm2 -> norm2
I0204 23:54:44.471110 10511 net.cpp:150] Setting up norm2
I0204 23:54:44.471117 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.471122 10511 net.cpp:165] Memory required for data: 50384800
I0204 23:54:44.471129 10511 layer_factory.hpp:77] Creating layer conv3
I0204 23:54:44.471139 10511 net.cpp:106] Creating Layer conv3
I0204 23:54:44.471145 10511 net.cpp:454] conv3 <- norm2
I0204 23:54:44.471154 10511 net.cpp:411] conv3 -> conv3
I0204 23:54:44.471180 10511 net.cpp:150] Setting up conv3
I0204 23:54:44.471187 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.471192 10511 net.cpp:165] Memory required for data: 50925600
I0204 23:54:44.471201 10511 layer_factory.hpp:77] Creating layer relu3
I0204 23:54:44.471210 10511 net.cpp:106] Creating Layer relu3
I0204 23:54:44.471215 10511 net.cpp:454] relu3 <- conv3
I0204 23:54:44.471221 10511 net.cpp:397] relu3 -> conv3 (in-place)
I0204 23:54:44.471228 10511 net.cpp:150] Setting up relu3
I0204 23:54:44.471235 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.471238 10511 net.cpp:165] Memory required for data: 51466400
I0204 23:54:44.471243 10511 layer_factory.hpp:77] Creating layer conv4
I0204 23:54:44.471253 10511 net.cpp:106] Creating Layer conv4
I0204 23:54:44.471258 10511 net.cpp:454] conv4 <- conv3
I0204 23:54:44.471268 10511 net.cpp:411] conv4 -> conv4
I0204 23:54:44.471289 10511 net.cpp:150] Setting up conv4
I0204 23:54:44.471297 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.471300 10511 net.cpp:165] Memory required for data: 52007200
I0204 23:54:44.471307 10511 layer_factory.hpp:77] Creating layer relu4
I0204 23:54:44.471315 10511 net.cpp:106] Creating Layer relu4
I0204 23:54:44.471320 10511 net.cpp:454] relu4 <- conv4
I0204 23:54:44.471326 10511 net.cpp:397] relu4 -> conv4 (in-place)
I0204 23:54:44.471333 10511 net.cpp:150] Setting up relu4
I0204 23:54:44.471339 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.471344 10511 net.cpp:165] Memory required for data: 52548000
I0204 23:54:44.471349 10511 layer_factory.hpp:77] Creating layer conv5
I0204 23:54:44.471375 10511 net.cpp:106] Creating Layer conv5
I0204 23:54:44.471387 10511 net.cpp:454] conv5 <- conv4
I0204 23:54:44.471397 10511 net.cpp:411] conv5 -> conv5
I0204 23:54:44.471416 10511 net.cpp:150] Setting up conv5
I0204 23:54:44.471422 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.471427 10511 net.cpp:165] Memory required for data: 53088800
I0204 23:54:44.471437 10511 layer_factory.hpp:77] Creating layer relu5
I0204 23:54:44.471444 10511 net.cpp:106] Creating Layer relu5
I0204 23:54:44.471449 10511 net.cpp:454] relu5 <- conv5
I0204 23:54:44.471456 10511 net.cpp:397] relu5 -> conv5 (in-place)
I0204 23:54:44.471462 10511 net.cpp:150] Setting up relu5
I0204 23:54:44.471467 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.471472 10511 net.cpp:165] Memory required for data: 53629600
I0204 23:54:44.471477 10511 layer_factory.hpp:77] Creating layer pool5
I0204 23:54:44.471484 10511 net.cpp:106] Creating Layer pool5
I0204 23:54:44.471489 10511 net.cpp:454] pool5 <- conv5
I0204 23:54:44.471496 10511 net.cpp:411] pool5 -> pool5
I0204 23:54:44.471504 10511 net.cpp:150] Setting up pool5
I0204 23:54:44.471510 10511 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0204 23:54:44.471515 10511 net.cpp:165] Memory required for data: 53744800
I0204 23:54:44.471519 10511 layer_factory.hpp:77] Creating layer fc6
I0204 23:54:44.471537 10511 net.cpp:106] Creating Layer fc6
I0204 23:54:44.471542 10511 net.cpp:454] fc6 <- pool5
I0204 23:54:44.471550 10511 net.cpp:411] fc6 -> fc6
I0204 23:54:44.472285 10511 net.cpp:150] Setting up fc6
I0204 23:54:44.472297 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.472302 10511 net.cpp:165] Memory required for data: 53847200
I0204 23:54:44.472311 10511 layer_factory.hpp:77] Creating layer relu6
I0204 23:54:44.472319 10511 net.cpp:106] Creating Layer relu6
I0204 23:54:44.472324 10511 net.cpp:454] relu6 <- fc6
I0204 23:54:44.472332 10511 net.cpp:397] relu6 -> fc6 (in-place)
I0204 23:54:44.472342 10511 net.cpp:150] Setting up relu6
I0204 23:54:44.472347 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.472352 10511 net.cpp:165] Memory required for data: 53949600
I0204 23:54:44.472369 10511 layer_factory.hpp:77] Creating layer drop6
I0204 23:54:44.472380 10511 net.cpp:106] Creating Layer drop6
I0204 23:54:44.472385 10511 net.cpp:454] drop6 <- fc6
I0204 23:54:44.472393 10511 net.cpp:397] drop6 -> fc6 (in-place)
I0204 23:54:44.472409 10511 net.cpp:150] Setting up drop6
I0204 23:54:44.472416 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.472420 10511 net.cpp:165] Memory required for data: 54052000
I0204 23:54:44.472425 10511 layer_factory.hpp:77] Creating layer fc7
I0204 23:54:44.472435 10511 net.cpp:106] Creating Layer fc7
I0204 23:54:44.472440 10511 net.cpp:454] fc7 <- fc6
I0204 23:54:44.472447 10511 net.cpp:411] fc7 -> fc7
I0204 23:54:44.473052 10511 net.cpp:150] Setting up fc7
I0204 23:54:44.473075 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.473080 10511 net.cpp:165] Memory required for data: 54154400
I0204 23:54:44.473088 10511 layer_factory.hpp:77] Creating layer relu7
I0204 23:54:44.473108 10511 net.cpp:106] Creating Layer relu7
I0204 23:54:44.473112 10511 net.cpp:454] relu7 <- fc7
I0204 23:54:44.473119 10511 net.cpp:397] relu7 -> fc7 (in-place)
I0204 23:54:44.473127 10511 net.cpp:150] Setting up relu7
I0204 23:54:44.473132 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.473137 10511 net.cpp:165] Memory required for data: 54256800
I0204 23:54:44.473142 10511 layer_factory.hpp:77] Creating layer drop7
I0204 23:54:44.473151 10511 net.cpp:106] Creating Layer drop7
I0204 23:54:44.473156 10511 net.cpp:454] drop7 <- fc7
I0204 23:54:44.473163 10511 net.cpp:397] drop7 -> fc7 (in-place)
I0204 23:54:44.473171 10511 net.cpp:150] Setting up drop7
I0204 23:54:44.473177 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.473181 10511 net.cpp:165] Memory required for data: 54359200
I0204 23:54:44.473186 10511 layer_factory.hpp:77] Creating layer fc8
I0204 23:54:44.473197 10511 net.cpp:106] Creating Layer fc8
I0204 23:54:44.473206 10511 net.cpp:454] fc8 <- fc7
I0204 23:54:44.473222 10511 net.cpp:411] fc8 -> fc8
I0204 23:54:44.473244 10511 net.cpp:150] Setting up fc8
I0204 23:54:44.473251 10511 net.cpp:157] Top shape: 100 2 (200)
I0204 23:54:44.473255 10511 net.cpp:165] Memory required for data: 54360000
I0204 23:54:44.473263 10511 layer_factory.hpp:77] Creating layer loss
I0204 23:54:44.473271 10511 net.cpp:106] Creating Layer loss
I0204 23:54:44.473276 10511 net.cpp:454] loss <- fc8
I0204 23:54:44.473281 10511 net.cpp:454] loss <- label
I0204 23:54:44.473292 10511 net.cpp:411] loss -> loss
I0204 23:54:44.473306 10511 layer_factory.hpp:77] Creating layer loss
I0204 23:54:44.473328 10511 net.cpp:150] Setting up loss
I0204 23:54:44.473335 10511 net.cpp:157] Top shape: (1)
I0204 23:54:44.473340 10511 net.cpp:160]     with loss weight 1
I0204 23:54:44.473381 10511 net.cpp:165] Memory required for data: 54360004
I0204 23:54:44.473388 10511 net.cpp:226] loss needs backward computation.
I0204 23:54:44.473394 10511 net.cpp:226] fc8 needs backward computation.
I0204 23:54:44.473398 10511 net.cpp:226] drop7 needs backward computation.
I0204 23:54:44.473403 10511 net.cpp:226] relu7 needs backward computation.
I0204 23:54:44.473408 10511 net.cpp:226] fc7 needs backward computation.
I0204 23:54:44.473413 10511 net.cpp:226] drop6 needs backward computation.
I0204 23:54:44.473417 10511 net.cpp:226] relu6 needs backward computation.
I0204 23:54:44.473423 10511 net.cpp:226] fc6 needs backward computation.
I0204 23:54:44.473428 10511 net.cpp:226] pool5 needs backward computation.
I0204 23:54:44.473434 10511 net.cpp:226] relu5 needs backward computation.
I0204 23:54:44.473439 10511 net.cpp:226] conv5 needs backward computation.
I0204 23:54:44.473444 10511 net.cpp:226] relu4 needs backward computation.
I0204 23:54:44.473449 10511 net.cpp:226] conv4 needs backward computation.
I0204 23:54:44.473453 10511 net.cpp:226] relu3 needs backward computation.
I0204 23:54:44.473459 10511 net.cpp:226] conv3 needs backward computation.
I0204 23:54:44.473467 10511 net.cpp:226] norm2 needs backward computation.
I0204 23:54:44.473472 10511 net.cpp:226] pool2 needs backward computation.
I0204 23:54:44.473477 10511 net.cpp:226] relu2 needs backward computation.
I0204 23:54:44.473484 10511 net.cpp:226] conv2 needs backward computation.
I0204 23:54:44.473489 10511 net.cpp:226] norm1 needs backward computation.
I0204 23:54:44.473506 10511 net.cpp:226] pool1 needs backward computation.
I0204 23:54:44.473510 10511 net.cpp:226] relu1 needs backward computation.
I0204 23:54:44.473515 10511 net.cpp:226] conv1 needs backward computation.
I0204 23:54:44.473520 10511 net.cpp:228] data does not need backward computation.
I0204 23:54:44.473526 10511 net.cpp:270] This network produces output loss
I0204 23:54:44.473549 10511 net.cpp:283] Network initialization done.
I0204 23:54:44.474258 10511 solver.cpp:181] Creating test net (#0) specified by net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/train_val.prototxt
I0204 23:54:44.474315 10511 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0204 23:54:44.474591 10511 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0204 23:54:44.474745 10511 layer_factory.hpp:77] Creating layer data
I0204 23:54:44.474895 10511 net.cpp:106] Creating Layer data
I0204 23:54:44.474907 10511 net.cpp:411] data -> data
I0204 23:54:44.474920 10511 net.cpp:411] data -> label
I0204 23:54:44.474930 10511 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto
I0204 23:54:44.475217 10515 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db
I0204 23:54:44.475953 10511 data_layer.cpp:41] output data size: 100,1,227,227
I0204 23:54:44.505022 10511 net.cpp:150] Setting up data
I0204 23:54:44.505062 10511 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0204 23:54:44.505069 10511 net.cpp:157] Top shape: 100 (100)
I0204 23:54:44.505086 10511 net.cpp:165] Memory required for data: 20612000
I0204 23:54:44.505120 10511 layer_factory.hpp:77] Creating layer label_data_1_split
I0204 23:54:44.505141 10511 net.cpp:106] Creating Layer label_data_1_split
I0204 23:54:44.505147 10511 net.cpp:454] label_data_1_split <- label
I0204 23:54:44.505161 10511 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0204 23:54:44.505175 10511 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0204 23:54:44.505187 10511 net.cpp:150] Setting up label_data_1_split
I0204 23:54:44.505195 10511 net.cpp:157] Top shape: 100 (100)
I0204 23:54:44.505201 10511 net.cpp:157] Top shape: 100 (100)
I0204 23:54:44.505206 10511 net.cpp:165] Memory required for data: 20612800
I0204 23:54:44.505211 10511 layer_factory.hpp:77] Creating layer conv1
I0204 23:54:44.505226 10511 net.cpp:106] Creating Layer conv1
I0204 23:54:44.505233 10511 net.cpp:454] conv1 <- data
I0204 23:54:44.505241 10511 net.cpp:411] conv1 -> conv1
I0204 23:54:44.505293 10511 net.cpp:150] Setting up conv1
I0204 23:54:44.505302 10511 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0204 23:54:44.505307 10511 net.cpp:165] Memory required for data: 30292800
I0204 23:54:44.505321 10511 layer_factory.hpp:77] Creating layer relu1
I0204 23:54:44.505331 10511 net.cpp:106] Creating Layer relu1
I0204 23:54:44.505337 10511 net.cpp:454] relu1 <- conv1
I0204 23:54:44.505345 10511 net.cpp:397] relu1 -> conv1 (in-place)
I0204 23:54:44.505354 10511 net.cpp:150] Setting up relu1
I0204 23:54:44.505360 10511 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0204 23:54:44.505365 10511 net.cpp:165] Memory required for data: 39972800
I0204 23:54:44.505370 10511 layer_factory.hpp:77] Creating layer pool1
I0204 23:54:44.505381 10511 net.cpp:106] Creating Layer pool1
I0204 23:54:44.505388 10511 net.cpp:454] pool1 <- conv1
I0204 23:54:44.505395 10511 net.cpp:411] pool1 -> pool1
I0204 23:54:44.505409 10511 net.cpp:150] Setting up pool1
I0204 23:54:44.505429 10511 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0204 23:54:44.505434 10511 net.cpp:165] Memory required for data: 42305600
I0204 23:54:44.505439 10511 layer_factory.hpp:77] Creating layer norm1
I0204 23:54:44.505450 10511 net.cpp:106] Creating Layer norm1
I0204 23:54:44.505455 10511 net.cpp:454] norm1 <- pool1
I0204 23:54:44.505462 10511 net.cpp:411] norm1 -> norm1
I0204 23:54:44.505472 10511 net.cpp:150] Setting up norm1
I0204 23:54:44.505478 10511 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0204 23:54:44.505483 10511 net.cpp:165] Memory required for data: 44638400
I0204 23:54:44.505488 10511 layer_factory.hpp:77] Creating layer conv2
I0204 23:54:44.505498 10511 net.cpp:106] Creating Layer conv2
I0204 23:54:44.505503 10511 net.cpp:454] conv2 <- norm1
I0204 23:54:44.505512 10511 net.cpp:411] conv2 -> conv2
I0204 23:54:44.505539 10511 net.cpp:150] Setting up conv2
I0204 23:54:44.505547 10511 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0204 23:54:44.505551 10511 net.cpp:165] Memory required for data: 46971200
I0204 23:54:44.505565 10511 layer_factory.hpp:77] Creating layer relu2
I0204 23:54:44.505573 10511 net.cpp:106] Creating Layer relu2
I0204 23:54:44.505578 10511 net.cpp:454] relu2 <- conv2
I0204 23:54:44.505585 10511 net.cpp:397] relu2 -> conv2 (in-place)
I0204 23:54:44.505604 10511 net.cpp:150] Setting up relu2
I0204 23:54:44.505622 10511 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0204 23:54:44.505626 10511 net.cpp:165] Memory required for data: 49304000
I0204 23:54:44.505631 10511 layer_factory.hpp:77] Creating layer pool2
I0204 23:54:44.505640 10511 net.cpp:106] Creating Layer pool2
I0204 23:54:44.505645 10511 net.cpp:454] pool2 <- conv2
I0204 23:54:44.505652 10511 net.cpp:411] pool2 -> pool2
I0204 23:54:44.505662 10511 net.cpp:150] Setting up pool2
I0204 23:54:44.505669 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.505673 10511 net.cpp:165] Memory required for data: 49844800
I0204 23:54:44.505678 10511 layer_factory.hpp:77] Creating layer norm2
I0204 23:54:44.505686 10511 net.cpp:106] Creating Layer norm2
I0204 23:54:44.505692 10511 net.cpp:454] norm2 <- pool2
I0204 23:54:44.505698 10511 net.cpp:411] norm2 -> norm2
I0204 23:54:44.505709 10511 net.cpp:150] Setting up norm2
I0204 23:54:44.505717 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.505722 10511 net.cpp:165] Memory required for data: 50385600
I0204 23:54:44.505728 10511 layer_factory.hpp:77] Creating layer conv3
I0204 23:54:44.505738 10511 net.cpp:106] Creating Layer conv3
I0204 23:54:44.505744 10511 net.cpp:454] conv3 <- norm2
I0204 23:54:44.505753 10511 net.cpp:411] conv3 -> conv3
I0204 23:54:44.505775 10511 net.cpp:150] Setting up conv3
I0204 23:54:44.505782 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.505786 10511 net.cpp:165] Memory required for data: 50926400
I0204 23:54:44.505796 10511 layer_factory.hpp:77] Creating layer relu3
I0204 23:54:44.505805 10511 net.cpp:106] Creating Layer relu3
I0204 23:54:44.505810 10511 net.cpp:454] relu3 <- conv3
I0204 23:54:44.505816 10511 net.cpp:397] relu3 -> conv3 (in-place)
I0204 23:54:44.505823 10511 net.cpp:150] Setting up relu3
I0204 23:54:44.505830 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.505833 10511 net.cpp:165] Memory required for data: 51467200
I0204 23:54:44.505838 10511 layer_factory.hpp:77] Creating layer conv4
I0204 23:54:44.505847 10511 net.cpp:106] Creating Layer conv4
I0204 23:54:44.505854 10511 net.cpp:454] conv4 <- conv3
I0204 23:54:44.505863 10511 net.cpp:411] conv4 -> conv4
I0204 23:54:44.505885 10511 net.cpp:150] Setting up conv4
I0204 23:54:44.505892 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.505897 10511 net.cpp:165] Memory required for data: 52008000
I0204 23:54:44.505903 10511 layer_factory.hpp:77] Creating layer relu4
I0204 23:54:44.505911 10511 net.cpp:106] Creating Layer relu4
I0204 23:54:44.505916 10511 net.cpp:454] relu4 <- conv4
I0204 23:54:44.505923 10511 net.cpp:397] relu4 -> conv4 (in-place)
I0204 23:54:44.505929 10511 net.cpp:150] Setting up relu4
I0204 23:54:44.505935 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.505939 10511 net.cpp:165] Memory required for data: 52548800
I0204 23:54:44.505944 10511 layer_factory.hpp:77] Creating layer conv5
I0204 23:54:44.505957 10511 net.cpp:106] Creating Layer conv5
I0204 23:54:44.505962 10511 net.cpp:454] conv5 <- conv4
I0204 23:54:44.505970 10511 net.cpp:411] conv5 -> conv5
I0204 23:54:44.505990 10511 net.cpp:150] Setting up conv5
I0204 23:54:44.505995 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.506000 10511 net.cpp:165] Memory required for data: 53089600
I0204 23:54:44.506011 10511 layer_factory.hpp:77] Creating layer relu5
I0204 23:54:44.506018 10511 net.cpp:106] Creating Layer relu5
I0204 23:54:44.506023 10511 net.cpp:454] relu5 <- conv5
I0204 23:54:44.506031 10511 net.cpp:397] relu5 -> conv5 (in-place)
I0204 23:54:44.506039 10511 net.cpp:150] Setting up relu5
I0204 23:54:44.506045 10511 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0204 23:54:44.506050 10511 net.cpp:165] Memory required for data: 53630400
I0204 23:54:44.506055 10511 layer_factory.hpp:77] Creating layer pool5
I0204 23:54:44.506064 10511 net.cpp:106] Creating Layer pool5
I0204 23:54:44.506069 10511 net.cpp:454] pool5 <- conv5
I0204 23:54:44.506088 10511 net.cpp:411] pool5 -> pool5
I0204 23:54:44.506108 10511 net.cpp:150] Setting up pool5
I0204 23:54:44.506124 10511 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0204 23:54:44.506129 10511 net.cpp:165] Memory required for data: 53745600
I0204 23:54:44.506134 10511 layer_factory.hpp:77] Creating layer fc6
I0204 23:54:44.506145 10511 net.cpp:106] Creating Layer fc6
I0204 23:54:44.506150 10511 net.cpp:454] fc6 <- pool5
I0204 23:54:44.506157 10511 net.cpp:411] fc6 -> fc6
I0204 23:54:44.506822 10511 net.cpp:150] Setting up fc6
I0204 23:54:44.506832 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.506836 10511 net.cpp:165] Memory required for data: 53848000
I0204 23:54:44.506844 10511 layer_factory.hpp:77] Creating layer relu6
I0204 23:54:44.506867 10511 net.cpp:106] Creating Layer relu6
I0204 23:54:44.506873 10511 net.cpp:454] relu6 <- fc6
I0204 23:54:44.506880 10511 net.cpp:397] relu6 -> fc6 (in-place)
I0204 23:54:44.506887 10511 net.cpp:150] Setting up relu6
I0204 23:54:44.506893 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.506897 10511 net.cpp:165] Memory required for data: 53950400
I0204 23:54:44.506902 10511 layer_factory.hpp:77] Creating layer drop6
I0204 23:54:44.506911 10511 net.cpp:106] Creating Layer drop6
I0204 23:54:44.506916 10511 net.cpp:454] drop6 <- fc6
I0204 23:54:44.506925 10511 net.cpp:397] drop6 -> fc6 (in-place)
I0204 23:54:44.506935 10511 net.cpp:150] Setting up drop6
I0204 23:54:44.506940 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.506945 10511 net.cpp:165] Memory required for data: 54052800
I0204 23:54:44.506953 10511 layer_factory.hpp:77] Creating layer fc7
I0204 23:54:44.506961 10511 net.cpp:106] Creating Layer fc7
I0204 23:54:44.506966 10511 net.cpp:454] fc7 <- fc6
I0204 23:54:44.506984 10511 net.cpp:411] fc7 -> fc7
I0204 23:54:44.507683 10511 net.cpp:150] Setting up fc7
I0204 23:54:44.507693 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.507696 10511 net.cpp:165] Memory required for data: 54155200
I0204 23:54:44.507704 10511 layer_factory.hpp:77] Creating layer relu7
I0204 23:54:44.507714 10511 net.cpp:106] Creating Layer relu7
I0204 23:54:44.507719 10511 net.cpp:454] relu7 <- fc7
I0204 23:54:44.507726 10511 net.cpp:397] relu7 -> fc7 (in-place)
I0204 23:54:44.507733 10511 net.cpp:150] Setting up relu7
I0204 23:54:44.507740 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.507745 10511 net.cpp:165] Memory required for data: 54257600
I0204 23:54:44.507750 10511 layer_factory.hpp:77] Creating layer drop7
I0204 23:54:44.507756 10511 net.cpp:106] Creating Layer drop7
I0204 23:54:44.507761 10511 net.cpp:454] drop7 <- fc7
I0204 23:54:44.507767 10511 net.cpp:397] drop7 -> fc7 (in-place)
I0204 23:54:44.507776 10511 net.cpp:150] Setting up drop7
I0204 23:54:44.507781 10511 net.cpp:157] Top shape: 100 256 (25600)
I0204 23:54:44.507786 10511 net.cpp:165] Memory required for data: 54360000
I0204 23:54:44.507791 10511 layer_factory.hpp:77] Creating layer fc8
I0204 23:54:44.507802 10511 net.cpp:106] Creating Layer fc8
I0204 23:54:44.507808 10511 net.cpp:454] fc8 <- fc7
I0204 23:54:44.507828 10511 net.cpp:411] fc8 -> fc8
I0204 23:54:44.507851 10511 net.cpp:150] Setting up fc8
I0204 23:54:44.507860 10511 net.cpp:157] Top shape: 100 2 (200)
I0204 23:54:44.507865 10511 net.cpp:165] Memory required for data: 54360800
I0204 23:54:44.507872 10511 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0204 23:54:44.507880 10511 net.cpp:106] Creating Layer fc8_fc8_0_split
I0204 23:54:44.507885 10511 net.cpp:454] fc8_fc8_0_split <- fc8
I0204 23:54:44.507891 10511 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0204 23:54:44.507899 10511 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0204 23:54:44.507906 10511 net.cpp:150] Setting up fc8_fc8_0_split
I0204 23:54:44.507912 10511 net.cpp:157] Top shape: 100 2 (200)
I0204 23:54:44.507917 10511 net.cpp:157] Top shape: 100 2 (200)
I0204 23:54:44.507922 10511 net.cpp:165] Memory required for data: 54362400
I0204 23:54:44.507926 10511 layer_factory.hpp:77] Creating layer accuracy
I0204 23:54:44.507941 10511 net.cpp:106] Creating Layer accuracy
I0204 23:54:44.507951 10511 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0204 23:54:44.507964 10511 net.cpp:454] accuracy <- label_data_1_split_0
I0204 23:54:44.507975 10511 net.cpp:411] accuracy -> accuracy
I0204 23:54:44.507987 10511 net.cpp:150] Setting up accuracy
I0204 23:54:44.507992 10511 net.cpp:157] Top shape: (1)
I0204 23:54:44.507997 10511 net.cpp:165] Memory required for data: 54362404
I0204 23:54:44.508002 10511 layer_factory.hpp:77] Creating layer loss
I0204 23:54:44.508009 10511 net.cpp:106] Creating Layer loss
I0204 23:54:44.508014 10511 net.cpp:454] loss <- fc8_fc8_0_split_1
I0204 23:54:44.508020 10511 net.cpp:454] loss <- label_data_1_split_1
I0204 23:54:44.508026 10511 net.cpp:411] loss -> loss
I0204 23:54:44.508035 10511 layer_factory.hpp:77] Creating layer loss
I0204 23:54:44.508054 10511 net.cpp:150] Setting up loss
I0204 23:54:44.508060 10511 net.cpp:157] Top shape: (1)
I0204 23:54:44.508064 10511 net.cpp:160]     with loss weight 1
I0204 23:54:44.508100 10511 net.cpp:165] Memory required for data: 54362408
I0204 23:54:44.508107 10511 net.cpp:226] loss needs backward computation.
I0204 23:54:44.508115 10511 net.cpp:228] accuracy does not need backward computation.
I0204 23:54:44.508121 10511 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0204 23:54:44.508126 10511 net.cpp:226] fc8 needs backward computation.
I0204 23:54:44.508131 10511 net.cpp:226] drop7 needs backward computation.
I0204 23:54:44.508139 10511 net.cpp:226] relu7 needs backward computation.
I0204 23:54:44.508144 10511 net.cpp:226] fc7 needs backward computation.
I0204 23:54:44.508149 10511 net.cpp:226] drop6 needs backward computation.
I0204 23:54:44.508154 10511 net.cpp:226] relu6 needs backward computation.
I0204 23:54:44.508159 10511 net.cpp:226] fc6 needs backward computation.
I0204 23:54:44.508164 10511 net.cpp:226] pool5 needs backward computation.
I0204 23:54:44.508169 10511 net.cpp:226] relu5 needs backward computation.
I0204 23:54:44.508173 10511 net.cpp:226] conv5 needs backward computation.
I0204 23:54:44.508178 10511 net.cpp:226] relu4 needs backward computation.
I0204 23:54:44.508183 10511 net.cpp:226] conv4 needs backward computation.
I0204 23:54:44.508188 10511 net.cpp:226] relu3 needs backward computation.
I0204 23:54:44.508193 10511 net.cpp:226] conv3 needs backward computation.
I0204 23:54:44.508198 10511 net.cpp:226] norm2 needs backward computation.
I0204 23:54:44.508204 10511 net.cpp:226] pool2 needs backward computation.
I0204 23:54:44.508209 10511 net.cpp:226] relu2 needs backward computation.
I0204 23:54:44.508213 10511 net.cpp:226] conv2 needs backward computation.
I0204 23:54:44.508219 10511 net.cpp:226] norm1 needs backward computation.
I0204 23:54:44.508224 10511 net.cpp:226] pool1 needs backward computation.
I0204 23:54:44.508229 10511 net.cpp:226] relu1 needs backward computation.
I0204 23:54:44.508234 10511 net.cpp:226] conv1 needs backward computation.
I0204 23:54:44.508240 10511 net.cpp:228] label_data_1_split does not need backward computation.
I0204 23:54:44.508245 10511 net.cpp:228] data does not need backward computation.
I0204 23:54:44.508251 10511 net.cpp:270] This network produces output accuracy
I0204 23:54:44.508258 10511 net.cpp:270] This network produces output loss
I0204 23:54:44.508286 10511 net.cpp:283] Network initialization done.
I0204 23:54:44.508400 10511 solver.cpp:60] Solver scaffolding done.
I0204 23:54:44.508455 10511 caffe.cpp:212] Starting Optimization
I0204 23:54:44.508462 10511 solver.cpp:288] Solving CaffeNet
I0204 23:54:44.508467 10511 solver.cpp:289] Learning Rate Policy: step
I0204 23:54:44.508904 10511 solver.cpp:341] Iteration 0, Testing net (#0)
I0204 23:54:44.508980 10511 blocking_queue.cpp:50] Data layer prefetch queue empty
I0204 23:54:46.961338 10511 solver.cpp:409]     Test net output #0: accuracy = 0.512
I0204 23:54:46.961406 10511 solver.cpp:409]     Test net output #1: loss = 0.896903 (* 1 = 0.896903 loss)
I0204 23:54:47.482095 10511 solver.cpp:237] Iteration 0, loss = 5.3431
I0204 23:54:47.482153 10511 solver.cpp:253]     Train net output #0: loss = 5.3431 (* 1 = 5.3431 loss)
I0204 23:54:47.482178 10511 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0204 23:54:52.331476 10511 solver.cpp:237] Iteration 10, loss = 1.16185
I0204 23:54:52.331542 10511 solver.cpp:253]     Train net output #0: loss = 1.16185 (* 1 = 1.16185 loss)
I0204 23:54:52.331552 10511 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0204 23:54:57.167793 10511 solver.cpp:237] Iteration 20, loss = 0.920297
I0204 23:54:57.167855 10511 solver.cpp:253]     Train net output #0: loss = 0.920297 (* 1 = 0.920297 loss)
I0204 23:54:57.167865 10511 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0204 23:55:02.007601 10511 solver.cpp:237] Iteration 30, loss = 0.931866
I0204 23:55:02.007665 10511 solver.cpp:253]     Train net output #0: loss = 0.931866 (* 1 = 0.931866 loss)
I0204 23:55:02.007678 10511 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0204 23:55:06.847753 10511 solver.cpp:237] Iteration 40, loss = 0.749102
I0204 23:55:06.847820 10511 solver.cpp:253]     Train net output #0: loss = 0.749102 (* 1 = 0.749102 loss)
I0204 23:55:06.847831 10511 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0204 23:55:11.682997 10511 solver.cpp:237] Iteration 50, loss = 0.961278
I0204 23:55:11.683061 10511 solver.cpp:253]     Train net output #0: loss = 0.961278 (* 1 = 0.961278 loss)
I0204 23:55:11.683071 10511 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0204 23:55:16.519165 10511 solver.cpp:237] Iteration 60, loss = 0.84409
I0204 23:55:16.519470 10511 solver.cpp:253]     Train net output #0: loss = 0.84409 (* 1 = 0.84409 loss)
I0204 23:55:16.519484 10511 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0204 23:55:21.355581 10511 solver.cpp:237] Iteration 70, loss = 0.82129
I0204 23:55:21.355643 10511 solver.cpp:253]     Train net output #0: loss = 0.82129 (* 1 = 0.82129 loss)
I0204 23:55:21.355656 10511 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0204 23:55:26.190490 10511 solver.cpp:237] Iteration 80, loss = 0.790737
I0204 23:55:26.190562 10511 solver.cpp:253]     Train net output #0: loss = 0.790737 (* 1 = 0.790737 loss)
I0204 23:55:26.190573 10511 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0204 23:55:31.026377 10511 solver.cpp:237] Iteration 90, loss = 0.773509
I0204 23:55:31.026444 10511 solver.cpp:253]     Train net output #0: loss = 0.773509 (* 1 = 0.773509 loss)
I0204 23:55:31.026455 10511 sgd_solver.cpp:106] Iteration 90, lr = 0.001
I0204 23:55:35.380306 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_100.caffemodel
I0204 23:55:35.383538 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_100.solverstate
I0204 23:55:35.384443 10511 solver.cpp:341] Iteration 100, Testing net (#0)
I0204 23:55:37.780992 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0204 23:55:37.781049 10511 solver.cpp:409]     Test net output #1: loss = 0.696851 (* 1 = 0.696851 loss)
I0204 23:55:38.270256 10511 solver.cpp:237] Iteration 100, loss = 0.624525
I0204 23:55:38.270309 10511 solver.cpp:253]     Train net output #0: loss = 0.624525 (* 1 = 0.624525 loss)
I0204 23:55:38.270323 10511 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0204 23:55:43.163748 10511 solver.cpp:237] Iteration 110, loss = 0.765935
I0204 23:55:43.163810 10511 solver.cpp:253]     Train net output #0: loss = 0.765935 (* 1 = 0.765935 loss)
I0204 23:55:43.163822 10511 sgd_solver.cpp:106] Iteration 110, lr = 0.001
I0204 23:55:48.062207 10511 solver.cpp:237] Iteration 120, loss = 0.701589
I0204 23:55:48.062402 10511 solver.cpp:253]     Train net output #0: loss = 0.701589 (* 1 = 0.701589 loss)
I0204 23:55:48.062417 10511 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0204 23:55:52.963814 10511 solver.cpp:237] Iteration 130, loss = 0.681251
I0204 23:55:52.963876 10511 solver.cpp:253]     Train net output #0: loss = 0.681251 (* 1 = 0.681251 loss)
I0204 23:55:52.963888 10511 sgd_solver.cpp:106] Iteration 130, lr = 0.001
I0204 23:55:57.863412 10511 solver.cpp:237] Iteration 140, loss = 0.823857
I0204 23:55:57.863471 10511 solver.cpp:253]     Train net output #0: loss = 0.823857 (* 1 = 0.823857 loss)
I0204 23:55:57.863483 10511 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0204 23:56:02.762025 10511 solver.cpp:237] Iteration 150, loss = 0.729293
I0204 23:56:02.762081 10511 solver.cpp:253]     Train net output #0: loss = 0.729293 (* 1 = 0.729293 loss)
I0204 23:56:02.762094 10511 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0204 23:56:07.658274 10511 solver.cpp:237] Iteration 160, loss = 0.719368
I0204 23:56:07.658334 10511 solver.cpp:253]     Train net output #0: loss = 0.719368 (* 1 = 0.719368 loss)
I0204 23:56:07.658346 10511 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0204 23:56:12.553993 10511 solver.cpp:237] Iteration 170, loss = 0.749287
I0204 23:56:12.554049 10511 solver.cpp:253]     Train net output #0: loss = 0.749287 (* 1 = 0.749287 loss)
I0204 23:56:12.554061 10511 sgd_solver.cpp:106] Iteration 170, lr = 0.001
I0204 23:56:17.452862 10511 solver.cpp:237] Iteration 180, loss = 0.749462
I0204 23:56:17.452920 10511 solver.cpp:253]     Train net output #0: loss = 0.749462 (* 1 = 0.749462 loss)
I0204 23:56:17.452939 10511 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0204 23:56:22.351061 10511 solver.cpp:237] Iteration 190, loss = 0.687152
I0204 23:56:22.351270 10511 solver.cpp:253]     Train net output #0: loss = 0.687152 (* 1 = 0.687152 loss)
I0204 23:56:22.351284 10511 sgd_solver.cpp:106] Iteration 190, lr = 0.001
I0204 23:56:26.764088 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_200.caffemodel
I0204 23:56:26.766160 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_200.solverstate
I0204 23:56:26.766971 10511 solver.cpp:341] Iteration 200, Testing net (#0)
I0204 23:56:29.125908 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0204 23:56:29.125968 10511 solver.cpp:409]     Test net output #1: loss = 0.694224 (* 1 = 0.694224 loss)
I0204 23:56:29.615782 10511 solver.cpp:237] Iteration 200, loss = 0.735339
I0204 23:56:29.615839 10511 solver.cpp:253]     Train net output #0: loss = 0.735339 (* 1 = 0.735339 loss)
I0204 23:56:29.615851 10511 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0204 23:56:34.525919 10511 solver.cpp:237] Iteration 210, loss = 0.750029
I0204 23:56:34.525977 10511 solver.cpp:253]     Train net output #0: loss = 0.750029 (* 1 = 0.750029 loss)
I0204 23:56:34.525990 10511 sgd_solver.cpp:106] Iteration 210, lr = 0.001
I0204 23:56:39.423470 10511 solver.cpp:237] Iteration 220, loss = 0.80237
I0204 23:56:39.423528 10511 solver.cpp:253]     Train net output #0: loss = 0.80237 (* 1 = 0.80237 loss)
I0204 23:56:39.423542 10511 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0204 23:56:44.321799 10511 solver.cpp:237] Iteration 230, loss = 0.715716
I0204 23:56:44.321861 10511 solver.cpp:253]     Train net output #0: loss = 0.715716 (* 1 = 0.715716 loss)
I0204 23:56:44.321872 10511 sgd_solver.cpp:106] Iteration 230, lr = 0.001
I0204 23:56:49.217778 10511 solver.cpp:237] Iteration 240, loss = 0.757607
I0204 23:56:49.217836 10511 solver.cpp:253]     Train net output #0: loss = 0.757607 (* 1 = 0.757607 loss)
I0204 23:56:49.217849 10511 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0204 23:56:54.114121 10511 solver.cpp:237] Iteration 250, loss = 0.740456
I0204 23:56:54.114302 10511 solver.cpp:253]     Train net output #0: loss = 0.740456 (* 1 = 0.740456 loss)
I0204 23:56:54.114316 10511 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0204 23:56:59.012127 10511 solver.cpp:237] Iteration 260, loss = 0.723483
I0204 23:56:59.012184 10511 solver.cpp:253]     Train net output #0: loss = 0.723483 (* 1 = 0.723483 loss)
I0204 23:56:59.012197 10511 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0204 23:57:03.909986 10511 solver.cpp:237] Iteration 270, loss = 0.75107
I0204 23:57:03.910055 10511 solver.cpp:253]     Train net output #0: loss = 0.75107 (* 1 = 0.75107 loss)
I0204 23:57:03.910068 10511 sgd_solver.cpp:106] Iteration 270, lr = 0.001
I0204 23:57:08.806203 10511 solver.cpp:237] Iteration 280, loss = 0.720165
I0204 23:57:08.806262 10511 solver.cpp:253]     Train net output #0: loss = 0.720165 (* 1 = 0.720165 loss)
I0204 23:57:08.806273 10511 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0204 23:57:13.707792 10511 solver.cpp:237] Iteration 290, loss = 0.743243
I0204 23:57:13.707850 10511 solver.cpp:253]     Train net output #0: loss = 0.743243 (* 1 = 0.743243 loss)
I0204 23:57:13.707862 10511 sgd_solver.cpp:106] Iteration 290, lr = 0.001
I0204 23:57:18.115311 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_300.caffemodel
I0204 23:57:18.117363 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_300.solverstate
I0204 23:57:18.118166 10511 solver.cpp:341] Iteration 300, Testing net (#0)
I0204 23:57:20.478009 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0204 23:57:20.478061 10511 solver.cpp:409]     Test net output #1: loss = 0.702897 (* 1 = 0.702897 loss)
I0204 23:57:20.967525 10511 solver.cpp:237] Iteration 300, loss = 0.796769
I0204 23:57:20.967581 10511 solver.cpp:253]     Train net output #0: loss = 0.796769 (* 1 = 0.796769 loss)
I0204 23:57:20.967593 10511 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0204 23:57:25.867576 10511 solver.cpp:237] Iteration 310, loss = 0.714002
I0204 23:57:25.867804 10511 solver.cpp:253]     Train net output #0: loss = 0.714002 (* 1 = 0.714002 loss)
I0204 23:57:25.867817 10511 sgd_solver.cpp:106] Iteration 310, lr = 0.001
I0204 23:57:30.764287 10511 solver.cpp:237] Iteration 320, loss = 0.678946
I0204 23:57:30.764344 10511 solver.cpp:253]     Train net output #0: loss = 0.678946 (* 1 = 0.678946 loss)
I0204 23:57:30.764356 10511 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0204 23:57:35.660183 10511 solver.cpp:237] Iteration 330, loss = 0.735988
I0204 23:57:35.660243 10511 solver.cpp:253]     Train net output #0: loss = 0.735988 (* 1 = 0.735988 loss)
I0204 23:57:35.660254 10511 sgd_solver.cpp:106] Iteration 330, lr = 0.001
I0204 23:57:40.552042 10511 solver.cpp:237] Iteration 340, loss = 0.683942
I0204 23:57:40.552105 10511 solver.cpp:253]     Train net output #0: loss = 0.683942 (* 1 = 0.683942 loss)
I0204 23:57:40.552117 10511 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0204 23:57:45.451160 10511 solver.cpp:237] Iteration 350, loss = 0.736286
I0204 23:57:45.451220 10511 solver.cpp:253]     Train net output #0: loss = 0.736286 (* 1 = 0.736286 loss)
I0204 23:57:45.451231 10511 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I0204 23:57:50.347208 10511 solver.cpp:237] Iteration 360, loss = 0.713627
I0204 23:57:50.347265 10511 solver.cpp:253]     Train net output #0: loss = 0.713627 (* 1 = 0.713627 loss)
I0204 23:57:50.347277 10511 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0204 23:57:55.242029 10511 solver.cpp:237] Iteration 370, loss = 0.702995
I0204 23:57:55.412334 10511 solver.cpp:253]     Train net output #0: loss = 0.702995 (* 1 = 0.702995 loss)
I0204 23:57:55.412369 10511 sgd_solver.cpp:106] Iteration 370, lr = 0.001
I0204 23:58:00.313498 10511 solver.cpp:237] Iteration 380, loss = 0.75799
I0204 23:58:00.313889 10511 solver.cpp:253]     Train net output #0: loss = 0.75799 (* 1 = 0.75799 loss)
I0204 23:58:00.313905 10511 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0204 23:58:05.213575 10511 solver.cpp:237] Iteration 390, loss = 0.715596
I0204 23:58:05.213632 10511 solver.cpp:253]     Train net output #0: loss = 0.715596 (* 1 = 0.715596 loss)
I0204 23:58:05.213644 10511 sgd_solver.cpp:106] Iteration 390, lr = 0.001
I0204 23:58:09.631536 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_400.caffemodel
I0204 23:58:09.633625 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_400.solverstate
I0204 23:58:09.634419 10511 solver.cpp:341] Iteration 400, Testing net (#0)
I0204 23:58:11.994274 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0204 23:58:11.994326 10511 solver.cpp:409]     Test net output #1: loss = 0.708763 (* 1 = 0.708763 loss)
I0204 23:58:12.483382 10511 solver.cpp:237] Iteration 400, loss = 0.710841
I0204 23:58:12.483439 10511 solver.cpp:253]     Train net output #0: loss = 0.710841 (* 1 = 0.710841 loss)
I0204 23:58:12.483451 10511 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0204 23:58:17.380151 10511 solver.cpp:237] Iteration 410, loss = 0.737237
I0204 23:58:17.380210 10511 solver.cpp:253]     Train net output #0: loss = 0.737237 (* 1 = 0.737237 loss)
I0204 23:58:17.380221 10511 sgd_solver.cpp:106] Iteration 410, lr = 0.001
I0204 23:58:22.274441 10511 solver.cpp:237] Iteration 420, loss = 0.708295
I0204 23:58:22.274498 10511 solver.cpp:253]     Train net output #0: loss = 0.708295 (* 1 = 0.708295 loss)
I0204 23:58:22.274510 10511 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0204 23:58:27.172250 10511 solver.cpp:237] Iteration 430, loss = 0.737769
I0204 23:58:27.172303 10511 solver.cpp:253]     Train net output #0: loss = 0.737769 (* 1 = 0.737769 loss)
I0204 23:58:27.172315 10511 sgd_solver.cpp:106] Iteration 430, lr = 0.001
I0204 23:58:32.071174 10511 solver.cpp:237] Iteration 440, loss = 0.722693
I0204 23:58:32.071394 10511 solver.cpp:253]     Train net output #0: loss = 0.722693 (* 1 = 0.722693 loss)
I0204 23:58:32.071408 10511 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0204 23:58:36.966212 10511 solver.cpp:237] Iteration 450, loss = 0.725894
I0204 23:58:36.966271 10511 solver.cpp:253]     Train net output #0: loss = 0.725894 (* 1 = 0.725894 loss)
I0204 23:58:36.966284 10511 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I0204 23:58:41.862203 10511 solver.cpp:237] Iteration 460, loss = 0.782875
I0204 23:58:41.862262 10511 solver.cpp:253]     Train net output #0: loss = 0.782875 (* 1 = 0.782875 loss)
I0204 23:58:41.862275 10511 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0204 23:58:46.755610 10511 solver.cpp:237] Iteration 470, loss = 0.682459
I0204 23:58:46.755667 10511 solver.cpp:253]     Train net output #0: loss = 0.682459 (* 1 = 0.682459 loss)
I0204 23:58:46.755678 10511 sgd_solver.cpp:106] Iteration 470, lr = 0.001
I0204 23:58:51.651147 10511 solver.cpp:237] Iteration 480, loss = 0.713742
I0204 23:58:51.651211 10511 solver.cpp:253]     Train net output #0: loss = 0.713742 (* 1 = 0.713742 loss)
I0204 23:58:51.651222 10511 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0204 23:58:56.546039 10511 solver.cpp:237] Iteration 490, loss = 0.771682
I0204 23:58:56.546098 10511 solver.cpp:253]     Train net output #0: loss = 0.771682 (* 1 = 0.771682 loss)
I0204 23:58:56.546110 10511 sgd_solver.cpp:106] Iteration 490, lr = 0.001
I0204 23:59:00.952289 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_500.caffemodel
I0204 23:59:00.954340 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_500.solverstate
I0204 23:59:00.955149 10511 solver.cpp:341] Iteration 500, Testing net (#0)
I0204 23:59:03.313653 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0204 23:59:03.313827 10511 solver.cpp:409]     Test net output #1: loss = 0.70573 (* 1 = 0.70573 loss)
I0204 23:59:03.802878 10511 solver.cpp:237] Iteration 500, loss = 0.662342
I0204 23:59:03.802939 10511 solver.cpp:253]     Train net output #0: loss = 0.662342 (* 1 = 0.662342 loss)
I0204 23:59:03.802953 10511 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0204 23:59:08.700248 10511 solver.cpp:237] Iteration 510, loss = 0.733347
I0204 23:59:08.700310 10511 solver.cpp:253]     Train net output #0: loss = 0.733347 (* 1 = 0.733347 loss)
I0204 23:59:08.700323 10511 sgd_solver.cpp:106] Iteration 510, lr = 0.001
I0204 23:59:13.593296 10511 solver.cpp:237] Iteration 520, loss = 0.731286
I0204 23:59:13.593351 10511 solver.cpp:253]     Train net output #0: loss = 0.731286 (* 1 = 0.731286 loss)
I0204 23:59:13.593364 10511 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I0204 23:59:18.488737 10511 solver.cpp:237] Iteration 530, loss = 0.699888
I0204 23:59:18.488795 10511 solver.cpp:253]     Train net output #0: loss = 0.699888 (* 1 = 0.699888 loss)
I0204 23:59:18.488807 10511 sgd_solver.cpp:106] Iteration 530, lr = 0.001
I0204 23:59:23.383929 10511 solver.cpp:237] Iteration 540, loss = 0.773737
I0204 23:59:23.383988 10511 solver.cpp:253]     Train net output #0: loss = 0.773737 (* 1 = 0.773737 loss)
I0204 23:59:23.384001 10511 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I0204 23:59:28.276504 10511 solver.cpp:237] Iteration 550, loss = 0.706521
I0204 23:59:28.276564 10511 solver.cpp:253]     Train net output #0: loss = 0.706521 (* 1 = 0.706521 loss)
I0204 23:59:28.276576 10511 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I0204 23:59:33.172602 10511 solver.cpp:237] Iteration 560, loss = 0.701581
I0204 23:59:33.172659 10511 solver.cpp:253]     Train net output #0: loss = 0.701581 (* 1 = 0.701581 loss)
I0204 23:59:33.172672 10511 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I0204 23:59:38.066350 10511 solver.cpp:237] Iteration 570, loss = 0.717377
I0204 23:59:38.066557 10511 solver.cpp:253]     Train net output #0: loss = 0.717377 (* 1 = 0.717377 loss)
I0204 23:59:38.066572 10511 sgd_solver.cpp:106] Iteration 570, lr = 0.001
I0204 23:59:42.960614 10511 solver.cpp:237] Iteration 580, loss = 0.69749
I0204 23:59:42.960678 10511 solver.cpp:253]     Train net output #0: loss = 0.69749 (* 1 = 0.69749 loss)
I0204 23:59:42.960690 10511 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I0204 23:59:47.852761 10511 solver.cpp:237] Iteration 590, loss = 0.747008
I0204 23:59:47.852819 10511 solver.cpp:253]     Train net output #0: loss = 0.747008 (* 1 = 0.747008 loss)
I0204 23:59:47.852833 10511 sgd_solver.cpp:106] Iteration 590, lr = 0.001
I0204 23:59:52.257491 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_600.caffemodel
I0204 23:59:52.259539 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_600.solverstate
I0204 23:59:52.260341 10511 solver.cpp:341] Iteration 600, Testing net (#0)
I0204 23:59:54.618337 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0204 23:59:54.618389 10511 solver.cpp:409]     Test net output #1: loss = 0.696385 (* 1 = 0.696385 loss)
I0204 23:59:55.107179 10511 solver.cpp:237] Iteration 600, loss = 0.716899
I0204 23:59:55.107234 10511 solver.cpp:253]     Train net output #0: loss = 0.716899 (* 1 = 0.716899 loss)
I0204 23:59:55.107246 10511 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0205 00:00:00.003211 10511 solver.cpp:237] Iteration 610, loss = 0.685557
I0205 00:00:00.003274 10511 solver.cpp:253]     Train net output #0: loss = 0.685557 (* 1 = 0.685557 loss)
I0205 00:00:00.003288 10511 sgd_solver.cpp:106] Iteration 610, lr = 0.001
I0205 00:00:04.901196 10511 solver.cpp:237] Iteration 620, loss = 0.727369
I0205 00:00:04.901252 10511 solver.cpp:253]     Train net output #0: loss = 0.727369 (* 1 = 0.727369 loss)
I0205 00:00:04.901264 10511 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I0205 00:00:09.793819 10511 solver.cpp:237] Iteration 630, loss = 0.736809
I0205 00:00:09.794008 10511 solver.cpp:253]     Train net output #0: loss = 0.736809 (* 1 = 0.736809 loss)
I0205 00:00:09.794023 10511 sgd_solver.cpp:106] Iteration 630, lr = 0.001
I0205 00:00:14.690753 10511 solver.cpp:237] Iteration 640, loss = 0.69003
I0205 00:00:14.690822 10511 solver.cpp:253]     Train net output #0: loss = 0.69003 (* 1 = 0.69003 loss)
I0205 00:00:14.690834 10511 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I0205 00:00:19.584159 10511 solver.cpp:237] Iteration 650, loss = 0.748102
I0205 00:00:19.584219 10511 solver.cpp:253]     Train net output #0: loss = 0.748102 (* 1 = 0.748102 loss)
I0205 00:00:19.584231 10511 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I0205 00:00:24.500185 10511 solver.cpp:237] Iteration 660, loss = 0.671096
I0205 00:00:24.500242 10511 solver.cpp:253]     Train net output #0: loss = 0.671096 (* 1 = 0.671096 loss)
I0205 00:00:24.500254 10511 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I0205 00:00:29.393934 10511 solver.cpp:237] Iteration 670, loss = 0.720818
I0205 00:00:29.393991 10511 solver.cpp:253]     Train net output #0: loss = 0.720818 (* 1 = 0.720818 loss)
I0205 00:00:29.394003 10511 sgd_solver.cpp:106] Iteration 670, lr = 0.001
I0205 00:00:34.290256 10511 solver.cpp:237] Iteration 680, loss = 0.705135
I0205 00:00:34.290310 10511 solver.cpp:253]     Train net output #0: loss = 0.705135 (* 1 = 0.705135 loss)
I0205 00:00:34.290323 10511 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I0205 00:00:39.182749 10511 solver.cpp:237] Iteration 690, loss = 0.690096
I0205 00:00:39.182807 10511 solver.cpp:253]     Train net output #0: loss = 0.690096 (* 1 = 0.690096 loss)
I0205 00:00:39.182819 10511 sgd_solver.cpp:106] Iteration 690, lr = 0.001
I0205 00:00:43.591903 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_700.caffemodel
I0205 00:00:43.594810 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_700.solverstate
I0205 00:00:43.595638 10511 solver.cpp:341] Iteration 700, Testing net (#0)
I0205 00:00:45.954484 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:00:45.954535 10511 solver.cpp:409]     Test net output #1: loss = 0.70173 (* 1 = 0.70173 loss)
I0205 00:00:46.444140 10511 solver.cpp:237] Iteration 700, loss = 0.77731
I0205 00:00:46.444192 10511 solver.cpp:253]     Train net output #0: loss = 0.77731 (* 1 = 0.77731 loss)
I0205 00:00:46.444205 10511 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0205 00:00:51.341212 10511 solver.cpp:237] Iteration 710, loss = 0.742344
I0205 00:00:51.341272 10511 solver.cpp:253]     Train net output #0: loss = 0.742344 (* 1 = 0.742344 loss)
I0205 00:00:51.341284 10511 sgd_solver.cpp:106] Iteration 710, lr = 0.001
I0205 00:00:56.235196 10511 solver.cpp:237] Iteration 720, loss = 0.689518
I0205 00:00:56.235251 10511 solver.cpp:253]     Train net output #0: loss = 0.689518 (* 1 = 0.689518 loss)
I0205 00:00:56.235265 10511 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I0205 00:01:01.130749 10511 solver.cpp:237] Iteration 730, loss = 0.730153
I0205 00:01:01.130810 10511 solver.cpp:253]     Train net output #0: loss = 0.730153 (* 1 = 0.730153 loss)
I0205 00:01:01.130822 10511 sgd_solver.cpp:106] Iteration 730, lr = 0.001
I0205 00:01:06.036074 10511 solver.cpp:237] Iteration 740, loss = 0.69576
I0205 00:01:06.036137 10511 solver.cpp:253]     Train net output #0: loss = 0.69576 (* 1 = 0.69576 loss)
I0205 00:01:06.036149 10511 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I0205 00:01:10.931938 10511 solver.cpp:237] Iteration 750, loss = 0.715465
I0205 00:01:10.931994 10511 solver.cpp:253]     Train net output #0: loss = 0.715465 (* 1 = 0.715465 loss)
I0205 00:01:10.932006 10511 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I0205 00:01:15.825706 10511 solver.cpp:237] Iteration 760, loss = 0.706242
I0205 00:01:15.825903 10511 solver.cpp:253]     Train net output #0: loss = 0.706242 (* 1 = 0.706242 loss)
I0205 00:01:15.825917 10511 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I0205 00:01:20.717260 10511 solver.cpp:237] Iteration 770, loss = 0.683217
I0205 00:01:20.717324 10511 solver.cpp:253]     Train net output #0: loss = 0.683217 (* 1 = 0.683217 loss)
I0205 00:01:20.717349 10511 sgd_solver.cpp:106] Iteration 770, lr = 0.001
I0205 00:01:25.612025 10511 solver.cpp:237] Iteration 780, loss = 0.756882
I0205 00:01:25.612082 10511 solver.cpp:253]     Train net output #0: loss = 0.756882 (* 1 = 0.756882 loss)
I0205 00:01:25.612092 10511 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I0205 00:01:30.507197 10511 solver.cpp:237] Iteration 790, loss = 0.734944
I0205 00:01:30.507256 10511 solver.cpp:253]     Train net output #0: loss = 0.734944 (* 1 = 0.734944 loss)
I0205 00:01:30.507268 10511 sgd_solver.cpp:106] Iteration 790, lr = 0.001
I0205 00:01:34.909562 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_800.caffemodel
I0205 00:01:34.911623 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_800.solverstate
I0205 00:01:34.912430 10511 solver.cpp:341] Iteration 800, Testing net (#0)
I0205 00:01:37.271420 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:01:37.271474 10511 solver.cpp:409]     Test net output #1: loss = 0.69609 (* 1 = 0.69609 loss)
I0205 00:01:37.760828 10511 solver.cpp:237] Iteration 800, loss = 0.684621
I0205 00:01:37.760882 10511 solver.cpp:253]     Train net output #0: loss = 0.684621 (* 1 = 0.684621 loss)
I0205 00:01:37.760895 10511 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0205 00:01:42.654253 10511 solver.cpp:237] Iteration 810, loss = 0.735806
I0205 00:01:42.654309 10511 solver.cpp:253]     Train net output #0: loss = 0.735806 (* 1 = 0.735806 loss)
I0205 00:01:42.654321 10511 sgd_solver.cpp:106] Iteration 810, lr = 0.001
I0205 00:01:47.550566 10511 solver.cpp:237] Iteration 820, loss = 0.684666
I0205 00:01:47.550803 10511 solver.cpp:253]     Train net output #0: loss = 0.684666 (* 1 = 0.684666 loss)
I0205 00:01:47.550817 10511 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I0205 00:01:52.444485 10511 solver.cpp:237] Iteration 830, loss = 0.736766
I0205 00:01:52.444543 10511 solver.cpp:253]     Train net output #0: loss = 0.736766 (* 1 = 0.736766 loss)
I0205 00:01:52.444556 10511 sgd_solver.cpp:106] Iteration 830, lr = 0.001
I0205 00:01:57.341286 10511 solver.cpp:237] Iteration 840, loss = 0.702473
I0205 00:01:57.341348 10511 solver.cpp:253]     Train net output #0: loss = 0.702473 (* 1 = 0.702473 loss)
I0205 00:01:57.341361 10511 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I0205 00:02:02.249301 10511 solver.cpp:237] Iteration 850, loss = 0.71386
I0205 00:02:02.249361 10511 solver.cpp:253]     Train net output #0: loss = 0.71386 (* 1 = 0.71386 loss)
I0205 00:02:02.249372 10511 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I0205 00:02:07.141254 10511 solver.cpp:237] Iteration 860, loss = 0.761141
I0205 00:02:07.141315 10511 solver.cpp:253]     Train net output #0: loss = 0.761141 (* 1 = 0.761141 loss)
I0205 00:02:07.141329 10511 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I0205 00:02:12.034193 10511 solver.cpp:237] Iteration 870, loss = 0.702532
I0205 00:02:12.034250 10511 solver.cpp:253]     Train net output #0: loss = 0.702532 (* 1 = 0.702532 loss)
I0205 00:02:12.034261 10511 sgd_solver.cpp:106] Iteration 870, lr = 0.001
I0205 00:02:16.930800 10511 solver.cpp:237] Iteration 880, loss = 0.719873
I0205 00:02:16.930860 10511 solver.cpp:253]     Train net output #0: loss = 0.719873 (* 1 = 0.719873 loss)
I0205 00:02:16.930873 10511 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I0205 00:02:21.825350 10511 solver.cpp:237] Iteration 890, loss = 0.728276
I0205 00:02:21.825527 10511 solver.cpp:253]     Train net output #0: loss = 0.728276 (* 1 = 0.728276 loss)
I0205 00:02:21.825542 10511 sgd_solver.cpp:106] Iteration 890, lr = 0.001
I0205 00:02:26.236068 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_900.caffemodel
I0205 00:02:26.238131 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_900.solverstate
I0205 00:02:26.238929 10511 solver.cpp:341] Iteration 900, Testing net (#0)
I0205 00:02:28.597785 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:02:28.597836 10511 solver.cpp:409]     Test net output #1: loss = 0.703692 (* 1 = 0.703692 loss)
I0205 00:02:29.087177 10511 solver.cpp:237] Iteration 900, loss = 0.714547
I0205 00:02:29.087231 10511 solver.cpp:253]     Train net output #0: loss = 0.714547 (* 1 = 0.714547 loss)
I0205 00:02:29.087244 10511 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0205 00:02:33.981989 10511 solver.cpp:237] Iteration 910, loss = 0.698397
I0205 00:02:33.982049 10511 solver.cpp:253]     Train net output #0: loss = 0.698397 (* 1 = 0.698397 loss)
I0205 00:02:33.982061 10511 sgd_solver.cpp:106] Iteration 910, lr = 0.001
I0205 00:02:38.876579 10511 solver.cpp:237] Iteration 920, loss = 0.703605
I0205 00:02:38.876634 10511 solver.cpp:253]     Train net output #0: loss = 0.703605 (* 1 = 0.703605 loss)
I0205 00:02:38.876647 10511 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I0205 00:02:43.772151 10511 solver.cpp:237] Iteration 930, loss = 0.708863
I0205 00:02:43.772212 10511 solver.cpp:253]     Train net output #0: loss = 0.708863 (* 1 = 0.708863 loss)
I0205 00:02:43.772223 10511 sgd_solver.cpp:106] Iteration 930, lr = 0.001
I0205 00:02:48.669018 10511 solver.cpp:237] Iteration 940, loss = 0.726539
I0205 00:02:48.669070 10511 solver.cpp:253]     Train net output #0: loss = 0.726539 (* 1 = 0.726539 loss)
I0205 00:02:48.669082 10511 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I0205 00:02:53.563468 10511 solver.cpp:237] Iteration 950, loss = 0.712885
I0205 00:02:53.563690 10511 solver.cpp:253]     Train net output #0: loss = 0.712885 (* 1 = 0.712885 loss)
I0205 00:02:53.563705 10511 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I0205 00:02:58.462548 10511 solver.cpp:237] Iteration 960, loss = 0.708702
I0205 00:02:58.462605 10511 solver.cpp:253]     Train net output #0: loss = 0.708702 (* 1 = 0.708702 loss)
I0205 00:02:58.462618 10511 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I0205 00:03:03.361197 10511 solver.cpp:237] Iteration 970, loss = 0.720535
I0205 00:03:03.361256 10511 solver.cpp:253]     Train net output #0: loss = 0.720535 (* 1 = 0.720535 loss)
I0205 00:03:03.361269 10511 sgd_solver.cpp:106] Iteration 970, lr = 0.001
I0205 00:03:08.253798 10511 solver.cpp:237] Iteration 980, loss = 0.679998
I0205 00:03:08.253859 10511 solver.cpp:253]     Train net output #0: loss = 0.679998 (* 1 = 0.679998 loss)
I0205 00:03:08.253871 10511 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I0205 00:03:13.150156 10511 solver.cpp:237] Iteration 990, loss = 0.739483
I0205 00:03:13.150213 10511 solver.cpp:253]     Train net output #0: loss = 0.739483 (* 1 = 0.739483 loss)
I0205 00:03:13.150225 10511 sgd_solver.cpp:106] Iteration 990, lr = 0.001
I0205 00:03:17.554625 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1000.caffemodel
I0205 00:03:17.556686 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1000.solverstate
I0205 00:03:17.557479 10511 solver.cpp:341] Iteration 1000, Testing net (#0)
I0205 00:03:19.915210 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:03:19.915262 10511 solver.cpp:409]     Test net output #1: loss = 0.695976 (* 1 = 0.695976 loss)
I0205 00:03:20.404996 10511 solver.cpp:237] Iteration 1000, loss = 0.728336
I0205 00:03:20.405051 10511 solver.cpp:253]     Train net output #0: loss = 0.728336 (* 1 = 0.728336 loss)
I0205 00:03:20.405063 10511 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0205 00:03:25.304314 10511 solver.cpp:237] Iteration 1010, loss = 0.689265
I0205 00:03:25.304534 10511 solver.cpp:253]     Train net output #0: loss = 0.689265 (* 1 = 0.689265 loss)
I0205 00:03:25.304549 10511 sgd_solver.cpp:106] Iteration 1010, lr = 0.001
I0205 00:03:30.200090 10511 solver.cpp:237] Iteration 1020, loss = 0.722525
I0205 00:03:30.200150 10511 solver.cpp:253]     Train net output #0: loss = 0.722525 (* 1 = 0.722525 loss)
I0205 00:03:30.200162 10511 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I0205 00:03:35.093436 10511 solver.cpp:237] Iteration 1030, loss = 0.714383
I0205 00:03:35.093492 10511 solver.cpp:253]     Train net output #0: loss = 0.714383 (* 1 = 0.714383 loss)
I0205 00:03:35.093503 10511 sgd_solver.cpp:106] Iteration 1030, lr = 0.001
I0205 00:03:39.986486 10511 solver.cpp:237] Iteration 1040, loss = 0.692255
I0205 00:03:39.986542 10511 solver.cpp:253]     Train net output #0: loss = 0.692255 (* 1 = 0.692255 loss)
I0205 00:03:39.986554 10511 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I0205 00:03:44.881209 10511 solver.cpp:237] Iteration 1050, loss = 0.715742
I0205 00:03:44.881269 10511 solver.cpp:253]     Train net output #0: loss = 0.715742 (* 1 = 0.715742 loss)
I0205 00:03:44.881281 10511 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I0205 00:03:49.777307 10511 solver.cpp:237] Iteration 1060, loss = 0.694486
I0205 00:03:49.777364 10511 solver.cpp:253]     Train net output #0: loss = 0.694486 (* 1 = 0.694486 loss)
I0205 00:03:49.777376 10511 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I0205 00:03:54.683502 10511 solver.cpp:237] Iteration 1070, loss = 0.72017
I0205 00:03:54.683562 10511 solver.cpp:253]     Train net output #0: loss = 0.72017 (* 1 = 0.72017 loss)
I0205 00:03:54.683574 10511 sgd_solver.cpp:106] Iteration 1070, lr = 0.001
I0205 00:03:59.582200 10511 solver.cpp:237] Iteration 1080, loss = 0.680854
I0205 00:03:59.582386 10511 solver.cpp:253]     Train net output #0: loss = 0.680854 (* 1 = 0.680854 loss)
I0205 00:03:59.582401 10511 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I0205 00:04:04.477844 10511 solver.cpp:237] Iteration 1090, loss = 0.688263
I0205 00:04:04.477905 10511 solver.cpp:253]     Train net output #0: loss = 0.688263 (* 1 = 0.688263 loss)
I0205 00:04:04.477918 10511 sgd_solver.cpp:106] Iteration 1090, lr = 0.001
I0205 00:04:08.883177 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1100.caffemodel
I0205 00:04:08.885263 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1100.solverstate
I0205 00:04:08.886061 10511 solver.cpp:341] Iteration 1100, Testing net (#0)
I0205 00:04:11.243989 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:04:11.244040 10511 solver.cpp:409]     Test net output #1: loss = 0.693605 (* 1 = 0.693605 loss)
I0205 00:04:11.734051 10511 solver.cpp:237] Iteration 1100, loss = 0.717507
I0205 00:04:11.734107 10511 solver.cpp:253]     Train net output #0: loss = 0.717507 (* 1 = 0.717507 loss)
I0205 00:04:11.734119 10511 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I0205 00:04:16.630530 10511 solver.cpp:237] Iteration 1110, loss = 0.700467
I0205 00:04:16.630589 10511 solver.cpp:253]     Train net output #0: loss = 0.700467 (* 1 = 0.700467 loss)
I0205 00:04:16.630600 10511 sgd_solver.cpp:106] Iteration 1110, lr = 0.001
I0205 00:04:21.522315 10511 solver.cpp:237] Iteration 1120, loss = 0.706606
I0205 00:04:21.522375 10511 solver.cpp:253]     Train net output #0: loss = 0.706606 (* 1 = 0.706606 loss)
I0205 00:04:21.522387 10511 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I0205 00:04:26.420136 10511 solver.cpp:237] Iteration 1130, loss = 0.741847
I0205 00:04:26.420197 10511 solver.cpp:253]     Train net output #0: loss = 0.741847 (* 1 = 0.741847 loss)
I0205 00:04:26.420210 10511 sgd_solver.cpp:106] Iteration 1130, lr = 0.001
I0205 00:04:31.310917 10511 solver.cpp:237] Iteration 1140, loss = 0.679774
I0205 00:04:31.311142 10511 solver.cpp:253]     Train net output #0: loss = 0.679774 (* 1 = 0.679774 loss)
I0205 00:04:31.311162 10511 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I0205 00:04:36.204696 10511 solver.cpp:237] Iteration 1150, loss = 0.700979
I0205 00:04:36.204751 10511 solver.cpp:253]     Train net output #0: loss = 0.700979 (* 1 = 0.700979 loss)
I0205 00:04:36.204762 10511 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I0205 00:04:41.103312 10511 solver.cpp:237] Iteration 1160, loss = 0.714781
I0205 00:04:41.103369 10511 solver.cpp:253]     Train net output #0: loss = 0.714781 (* 1 = 0.714781 loss)
I0205 00:04:41.103382 10511 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I0205 00:04:46.002861 10511 solver.cpp:237] Iteration 1170, loss = 0.720605
I0205 00:04:46.002921 10511 solver.cpp:253]     Train net output #0: loss = 0.720605 (* 1 = 0.720605 loss)
I0205 00:04:46.002938 10511 sgd_solver.cpp:106] Iteration 1170, lr = 0.001
I0205 00:04:50.897744 10511 solver.cpp:237] Iteration 1180, loss = 0.735964
I0205 00:04:50.897799 10511 solver.cpp:253]     Train net output #0: loss = 0.735964 (* 1 = 0.735964 loss)
I0205 00:04:50.897811 10511 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I0205 00:04:55.791061 10511 solver.cpp:237] Iteration 1190, loss = 0.716597
I0205 00:04:55.791120 10511 solver.cpp:253]     Train net output #0: loss = 0.716597 (* 1 = 0.716597 loss)
I0205 00:04:55.791131 10511 sgd_solver.cpp:106] Iteration 1190, lr = 0.001
I0205 00:05:00.197285 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1200.caffemodel
I0205 00:05:00.199336 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1200.solverstate
I0205 00:05:00.200134 10511 solver.cpp:341] Iteration 1200, Testing net (#0)
I0205 00:05:02.576292 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:05:02.576477 10511 solver.cpp:409]     Test net output #1: loss = 0.695568 (* 1 = 0.695568 loss)
I0205 00:05:03.067245 10511 solver.cpp:237] Iteration 1200, loss = 0.714717
I0205 00:05:03.067299 10511 solver.cpp:253]     Train net output #0: loss = 0.714717 (* 1 = 0.714717 loss)
I0205 00:05:03.067312 10511 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0205 00:05:07.965893 10511 solver.cpp:237] Iteration 1210, loss = 0.746895
I0205 00:05:07.965960 10511 solver.cpp:253]     Train net output #0: loss = 0.746895 (* 1 = 0.746895 loss)
I0205 00:05:07.965971 10511 sgd_solver.cpp:106] Iteration 1210, lr = 0.001
I0205 00:05:12.859081 10511 solver.cpp:237] Iteration 1220, loss = 0.707813
I0205 00:05:12.859136 10511 solver.cpp:253]     Train net output #0: loss = 0.707813 (* 1 = 0.707813 loss)
I0205 00:05:12.859148 10511 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I0205 00:05:17.753530 10511 solver.cpp:237] Iteration 1230, loss = 0.712298
I0205 00:05:17.753589 10511 solver.cpp:253]     Train net output #0: loss = 0.712298 (* 1 = 0.712298 loss)
I0205 00:05:17.753602 10511 sgd_solver.cpp:106] Iteration 1230, lr = 0.001
I0205 00:05:22.648854 10511 solver.cpp:237] Iteration 1240, loss = 0.706261
I0205 00:05:22.648906 10511 solver.cpp:253]     Train net output #0: loss = 0.706261 (* 1 = 0.706261 loss)
I0205 00:05:22.648918 10511 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I0205 00:05:27.543272 10511 solver.cpp:237] Iteration 1250, loss = 0.693142
I0205 00:05:27.543334 10511 solver.cpp:253]     Train net output #0: loss = 0.693142 (* 1 = 0.693142 loss)
I0205 00:05:27.543345 10511 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I0205 00:05:32.489248 10511 solver.cpp:237] Iteration 1260, loss = 0.697311
I0205 00:05:32.489303 10511 solver.cpp:253]     Train net output #0: loss = 0.697311 (* 1 = 0.697311 loss)
I0205 00:05:32.489315 10511 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I0205 00:05:37.428511 10511 solver.cpp:237] Iteration 1270, loss = 0.723187
I0205 00:05:37.428716 10511 solver.cpp:253]     Train net output #0: loss = 0.723187 (* 1 = 0.723187 loss)
I0205 00:05:37.428735 10511 sgd_solver.cpp:106] Iteration 1270, lr = 0.001
I0205 00:05:42.362447 10511 solver.cpp:237] Iteration 1280, loss = 0.687122
I0205 00:05:42.362498 10511 solver.cpp:253]     Train net output #0: loss = 0.687122 (* 1 = 0.687122 loss)
I0205 00:05:42.362510 10511 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I0205 00:05:47.261061 10511 solver.cpp:237] Iteration 1290, loss = 0.733152
I0205 00:05:47.261133 10511 solver.cpp:253]     Train net output #0: loss = 0.733152 (* 1 = 0.733152 loss)
I0205 00:05:47.261147 10511 sgd_solver.cpp:106] Iteration 1290, lr = 0.001
I0205 00:05:51.582373 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1300.caffemodel
I0205 00:05:51.585368 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1300.solverstate
I0205 00:05:51.586164 10511 solver.cpp:341] Iteration 1300, Testing net (#0)
I0205 00:05:53.940037 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:05:53.940111 10511 solver.cpp:409]     Test net output #1: loss = 0.70081 (* 1 = 0.70081 loss)
I0205 00:05:54.420707 10511 solver.cpp:237] Iteration 1300, loss = 0.690399
I0205 00:05:54.420778 10511 solver.cpp:253]     Train net output #0: loss = 0.690399 (* 1 = 0.690399 loss)
I0205 00:05:54.420790 10511 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I0205 00:05:59.220088 10511 solver.cpp:237] Iteration 1310, loss = 0.71149
I0205 00:05:59.220155 10511 solver.cpp:253]     Train net output #0: loss = 0.71149 (* 1 = 0.71149 loss)
I0205 00:05:59.220167 10511 sgd_solver.cpp:106] Iteration 1310, lr = 0.001
I0205 00:06:04.022683 10511 solver.cpp:237] Iteration 1320, loss = 0.716742
I0205 00:06:04.022754 10511 solver.cpp:253]     Train net output #0: loss = 0.716742 (* 1 = 0.716742 loss)
I0205 00:06:04.022768 10511 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I0205 00:06:08.821001 10511 solver.cpp:237] Iteration 1330, loss = 0.70208
I0205 00:06:08.821223 10511 solver.cpp:253]     Train net output #0: loss = 0.70208 (* 1 = 0.70208 loss)
I0205 00:06:08.821238 10511 sgd_solver.cpp:106] Iteration 1330, lr = 0.001
I0205 00:06:13.618844 10511 solver.cpp:237] Iteration 1340, loss = 0.71444
I0205 00:06:13.618924 10511 solver.cpp:253]     Train net output #0: loss = 0.71444 (* 1 = 0.71444 loss)
I0205 00:06:13.618938 10511 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I0205 00:06:18.416483 10511 solver.cpp:237] Iteration 1350, loss = 0.704845
I0205 00:06:18.416556 10511 solver.cpp:253]     Train net output #0: loss = 0.704845 (* 1 = 0.704845 loss)
I0205 00:06:18.416569 10511 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I0205 00:06:23.215767 10511 solver.cpp:237] Iteration 1360, loss = 0.68588
I0205 00:06:23.215843 10511 solver.cpp:253]     Train net output #0: loss = 0.68588 (* 1 = 0.68588 loss)
I0205 00:06:23.215857 10511 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I0205 00:06:28.013677 10511 solver.cpp:237] Iteration 1370, loss = 0.745373
I0205 00:06:28.013746 10511 solver.cpp:253]     Train net output #0: loss = 0.745373 (* 1 = 0.745373 loss)
I0205 00:06:28.013759 10511 sgd_solver.cpp:106] Iteration 1370, lr = 0.001
I0205 00:06:32.811864 10511 solver.cpp:237] Iteration 1380, loss = 0.701561
I0205 00:06:32.811944 10511 solver.cpp:253]     Train net output #0: loss = 0.701561 (* 1 = 0.701561 loss)
I0205 00:06:32.811955 10511 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I0205 00:06:37.610424 10511 solver.cpp:237] Iteration 1390, loss = 0.726387
I0205 00:06:37.610497 10511 solver.cpp:253]     Train net output #0: loss = 0.726387 (* 1 = 0.726387 loss)
I0205 00:06:37.610510 10511 sgd_solver.cpp:106] Iteration 1390, lr = 0.001
I0205 00:06:41.928392 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1400.caffemodel
I0205 00:06:41.930683 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1400.solverstate
I0205 00:06:41.931541 10511 solver.cpp:341] Iteration 1400, Testing net (#0)
I0205 00:06:44.283814 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:06:44.283880 10511 solver.cpp:409]     Test net output #1: loss = 0.694515 (* 1 = 0.694515 loss)
I0205 00:06:44.763268 10511 solver.cpp:237] Iteration 1400, loss = 0.720107
I0205 00:06:44.763337 10511 solver.cpp:253]     Train net output #0: loss = 0.720107 (* 1 = 0.720107 loss)
I0205 00:06:44.763350 10511 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0205 00:06:49.563567 10511 solver.cpp:237] Iteration 1410, loss = 0.688047
I0205 00:06:49.563643 10511 solver.cpp:253]     Train net output #0: loss = 0.688047 (* 1 = 0.688047 loss)
I0205 00:06:49.563655 10511 sgd_solver.cpp:106] Iteration 1410, lr = 0.001
I0205 00:06:54.361335 10511 solver.cpp:237] Iteration 1420, loss = 0.702019
I0205 00:06:54.361407 10511 solver.cpp:253]     Train net output #0: loss = 0.702019 (* 1 = 0.702019 loss)
I0205 00:06:54.361419 10511 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I0205 00:06:59.159330 10511 solver.cpp:237] Iteration 1430, loss = 0.710557
I0205 00:06:59.159406 10511 solver.cpp:253]     Train net output #0: loss = 0.710557 (* 1 = 0.710557 loss)
I0205 00:06:59.159420 10511 sgd_solver.cpp:106] Iteration 1430, lr = 0.001
I0205 00:07:03.957629 10511 solver.cpp:237] Iteration 1440, loss = 0.717942
I0205 00:07:03.957695 10511 solver.cpp:253]     Train net output #0: loss = 0.717942 (* 1 = 0.717942 loss)
I0205 00:07:03.957708 10511 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I0205 00:07:08.754231 10511 solver.cpp:237] Iteration 1450, loss = 0.731328
I0205 00:07:08.754308 10511 solver.cpp:253]     Train net output #0: loss = 0.731328 (* 1 = 0.731328 loss)
I0205 00:07:08.754320 10511 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I0205 00:07:13.551574 10511 solver.cpp:237] Iteration 1460, loss = 0.680832
I0205 00:07:13.553314 10511 solver.cpp:253]     Train net output #0: loss = 0.680832 (* 1 = 0.680832 loss)
I0205 00:07:13.553339 10511 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I0205 00:07:18.246698 10511 solver.cpp:237] Iteration 1470, loss = 0.700002
I0205 00:07:18.246773 10511 solver.cpp:253]     Train net output #0: loss = 0.700002 (* 1 = 0.700002 loss)
I0205 00:07:18.246785 10511 sgd_solver.cpp:106] Iteration 1470, lr = 0.001
I0205 00:07:22.936136 10511 solver.cpp:237] Iteration 1480, loss = 0.729122
I0205 00:07:22.936206 10511 solver.cpp:253]     Train net output #0: loss = 0.729122 (* 1 = 0.729122 loss)
I0205 00:07:22.936219 10511 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I0205 00:07:27.622570 10511 solver.cpp:237] Iteration 1490, loss = 0.697074
I0205 00:07:27.622649 10511 solver.cpp:253]     Train net output #0: loss = 0.697074 (* 1 = 0.697074 loss)
I0205 00:07:27.622663 10511 sgd_solver.cpp:106] Iteration 1490, lr = 0.001
I0205 00:07:31.841917 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1500.caffemodel
I0205 00:07:31.843979 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1500.solverstate
I0205 00:07:31.844775 10511 solver.cpp:341] Iteration 1500, Testing net (#0)
I0205 00:07:34.120918 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:07:34.120988 10511 solver.cpp:409]     Test net output #1: loss = 0.69465 (* 1 = 0.69465 loss)
I0205 00:07:34.589313 10511 solver.cpp:237] Iteration 1500, loss = 0.727481
I0205 00:07:34.589383 10511 solver.cpp:253]     Train net output #0: loss = 0.727481 (* 1 = 0.727481 loss)
I0205 00:07:34.589396 10511 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I0205 00:07:39.282935 10511 solver.cpp:237] Iteration 1510, loss = 0.714396
I0205 00:07:39.283022 10511 solver.cpp:253]     Train net output #0: loss = 0.714396 (* 1 = 0.714396 loss)
I0205 00:07:39.283035 10511 sgd_solver.cpp:106] Iteration 1510, lr = 0.001
I0205 00:07:43.972465 10511 solver.cpp:237] Iteration 1520, loss = 0.703895
I0205 00:07:43.972705 10511 solver.cpp:253]     Train net output #0: loss = 0.703895 (* 1 = 0.703895 loss)
I0205 00:07:43.972720 10511 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I0205 00:07:48.663293 10511 solver.cpp:237] Iteration 1530, loss = 0.720794
I0205 00:07:48.663370 10511 solver.cpp:253]     Train net output #0: loss = 0.720794 (* 1 = 0.720794 loss)
I0205 00:07:48.663383 10511 sgd_solver.cpp:106] Iteration 1530, lr = 0.001
I0205 00:07:53.353304 10511 solver.cpp:237] Iteration 1540, loss = 0.681602
I0205 00:07:53.353379 10511 solver.cpp:253]     Train net output #0: loss = 0.681602 (* 1 = 0.681602 loss)
I0205 00:07:53.353391 10511 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I0205 00:07:58.042778 10511 solver.cpp:237] Iteration 1550, loss = 0.691958
I0205 00:07:58.042851 10511 solver.cpp:253]     Train net output #0: loss = 0.691958 (* 1 = 0.691958 loss)
I0205 00:07:58.042865 10511 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I0205 00:08:02.731531 10511 solver.cpp:237] Iteration 1560, loss = 0.700956
I0205 00:08:02.731609 10511 solver.cpp:253]     Train net output #0: loss = 0.700956 (* 1 = 0.700956 loss)
I0205 00:08:02.731622 10511 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I0205 00:08:07.421346 10511 solver.cpp:237] Iteration 1570, loss = 0.716034
I0205 00:08:07.421424 10511 solver.cpp:253]     Train net output #0: loss = 0.716034 (* 1 = 0.716034 loss)
I0205 00:08:07.421437 10511 sgd_solver.cpp:106] Iteration 1570, lr = 0.001
I0205 00:08:12.112121 10511 solver.cpp:237] Iteration 1580, loss = 0.717568
I0205 00:08:12.112195 10511 solver.cpp:253]     Train net output #0: loss = 0.717568 (* 1 = 0.717568 loss)
I0205 00:08:12.112206 10511 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I0205 00:08:16.801625 10511 solver.cpp:237] Iteration 1590, loss = 0.712605
I0205 00:08:16.801839 10511 solver.cpp:253]     Train net output #0: loss = 0.712605 (* 1 = 0.712605 loss)
I0205 00:08:16.801856 10511 sgd_solver.cpp:106] Iteration 1590, lr = 0.001
I0205 00:08:21.023406 10511 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1600.caffemodel
I0205 00:08:21.025468 10511 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed11/snaps/snap__iter_1600.solverstate
I0205 00:08:21.252470 10511 solver.cpp:321] Iteration 1600, loss = 0.679829
I0205 00:08:21.252526 10511 solver.cpp:341] Iteration 1600, Testing net (#0)
I0205 00:08:23.528547 10511 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 00:08:23.528614 10511 solver.cpp:409]     Test net output #1: loss = 0.695749 (* 1 = 0.695749 loss)
I0205 00:08:23.528623 10511 solver.cpp:326] Optimization Done.
I0205 00:08:23.528630 10511 caffe.cpp:215] Optimization Done.
