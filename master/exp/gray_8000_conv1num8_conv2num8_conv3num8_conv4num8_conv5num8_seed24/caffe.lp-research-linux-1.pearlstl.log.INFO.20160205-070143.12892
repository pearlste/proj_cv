Log file created at: 2016/02/05 07:01:43
Running on machine: lp-research-linux-1
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0205 07:01:43.964197 12892 caffe.cpp:177] Use CPU.
I0205 07:01:43.965075 12892 solver.cpp:48] Initializing solver from parameters: 
test_iter: 10
test_interval: 100
base_lr: 0.001
display: 10
max_iter: 1600
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 100
snapshot_prefix: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap_"
solver_mode: CPU
random_seed: 24
net: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/train_val.prototxt"
I0205 07:01:43.965248 12892 solver.cpp:91] Creating training net from net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/train_val.prototxt
I0205 07:01:43.965864 12892 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0205 07:01:43.965900 12892 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0205 07:01:43.966178 12892 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 07:01:43.966325 12892 layer_factory.hpp:77] Creating layer data
I0205 07:01:43.966514 12892 net.cpp:106] Creating Layer data
I0205 07:01:43.966534 12892 net.cpp:411] data -> data
I0205 07:01:43.966639 12892 net.cpp:411] data -> label
I0205 07:01:43.966666 12892 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto
I0205 07:01:43.966830 12893 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db
I0205 07:01:43.967816 12892 data_layer.cpp:41] output data size: 100,1,227,227
I0205 07:01:43.996435 12892 net.cpp:150] Setting up data
I0205 07:01:43.996469 12892 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 07:01:43.996477 12892 net.cpp:157] Top shape: 100 (100)
I0205 07:01:43.996484 12892 net.cpp:165] Memory required for data: 20612000
I0205 07:01:43.996498 12892 layer_factory.hpp:77] Creating layer conv1
I0205 07:01:43.996539 12892 net.cpp:106] Creating Layer conv1
I0205 07:01:43.996549 12892 net.cpp:454] conv1 <- data
I0205 07:01:43.996569 12892 net.cpp:411] conv1 -> conv1
I0205 07:01:43.996683 12892 net.cpp:150] Setting up conv1
I0205 07:01:43.996695 12892 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 07:01:43.996701 12892 net.cpp:165] Memory required for data: 30292000
I0205 07:01:43.996718 12892 layer_factory.hpp:77] Creating layer relu1
I0205 07:01:43.996729 12892 net.cpp:106] Creating Layer relu1
I0205 07:01:43.996736 12892 net.cpp:454] relu1 <- conv1
I0205 07:01:43.996744 12892 net.cpp:397] relu1 -> conv1 (in-place)
I0205 07:01:43.996757 12892 net.cpp:150] Setting up relu1
I0205 07:01:43.996765 12892 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 07:01:43.996770 12892 net.cpp:165] Memory required for data: 39972000
I0205 07:01:43.996775 12892 layer_factory.hpp:77] Creating layer pool1
I0205 07:01:43.996784 12892 net.cpp:106] Creating Layer pool1
I0205 07:01:43.996790 12892 net.cpp:454] pool1 <- conv1
I0205 07:01:43.996801 12892 net.cpp:411] pool1 -> pool1
I0205 07:01:43.996825 12892 net.cpp:150] Setting up pool1
I0205 07:01:43.996832 12892 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 07:01:43.996837 12892 net.cpp:165] Memory required for data: 42304800
I0205 07:01:43.996844 12892 layer_factory.hpp:77] Creating layer norm1
I0205 07:01:43.996862 12892 net.cpp:106] Creating Layer norm1
I0205 07:01:43.996877 12892 net.cpp:454] norm1 <- pool1
I0205 07:01:43.996886 12892 net.cpp:411] norm1 -> norm1
I0205 07:01:43.996903 12892 net.cpp:150] Setting up norm1
I0205 07:01:43.996912 12892 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 07:01:43.996917 12892 net.cpp:165] Memory required for data: 44637600
I0205 07:01:43.996924 12892 layer_factory.hpp:77] Creating layer conv2
I0205 07:01:43.996935 12892 net.cpp:106] Creating Layer conv2
I0205 07:01:43.996940 12892 net.cpp:454] conv2 <- norm1
I0205 07:01:43.996951 12892 net.cpp:411] conv2 -> conv2
I0205 07:01:43.996989 12892 net.cpp:150] Setting up conv2
I0205 07:01:43.996997 12892 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 07:01:43.997002 12892 net.cpp:165] Memory required for data: 46970400
I0205 07:01:43.997012 12892 layer_factory.hpp:77] Creating layer relu2
I0205 07:01:43.997021 12892 net.cpp:106] Creating Layer relu2
I0205 07:01:43.997026 12892 net.cpp:454] relu2 <- conv2
I0205 07:01:43.997035 12892 net.cpp:397] relu2 -> conv2 (in-place)
I0205 07:01:43.997043 12892 net.cpp:150] Setting up relu2
I0205 07:01:43.997051 12892 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 07:01:43.997057 12892 net.cpp:165] Memory required for data: 49303200
I0205 07:01:43.997063 12892 layer_factory.hpp:77] Creating layer pool2
I0205 07:01:43.997072 12892 net.cpp:106] Creating Layer pool2
I0205 07:01:43.997077 12892 net.cpp:454] pool2 <- conv2
I0205 07:01:43.997087 12892 net.cpp:411] pool2 -> pool2
I0205 07:01:43.997097 12892 net.cpp:150] Setting up pool2
I0205 07:01:43.997103 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:43.997109 12892 net.cpp:165] Memory required for data: 49844000
I0205 07:01:43.997114 12892 layer_factory.hpp:77] Creating layer norm2
I0205 07:01:43.997124 12892 net.cpp:106] Creating Layer norm2
I0205 07:01:43.997130 12892 net.cpp:454] norm2 <- pool2
I0205 07:01:43.997138 12892 net.cpp:411] norm2 -> norm2
I0205 07:01:43.997146 12892 net.cpp:150] Setting up norm2
I0205 07:01:43.997153 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:43.997158 12892 net.cpp:165] Memory required for data: 50384800
I0205 07:01:43.997164 12892 layer_factory.hpp:77] Creating layer conv3
I0205 07:01:43.997172 12892 net.cpp:106] Creating Layer conv3
I0205 07:01:43.997177 12892 net.cpp:454] conv3 <- norm2
I0205 07:01:43.997185 12892 net.cpp:411] conv3 -> conv3
I0205 07:01:43.997215 12892 net.cpp:150] Setting up conv3
I0205 07:01:43.997222 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:43.997227 12892 net.cpp:165] Memory required for data: 50925600
I0205 07:01:43.997237 12892 layer_factory.hpp:77] Creating layer relu3
I0205 07:01:43.997246 12892 net.cpp:106] Creating Layer relu3
I0205 07:01:43.997251 12892 net.cpp:454] relu3 <- conv3
I0205 07:01:43.997258 12892 net.cpp:397] relu3 -> conv3 (in-place)
I0205 07:01:43.997267 12892 net.cpp:150] Setting up relu3
I0205 07:01:43.997272 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:43.997277 12892 net.cpp:165] Memory required for data: 51466400
I0205 07:01:43.997283 12892 layer_factory.hpp:77] Creating layer conv4
I0205 07:01:43.997292 12892 net.cpp:106] Creating Layer conv4
I0205 07:01:43.997298 12892 net.cpp:454] conv4 <- conv3
I0205 07:01:43.997305 12892 net.cpp:411] conv4 -> conv4
I0205 07:01:43.997330 12892 net.cpp:150] Setting up conv4
I0205 07:01:43.997339 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:43.997344 12892 net.cpp:165] Memory required for data: 52007200
I0205 07:01:43.997351 12892 layer_factory.hpp:77] Creating layer relu4
I0205 07:01:43.997359 12892 net.cpp:106] Creating Layer relu4
I0205 07:01:43.997364 12892 net.cpp:454] relu4 <- conv4
I0205 07:01:43.997371 12892 net.cpp:397] relu4 -> conv4 (in-place)
I0205 07:01:43.997380 12892 net.cpp:150] Setting up relu4
I0205 07:01:43.997386 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:43.997391 12892 net.cpp:165] Memory required for data: 52548000
I0205 07:01:43.997396 12892 layer_factory.hpp:77] Creating layer conv5
I0205 07:01:43.997411 12892 net.cpp:106] Creating Layer conv5
I0205 07:01:43.997422 12892 net.cpp:454] conv5 <- conv4
I0205 07:01:43.997431 12892 net.cpp:411] conv5 -> conv5
I0205 07:01:43.997454 12892 net.cpp:150] Setting up conv5
I0205 07:01:43.997462 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:43.997467 12892 net.cpp:165] Memory required for data: 53088800
I0205 07:01:43.997478 12892 layer_factory.hpp:77] Creating layer relu5
I0205 07:01:43.997485 12892 net.cpp:106] Creating Layer relu5
I0205 07:01:43.997491 12892 net.cpp:454] relu5 <- conv5
I0205 07:01:43.997498 12892 net.cpp:397] relu5 -> conv5 (in-place)
I0205 07:01:43.997506 12892 net.cpp:150] Setting up relu5
I0205 07:01:43.997512 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:43.997517 12892 net.cpp:165] Memory required for data: 53629600
I0205 07:01:43.997524 12892 layer_factory.hpp:77] Creating layer pool5
I0205 07:01:43.997530 12892 net.cpp:106] Creating Layer pool5
I0205 07:01:43.997535 12892 net.cpp:454] pool5 <- conv5
I0205 07:01:43.997544 12892 net.cpp:411] pool5 -> pool5
I0205 07:01:43.997553 12892 net.cpp:150] Setting up pool5
I0205 07:01:43.997560 12892 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0205 07:01:43.997565 12892 net.cpp:165] Memory required for data: 53744800
I0205 07:01:43.997572 12892 layer_factory.hpp:77] Creating layer fc6
I0205 07:01:43.997587 12892 net.cpp:106] Creating Layer fc6
I0205 07:01:43.997593 12892 net.cpp:454] fc6 <- pool5
I0205 07:01:43.997601 12892 net.cpp:411] fc6 -> fc6
I0205 07:01:43.998406 12892 net.cpp:150] Setting up fc6
I0205 07:01:43.998420 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:43.998425 12892 net.cpp:165] Memory required for data: 53847200
I0205 07:01:43.998433 12892 layer_factory.hpp:77] Creating layer relu6
I0205 07:01:43.998442 12892 net.cpp:106] Creating Layer relu6
I0205 07:01:43.998447 12892 net.cpp:454] relu6 <- fc6
I0205 07:01:43.998456 12892 net.cpp:397] relu6 -> fc6 (in-place)
I0205 07:01:43.998464 12892 net.cpp:150] Setting up relu6
I0205 07:01:43.998471 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:43.998476 12892 net.cpp:165] Memory required for data: 53949600
I0205 07:01:43.998481 12892 layer_factory.hpp:77] Creating layer drop6
I0205 07:01:43.998491 12892 net.cpp:106] Creating Layer drop6
I0205 07:01:43.998497 12892 net.cpp:454] drop6 <- fc6
I0205 07:01:43.998503 12892 net.cpp:397] drop6 -> fc6 (in-place)
I0205 07:01:43.998522 12892 net.cpp:150] Setting up drop6
I0205 07:01:43.998529 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:43.998534 12892 net.cpp:165] Memory required for data: 54052000
I0205 07:01:43.998539 12892 layer_factory.hpp:77] Creating layer fc7
I0205 07:01:43.998548 12892 net.cpp:106] Creating Layer fc7
I0205 07:01:43.998553 12892 net.cpp:454] fc7 <- fc6
I0205 07:01:43.998561 12892 net.cpp:411] fc7 -> fc7
I0205 07:01:43.999255 12892 net.cpp:150] Setting up fc7
I0205 07:01:43.999267 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:43.999272 12892 net.cpp:165] Memory required for data: 54154400
I0205 07:01:43.999281 12892 layer_factory.hpp:77] Creating layer relu7
I0205 07:01:43.999289 12892 net.cpp:106] Creating Layer relu7
I0205 07:01:43.999297 12892 net.cpp:454] relu7 <- fc7
I0205 07:01:43.999305 12892 net.cpp:397] relu7 -> fc7 (in-place)
I0205 07:01:43.999315 12892 net.cpp:150] Setting up relu7
I0205 07:01:43.999320 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:43.999331 12892 net.cpp:165] Memory required for data: 54256800
I0205 07:01:43.999336 12892 layer_factory.hpp:77] Creating layer drop7
I0205 07:01:43.999346 12892 net.cpp:106] Creating Layer drop7
I0205 07:01:43.999352 12892 net.cpp:454] drop7 <- fc7
I0205 07:01:43.999359 12892 net.cpp:397] drop7 -> fc7 (in-place)
I0205 07:01:43.999369 12892 net.cpp:150] Setting up drop7
I0205 07:01:43.999377 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:43.999382 12892 net.cpp:165] Memory required for data: 54359200
I0205 07:01:43.999387 12892 layer_factory.hpp:77] Creating layer fc8
I0205 07:01:43.999397 12892 net.cpp:106] Creating Layer fc8
I0205 07:01:43.999408 12892 net.cpp:454] fc8 <- fc7
I0205 07:01:43.999423 12892 net.cpp:411] fc8 -> fc8
I0205 07:01:43.999450 12892 net.cpp:150] Setting up fc8
I0205 07:01:43.999457 12892 net.cpp:157] Top shape: 100 2 (200)
I0205 07:01:43.999462 12892 net.cpp:165] Memory required for data: 54360000
I0205 07:01:43.999470 12892 layer_factory.hpp:77] Creating layer loss
I0205 07:01:43.999478 12892 net.cpp:106] Creating Layer loss
I0205 07:01:43.999485 12892 net.cpp:454] loss <- fc8
I0205 07:01:43.999490 12892 net.cpp:454] loss <- label
I0205 07:01:43.999501 12892 net.cpp:411] loss -> loss
I0205 07:01:43.999516 12892 layer_factory.hpp:77] Creating layer loss
I0205 07:01:43.999538 12892 net.cpp:150] Setting up loss
I0205 07:01:43.999546 12892 net.cpp:157] Top shape: (1)
I0205 07:01:43.999550 12892 net.cpp:160]     with loss weight 1
I0205 07:01:43.999577 12892 net.cpp:165] Memory required for data: 54360004
I0205 07:01:43.999585 12892 net.cpp:226] loss needs backward computation.
I0205 07:01:43.999593 12892 net.cpp:226] fc8 needs backward computation.
I0205 07:01:43.999598 12892 net.cpp:226] drop7 needs backward computation.
I0205 07:01:43.999603 12892 net.cpp:226] relu7 needs backward computation.
I0205 07:01:43.999608 12892 net.cpp:226] fc7 needs backward computation.
I0205 07:01:43.999614 12892 net.cpp:226] drop6 needs backward computation.
I0205 07:01:43.999619 12892 net.cpp:226] relu6 needs backward computation.
I0205 07:01:43.999624 12892 net.cpp:226] fc6 needs backward computation.
I0205 07:01:43.999629 12892 net.cpp:226] pool5 needs backward computation.
I0205 07:01:43.999635 12892 net.cpp:226] relu5 needs backward computation.
I0205 07:01:43.999640 12892 net.cpp:226] conv5 needs backward computation.
I0205 07:01:43.999645 12892 net.cpp:226] relu4 needs backward computation.
I0205 07:01:43.999650 12892 net.cpp:226] conv4 needs backward computation.
I0205 07:01:43.999656 12892 net.cpp:226] relu3 needs backward computation.
I0205 07:01:43.999661 12892 net.cpp:226] conv3 needs backward computation.
I0205 07:01:43.999670 12892 net.cpp:226] norm2 needs backward computation.
I0205 07:01:43.999676 12892 net.cpp:226] pool2 needs backward computation.
I0205 07:01:43.999682 12892 net.cpp:226] relu2 needs backward computation.
I0205 07:01:43.999689 12892 net.cpp:226] conv2 needs backward computation.
I0205 07:01:43.999696 12892 net.cpp:226] norm1 needs backward computation.
I0205 07:01:43.999701 12892 net.cpp:226] pool1 needs backward computation.
I0205 07:01:43.999706 12892 net.cpp:226] relu1 needs backward computation.
I0205 07:01:43.999712 12892 net.cpp:226] conv1 needs backward computation.
I0205 07:01:43.999718 12892 net.cpp:228] data does not need backward computation.
I0205 07:01:43.999723 12892 net.cpp:270] This network produces output loss
I0205 07:01:43.999752 12892 net.cpp:283] Network initialization done.
I0205 07:01:44.000476 12892 solver.cpp:181] Creating test net (#0) specified by net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/train_val.prototxt
I0205 07:01:44.000531 12892 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0205 07:01:44.000818 12892 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 07:01:44.001016 12892 layer_factory.hpp:77] Creating layer data
I0205 07:01:44.001183 12892 net.cpp:106] Creating Layer data
I0205 07:01:44.001196 12892 net.cpp:411] data -> data
I0205 07:01:44.001209 12892 net.cpp:411] data -> label
I0205 07:01:44.001220 12892 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto
I0205 07:01:44.001432 12896 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db
I0205 07:01:44.002162 12892 data_layer.cpp:41] output data size: 100,1,227,227
I0205 07:01:44.030941 12892 net.cpp:150] Setting up data
I0205 07:01:44.030961 12892 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 07:01:44.030974 12892 net.cpp:157] Top shape: 100 (100)
I0205 07:01:44.030982 12892 net.cpp:165] Memory required for data: 20612000
I0205 07:01:44.030989 12892 layer_factory.hpp:77] Creating layer label_data_1_split
I0205 07:01:44.031004 12892 net.cpp:106] Creating Layer label_data_1_split
I0205 07:01:44.031011 12892 net.cpp:454] label_data_1_split <- label
I0205 07:01:44.031023 12892 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0205 07:01:44.031034 12892 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0205 07:01:44.031045 12892 net.cpp:150] Setting up label_data_1_split
I0205 07:01:44.031052 12892 net.cpp:157] Top shape: 100 (100)
I0205 07:01:44.031059 12892 net.cpp:157] Top shape: 100 (100)
I0205 07:01:44.031080 12892 net.cpp:165] Memory required for data: 20612800
I0205 07:01:44.031085 12892 layer_factory.hpp:77] Creating layer conv1
I0205 07:01:44.031097 12892 net.cpp:106] Creating Layer conv1
I0205 07:01:44.031103 12892 net.cpp:454] conv1 <- data
I0205 07:01:44.031111 12892 net.cpp:411] conv1 -> conv1
I0205 07:01:44.031153 12892 net.cpp:150] Setting up conv1
I0205 07:01:44.031162 12892 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 07:01:44.031167 12892 net.cpp:165] Memory required for data: 30292800
I0205 07:01:44.031179 12892 layer_factory.hpp:77] Creating layer relu1
I0205 07:01:44.031190 12892 net.cpp:106] Creating Layer relu1
I0205 07:01:44.031196 12892 net.cpp:454] relu1 <- conv1
I0205 07:01:44.031203 12892 net.cpp:397] relu1 -> conv1 (in-place)
I0205 07:01:44.031213 12892 net.cpp:150] Setting up relu1
I0205 07:01:44.031219 12892 net.cpp:157] Top shape: 100 8 55 55 (2420000)
I0205 07:01:44.031224 12892 net.cpp:165] Memory required for data: 39972800
I0205 07:01:44.031229 12892 layer_factory.hpp:77] Creating layer pool1
I0205 07:01:44.031239 12892 net.cpp:106] Creating Layer pool1
I0205 07:01:44.031244 12892 net.cpp:454] pool1 <- conv1
I0205 07:01:44.031252 12892 net.cpp:411] pool1 -> pool1
I0205 07:01:44.031265 12892 net.cpp:150] Setting up pool1
I0205 07:01:44.031271 12892 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 07:01:44.031276 12892 net.cpp:165] Memory required for data: 42305600
I0205 07:01:44.031285 12892 layer_factory.hpp:77] Creating layer norm1
I0205 07:01:44.031292 12892 net.cpp:106] Creating Layer norm1
I0205 07:01:44.031298 12892 net.cpp:454] norm1 <- pool1
I0205 07:01:44.031306 12892 net.cpp:411] norm1 -> norm1
I0205 07:01:44.031316 12892 net.cpp:150] Setting up norm1
I0205 07:01:44.031322 12892 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 07:01:44.031327 12892 net.cpp:165] Memory required for data: 44638400
I0205 07:01:44.031333 12892 layer_factory.hpp:77] Creating layer conv2
I0205 07:01:44.031344 12892 net.cpp:106] Creating Layer conv2
I0205 07:01:44.031350 12892 net.cpp:454] conv2 <- norm1
I0205 07:01:44.031358 12892 net.cpp:411] conv2 -> conv2
I0205 07:01:44.031386 12892 net.cpp:150] Setting up conv2
I0205 07:01:44.031394 12892 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 07:01:44.031399 12892 net.cpp:165] Memory required for data: 46971200
I0205 07:01:44.031409 12892 layer_factory.hpp:77] Creating layer relu2
I0205 07:01:44.031417 12892 net.cpp:106] Creating Layer relu2
I0205 07:01:44.031424 12892 net.cpp:454] relu2 <- conv2
I0205 07:01:44.031430 12892 net.cpp:397] relu2 -> conv2 (in-place)
I0205 07:01:44.031446 12892 net.cpp:150] Setting up relu2
I0205 07:01:44.031461 12892 net.cpp:157] Top shape: 100 8 27 27 (583200)
I0205 07:01:44.031466 12892 net.cpp:165] Memory required for data: 49304000
I0205 07:01:44.031471 12892 layer_factory.hpp:77] Creating layer pool2
I0205 07:01:44.031479 12892 net.cpp:106] Creating Layer pool2
I0205 07:01:44.031484 12892 net.cpp:454] pool2 <- conv2
I0205 07:01:44.031492 12892 net.cpp:411] pool2 -> pool2
I0205 07:01:44.031505 12892 net.cpp:150] Setting up pool2
I0205 07:01:44.031513 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:44.031518 12892 net.cpp:165] Memory required for data: 49844800
I0205 07:01:44.031523 12892 layer_factory.hpp:77] Creating layer norm2
I0205 07:01:44.031530 12892 net.cpp:106] Creating Layer norm2
I0205 07:01:44.031536 12892 net.cpp:454] norm2 <- pool2
I0205 07:01:44.031544 12892 net.cpp:411] norm2 -> norm2
I0205 07:01:44.031553 12892 net.cpp:150] Setting up norm2
I0205 07:01:44.031559 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:44.031564 12892 net.cpp:165] Memory required for data: 50385600
I0205 07:01:44.031569 12892 layer_factory.hpp:77] Creating layer conv3
I0205 07:01:44.031579 12892 net.cpp:106] Creating Layer conv3
I0205 07:01:44.031584 12892 net.cpp:454] conv3 <- norm2
I0205 07:01:44.031591 12892 net.cpp:411] conv3 -> conv3
I0205 07:01:44.031618 12892 net.cpp:150] Setting up conv3
I0205 07:01:44.031626 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:44.031631 12892 net.cpp:165] Memory required for data: 50926400
I0205 07:01:44.031641 12892 layer_factory.hpp:77] Creating layer relu3
I0205 07:01:44.031648 12892 net.cpp:106] Creating Layer relu3
I0205 07:01:44.031656 12892 net.cpp:454] relu3 <- conv3
I0205 07:01:44.031663 12892 net.cpp:397] relu3 -> conv3 (in-place)
I0205 07:01:44.031672 12892 net.cpp:150] Setting up relu3
I0205 07:01:44.031678 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:44.031683 12892 net.cpp:165] Memory required for data: 51467200
I0205 07:01:44.031689 12892 layer_factory.hpp:77] Creating layer conv4
I0205 07:01:44.031697 12892 net.cpp:106] Creating Layer conv4
I0205 07:01:44.031702 12892 net.cpp:454] conv4 <- conv3
I0205 07:01:44.031710 12892 net.cpp:411] conv4 -> conv4
I0205 07:01:44.031730 12892 net.cpp:150] Setting up conv4
I0205 07:01:44.031738 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:44.031743 12892 net.cpp:165] Memory required for data: 52008000
I0205 07:01:44.031752 12892 layer_factory.hpp:77] Creating layer relu4
I0205 07:01:44.031760 12892 net.cpp:106] Creating Layer relu4
I0205 07:01:44.031765 12892 net.cpp:454] relu4 <- conv4
I0205 07:01:44.031772 12892 net.cpp:397] relu4 -> conv4 (in-place)
I0205 07:01:44.031780 12892 net.cpp:150] Setting up relu4
I0205 07:01:44.031786 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:44.031792 12892 net.cpp:165] Memory required for data: 52548800
I0205 07:01:44.031797 12892 layer_factory.hpp:77] Creating layer conv5
I0205 07:01:44.031806 12892 net.cpp:106] Creating Layer conv5
I0205 07:01:44.031815 12892 net.cpp:454] conv5 <- conv4
I0205 07:01:44.031822 12892 net.cpp:411] conv5 -> conv5
I0205 07:01:44.031844 12892 net.cpp:150] Setting up conv5
I0205 07:01:44.031852 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:44.031857 12892 net.cpp:165] Memory required for data: 53089600
I0205 07:01:44.031865 12892 layer_factory.hpp:77] Creating layer relu5
I0205 07:01:44.031873 12892 net.cpp:106] Creating Layer relu5
I0205 07:01:44.031878 12892 net.cpp:454] relu5 <- conv5
I0205 07:01:44.031885 12892 net.cpp:397] relu5 -> conv5 (in-place)
I0205 07:01:44.031893 12892 net.cpp:150] Setting up relu5
I0205 07:01:44.031901 12892 net.cpp:157] Top shape: 100 8 13 13 (135200)
I0205 07:01:44.031908 12892 net.cpp:165] Memory required for data: 53630400
I0205 07:01:44.031913 12892 layer_factory.hpp:77] Creating layer pool5
I0205 07:01:44.031921 12892 net.cpp:106] Creating Layer pool5
I0205 07:01:44.031929 12892 net.cpp:454] pool5 <- conv5
I0205 07:01:44.031935 12892 net.cpp:411] pool5 -> pool5
I0205 07:01:44.031950 12892 net.cpp:150] Setting up pool5
I0205 07:01:44.031975 12892 net.cpp:157] Top shape: 100 8 6 6 (28800)
I0205 07:01:44.031980 12892 net.cpp:165] Memory required for data: 53745600
I0205 07:01:44.031986 12892 layer_factory.hpp:77] Creating layer fc6
I0205 07:01:44.031996 12892 net.cpp:106] Creating Layer fc6
I0205 07:01:44.032002 12892 net.cpp:454] fc6 <- pool5
I0205 07:01:44.032011 12892 net.cpp:411] fc6 -> fc6
I0205 07:01:44.032737 12892 net.cpp:150] Setting up fc6
I0205 07:01:44.032752 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:44.032757 12892 net.cpp:165] Memory required for data: 53848000
I0205 07:01:44.032765 12892 layer_factory.hpp:77] Creating layer relu6
I0205 07:01:44.032776 12892 net.cpp:106] Creating Layer relu6
I0205 07:01:44.032783 12892 net.cpp:454] relu6 <- fc6
I0205 07:01:44.032789 12892 net.cpp:397] relu6 -> fc6 (in-place)
I0205 07:01:44.032799 12892 net.cpp:150] Setting up relu6
I0205 07:01:44.032804 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:44.032809 12892 net.cpp:165] Memory required for data: 53950400
I0205 07:01:44.032814 12892 layer_factory.hpp:77] Creating layer drop6
I0205 07:01:44.032824 12892 net.cpp:106] Creating Layer drop6
I0205 07:01:44.032829 12892 net.cpp:454] drop6 <- fc6
I0205 07:01:44.032841 12892 net.cpp:397] drop6 -> fc6 (in-place)
I0205 07:01:44.032852 12892 net.cpp:150] Setting up drop6
I0205 07:01:44.032858 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:44.032863 12892 net.cpp:165] Memory required for data: 54052800
I0205 07:01:44.032868 12892 layer_factory.hpp:77] Creating layer fc7
I0205 07:01:44.032877 12892 net.cpp:106] Creating Layer fc7
I0205 07:01:44.032882 12892 net.cpp:454] fc7 <- fc6
I0205 07:01:44.032892 12892 net.cpp:411] fc7 -> fc7
I0205 07:01:44.033649 12892 net.cpp:150] Setting up fc7
I0205 07:01:44.033663 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:44.033668 12892 net.cpp:165] Memory required for data: 54155200
I0205 07:01:44.033676 12892 layer_factory.hpp:77] Creating layer relu7
I0205 07:01:44.033684 12892 net.cpp:106] Creating Layer relu7
I0205 07:01:44.033690 12892 net.cpp:454] relu7 <- fc7
I0205 07:01:44.033700 12892 net.cpp:397] relu7 -> fc7 (in-place)
I0205 07:01:44.033710 12892 net.cpp:150] Setting up relu7
I0205 07:01:44.033715 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:44.033720 12892 net.cpp:165] Memory required for data: 54257600
I0205 07:01:44.033726 12892 layer_factory.hpp:77] Creating layer drop7
I0205 07:01:44.033735 12892 net.cpp:106] Creating Layer drop7
I0205 07:01:44.033741 12892 net.cpp:454] drop7 <- fc7
I0205 07:01:44.033748 12892 net.cpp:397] drop7 -> fc7 (in-place)
I0205 07:01:44.033757 12892 net.cpp:150] Setting up drop7
I0205 07:01:44.033764 12892 net.cpp:157] Top shape: 100 256 (25600)
I0205 07:01:44.033769 12892 net.cpp:165] Memory required for data: 54360000
I0205 07:01:44.033776 12892 layer_factory.hpp:77] Creating layer fc8
I0205 07:01:44.033788 12892 net.cpp:106] Creating Layer fc8
I0205 07:01:44.033794 12892 net.cpp:454] fc8 <- fc7
I0205 07:01:44.033804 12892 net.cpp:411] fc8 -> fc8
I0205 07:01:44.033828 12892 net.cpp:150] Setting up fc8
I0205 07:01:44.033838 12892 net.cpp:157] Top shape: 100 2 (200)
I0205 07:01:44.033843 12892 net.cpp:165] Memory required for data: 54360800
I0205 07:01:44.033851 12892 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0205 07:01:44.033859 12892 net.cpp:106] Creating Layer fc8_fc8_0_split
I0205 07:01:44.033864 12892 net.cpp:454] fc8_fc8_0_split <- fc8
I0205 07:01:44.033871 12892 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0205 07:01:44.033881 12892 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0205 07:01:44.033891 12892 net.cpp:150] Setting up fc8_fc8_0_split
I0205 07:01:44.033898 12892 net.cpp:157] Top shape: 100 2 (200)
I0205 07:01:44.033905 12892 net.cpp:157] Top shape: 100 2 (200)
I0205 07:01:44.033910 12892 net.cpp:165] Memory required for data: 54362400
I0205 07:01:44.033915 12892 layer_factory.hpp:77] Creating layer accuracy
I0205 07:01:44.033931 12892 net.cpp:106] Creating Layer accuracy
I0205 07:01:44.033941 12892 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0205 07:01:44.033959 12892 net.cpp:454] accuracy <- label_data_1_split_0
I0205 07:01:44.033977 12892 net.cpp:411] accuracy -> accuracy
I0205 07:01:44.033989 12892 net.cpp:150] Setting up accuracy
I0205 07:01:44.033996 12892 net.cpp:157] Top shape: (1)
I0205 07:01:44.034001 12892 net.cpp:165] Memory required for data: 54362404
I0205 07:01:44.034006 12892 layer_factory.hpp:77] Creating layer loss
I0205 07:01:44.034014 12892 net.cpp:106] Creating Layer loss
I0205 07:01:44.034020 12892 net.cpp:454] loss <- fc8_fc8_0_split_1
I0205 07:01:44.034028 12892 net.cpp:454] loss <- label_data_1_split_1
I0205 07:01:44.034037 12892 net.cpp:411] loss -> loss
I0205 07:01:44.034047 12892 layer_factory.hpp:77] Creating layer loss
I0205 07:01:44.034083 12892 net.cpp:150] Setting up loss
I0205 07:01:44.034092 12892 net.cpp:157] Top shape: (1)
I0205 07:01:44.034097 12892 net.cpp:160]     with loss weight 1
I0205 07:01:44.034107 12892 net.cpp:165] Memory required for data: 54362408
I0205 07:01:44.034113 12892 net.cpp:226] loss needs backward computation.
I0205 07:01:44.034119 12892 net.cpp:228] accuracy does not need backward computation.
I0205 07:01:44.034126 12892 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0205 07:01:44.034132 12892 net.cpp:226] fc8 needs backward computation.
I0205 07:01:44.034137 12892 net.cpp:226] drop7 needs backward computation.
I0205 07:01:44.034143 12892 net.cpp:226] relu7 needs backward computation.
I0205 07:01:44.034148 12892 net.cpp:226] fc7 needs backward computation.
I0205 07:01:44.034153 12892 net.cpp:226] drop6 needs backward computation.
I0205 07:01:44.034159 12892 net.cpp:226] relu6 needs backward computation.
I0205 07:01:44.034164 12892 net.cpp:226] fc6 needs backward computation.
I0205 07:01:44.034169 12892 net.cpp:226] pool5 needs backward computation.
I0205 07:01:44.034175 12892 net.cpp:226] relu5 needs backward computation.
I0205 07:01:44.034180 12892 net.cpp:226] conv5 needs backward computation.
I0205 07:01:44.034188 12892 net.cpp:226] relu4 needs backward computation.
I0205 07:01:44.034194 12892 net.cpp:226] conv4 needs backward computation.
I0205 07:01:44.034199 12892 net.cpp:226] relu3 needs backward computation.
I0205 07:01:44.034204 12892 net.cpp:226] conv3 needs backward computation.
I0205 07:01:44.034210 12892 net.cpp:226] norm2 needs backward computation.
I0205 07:01:44.034219 12892 net.cpp:226] pool2 needs backward computation.
I0205 07:01:44.034224 12892 net.cpp:226] relu2 needs backward computation.
I0205 07:01:44.034229 12892 net.cpp:226] conv2 needs backward computation.
I0205 07:01:44.034235 12892 net.cpp:226] norm1 needs backward computation.
I0205 07:01:44.034240 12892 net.cpp:226] pool1 needs backward computation.
I0205 07:01:44.034246 12892 net.cpp:226] relu1 needs backward computation.
I0205 07:01:44.034251 12892 net.cpp:226] conv1 needs backward computation.
I0205 07:01:44.034257 12892 net.cpp:228] label_data_1_split does not need backward computation.
I0205 07:01:44.034265 12892 net.cpp:228] data does not need backward computation.
I0205 07:01:44.034270 12892 net.cpp:270] This network produces output accuracy
I0205 07:01:44.034276 12892 net.cpp:270] This network produces output loss
I0205 07:01:44.034304 12892 net.cpp:283] Network initialization done.
I0205 07:01:44.034397 12892 solver.cpp:60] Solver scaffolding done.
I0205 07:01:44.034448 12892 caffe.cpp:212] Starting Optimization
I0205 07:01:44.034456 12892 solver.cpp:288] Solving CaffeNet
I0205 07:01:44.034461 12892 solver.cpp:289] Learning Rate Policy: step
I0205 07:01:44.035009 12892 solver.cpp:341] Iteration 0, Testing net (#0)
I0205 07:01:44.035073 12892 blocking_queue.cpp:50] Data layer prefetch queue empty
I0205 07:01:46.531520 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:01:46.531576 12892 solver.cpp:409]     Test net output #1: loss = 8.90837 (* 1 = 8.90837 loss)
I0205 07:01:47.076586 12892 solver.cpp:237] Iteration 0, loss = 14.9383
I0205 07:01:47.076638 12892 solver.cpp:253]     Train net output #0: loss = 14.9383 (* 1 = 14.9383 loss)
I0205 07:01:47.076660 12892 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0205 07:01:51.919350 12892 solver.cpp:237] Iteration 10, loss = 1.25498
I0205 07:01:51.919402 12892 solver.cpp:253]     Train net output #0: loss = 1.25498 (* 1 = 1.25498 loss)
I0205 07:01:51.919412 12892 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0205 07:01:56.751709 12892 solver.cpp:237] Iteration 20, loss = 0.906164
I0205 07:01:56.751761 12892 solver.cpp:253]     Train net output #0: loss = 0.906164 (* 1 = 0.906164 loss)
I0205 07:01:56.751772 12892 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0205 07:02:01.581878 12892 solver.cpp:237] Iteration 30, loss = 0.806419
I0205 07:02:01.581931 12892 solver.cpp:253]     Train net output #0: loss = 0.806419 (* 1 = 0.806419 loss)
I0205 07:02:01.581943 12892 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0205 07:02:06.411275 12892 solver.cpp:237] Iteration 40, loss = 0.955222
I0205 07:02:06.411326 12892 solver.cpp:253]     Train net output #0: loss = 0.955222 (* 1 = 0.955222 loss)
I0205 07:02:06.411337 12892 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0205 07:02:11.237181 12892 solver.cpp:237] Iteration 50, loss = 0.869153
I0205 07:02:11.237231 12892 solver.cpp:253]     Train net output #0: loss = 0.869153 (* 1 = 0.869153 loss)
I0205 07:02:11.237242 12892 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0205 07:02:16.065537 12892 solver.cpp:237] Iteration 60, loss = 0.78618
I0205 07:02:16.065781 12892 solver.cpp:253]     Train net output #0: loss = 0.78618 (* 1 = 0.78618 loss)
I0205 07:02:16.065794 12892 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0205 07:02:20.896940 12892 solver.cpp:237] Iteration 70, loss = 0.770758
I0205 07:02:20.896998 12892 solver.cpp:253]     Train net output #0: loss = 0.770758 (* 1 = 0.770758 loss)
I0205 07:02:20.897009 12892 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0205 07:02:25.727857 12892 solver.cpp:237] Iteration 80, loss = 0.750935
I0205 07:02:25.727915 12892 solver.cpp:253]     Train net output #0: loss = 0.750935 (* 1 = 0.750935 loss)
I0205 07:02:25.727926 12892 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0205 07:02:30.574647 12892 solver.cpp:237] Iteration 90, loss = 0.789878
I0205 07:02:30.574698 12892 solver.cpp:253]     Train net output #0: loss = 0.789878 (* 1 = 0.789878 loss)
I0205 07:02:30.574709 12892 sgd_solver.cpp:106] Iteration 90, lr = 0.001
I0205 07:02:34.945086 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_100.caffemodel
I0205 07:02:34.947805 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_100.solverstate
I0205 07:02:34.948603 12892 solver.cpp:341] Iteration 100, Testing net (#0)
I0205 07:02:37.314239 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:02:37.314291 12892 solver.cpp:409]     Test net output #1: loss = 0.707942 (* 1 = 0.707942 loss)
I0205 07:02:37.799351 12892 solver.cpp:237] Iteration 100, loss = 0.734951
I0205 07:02:37.799398 12892 solver.cpp:253]     Train net output #0: loss = 0.734951 (* 1 = 0.734951 loss)
I0205 07:02:37.799410 12892 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0205 07:02:42.659592 12892 solver.cpp:237] Iteration 110, loss = 0.814073
I0205 07:02:42.659651 12892 solver.cpp:253]     Train net output #0: loss = 0.814073 (* 1 = 0.814073 loss)
I0205 07:02:42.659662 12892 sgd_solver.cpp:106] Iteration 110, lr = 0.001
I0205 07:02:47.513131 12892 solver.cpp:237] Iteration 120, loss = 0.766108
I0205 07:02:47.513236 12892 solver.cpp:253]     Train net output #0: loss = 0.766108 (* 1 = 0.766108 loss)
I0205 07:02:47.513247 12892 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0205 07:02:52.369843 12892 solver.cpp:237] Iteration 130, loss = 0.701414
I0205 07:02:52.369901 12892 solver.cpp:253]     Train net output #0: loss = 0.701414 (* 1 = 0.701414 loss)
I0205 07:02:52.369912 12892 sgd_solver.cpp:106] Iteration 130, lr = 0.001
I0205 07:02:57.227258 12892 solver.cpp:237] Iteration 140, loss = 0.78958
I0205 07:02:57.227308 12892 solver.cpp:253]     Train net output #0: loss = 0.78958 (* 1 = 0.78958 loss)
I0205 07:02:57.227319 12892 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0205 07:03:02.084866 12892 solver.cpp:237] Iteration 150, loss = 0.717153
I0205 07:03:02.084918 12892 solver.cpp:253]     Train net output #0: loss = 0.717153 (* 1 = 0.717153 loss)
I0205 07:03:02.084928 12892 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0205 07:03:06.938537 12892 solver.cpp:237] Iteration 160, loss = 0.748856
I0205 07:03:06.938586 12892 solver.cpp:253]     Train net output #0: loss = 0.748856 (* 1 = 0.748856 loss)
I0205 07:03:06.938596 12892 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0205 07:03:11.791858 12892 solver.cpp:237] Iteration 170, loss = 0.731617
I0205 07:03:11.791913 12892 solver.cpp:253]     Train net output #0: loss = 0.731617 (* 1 = 0.731617 loss)
I0205 07:03:11.791923 12892 sgd_solver.cpp:106] Iteration 170, lr = 0.001
I0205 07:03:16.649309 12892 solver.cpp:237] Iteration 180, loss = 0.705628
I0205 07:03:16.649359 12892 solver.cpp:253]     Train net output #0: loss = 0.705628 (* 1 = 0.705628 loss)
I0205 07:03:16.649370 12892 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0205 07:03:21.505833 12892 solver.cpp:237] Iteration 190, loss = 0.713391
I0205 07:03:21.506067 12892 solver.cpp:253]     Train net output #0: loss = 0.713391 (* 1 = 0.713391 loss)
I0205 07:03:21.506079 12892 sgd_solver.cpp:106] Iteration 190, lr = 0.001
I0205 07:03:25.876452 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_200.caffemodel
I0205 07:03:25.878430 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_200.solverstate
I0205 07:03:25.879225 12892 solver.cpp:341] Iteration 200, Testing net (#0)
I0205 07:03:28.244338 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:03:28.244388 12892 solver.cpp:409]     Test net output #1: loss = 0.694063 (* 1 = 0.694063 loss)
I0205 07:03:28.729590 12892 solver.cpp:237] Iteration 200, loss = 0.761485
I0205 07:03:28.729639 12892 solver.cpp:253]     Train net output #0: loss = 0.761485 (* 1 = 0.761485 loss)
I0205 07:03:28.729650 12892 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0205 07:03:33.583948 12892 solver.cpp:237] Iteration 210, loss = 0.722195
I0205 07:03:33.584009 12892 solver.cpp:253]     Train net output #0: loss = 0.722195 (* 1 = 0.722195 loss)
I0205 07:03:33.584020 12892 sgd_solver.cpp:106] Iteration 210, lr = 0.001
I0205 07:03:38.440228 12892 solver.cpp:237] Iteration 220, loss = 0.754506
I0205 07:03:38.440278 12892 solver.cpp:253]     Train net output #0: loss = 0.754506 (* 1 = 0.754506 loss)
I0205 07:03:38.440289 12892 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0205 07:03:43.295837 12892 solver.cpp:237] Iteration 230, loss = 0.739742
I0205 07:03:43.295892 12892 solver.cpp:253]     Train net output #0: loss = 0.739742 (* 1 = 0.739742 loss)
I0205 07:03:43.295903 12892 sgd_solver.cpp:106] Iteration 230, lr = 0.001
I0205 07:03:48.151536 12892 solver.cpp:237] Iteration 240, loss = 0.722735
I0205 07:03:48.151592 12892 solver.cpp:253]     Train net output #0: loss = 0.722735 (* 1 = 0.722735 loss)
I0205 07:03:48.151603 12892 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0205 07:03:53.008304 12892 solver.cpp:237] Iteration 250, loss = 0.745036
I0205 07:03:53.008488 12892 solver.cpp:253]     Train net output #0: loss = 0.745036 (* 1 = 0.745036 loss)
I0205 07:03:53.008502 12892 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0205 07:03:57.864605 12892 solver.cpp:237] Iteration 260, loss = 0.671988
I0205 07:03:57.864661 12892 solver.cpp:253]     Train net output #0: loss = 0.671988 (* 1 = 0.671988 loss)
I0205 07:03:57.864672 12892 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0205 07:04:02.720494 12892 solver.cpp:237] Iteration 270, loss = 0.720014
I0205 07:04:02.720559 12892 solver.cpp:253]     Train net output #0: loss = 0.720014 (* 1 = 0.720014 loss)
I0205 07:04:02.720571 12892 sgd_solver.cpp:106] Iteration 270, lr = 0.001
I0205 07:04:07.574823 12892 solver.cpp:237] Iteration 280, loss = 0.724211
I0205 07:04:07.574874 12892 solver.cpp:253]     Train net output #0: loss = 0.724211 (* 1 = 0.724211 loss)
I0205 07:04:07.574884 12892 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0205 07:04:12.430135 12892 solver.cpp:237] Iteration 290, loss = 0.721874
I0205 07:04:12.430187 12892 solver.cpp:253]     Train net output #0: loss = 0.721874 (* 1 = 0.721874 loss)
I0205 07:04:12.430198 12892 sgd_solver.cpp:106] Iteration 290, lr = 0.001
I0205 07:04:16.799737 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_300.caffemodel
I0205 07:04:16.801774 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_300.solverstate
I0205 07:04:16.802578 12892 solver.cpp:341] Iteration 300, Testing net (#0)
I0205 07:04:19.166848 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:04:19.166898 12892 solver.cpp:409]     Test net output #1: loss = 0.703247 (* 1 = 0.703247 loss)
I0205 07:04:19.652590 12892 solver.cpp:237] Iteration 300, loss = 0.7762
I0205 07:04:19.652637 12892 solver.cpp:253]     Train net output #0: loss = 0.7762 (* 1 = 0.7762 loss)
I0205 07:04:19.652648 12892 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0205 07:04:24.506218 12892 solver.cpp:237] Iteration 310, loss = 0.737172
I0205 07:04:24.506435 12892 solver.cpp:253]     Train net output #0: loss = 0.737172 (* 1 = 0.737172 loss)
I0205 07:04:24.506448 12892 sgd_solver.cpp:106] Iteration 310, lr = 0.001
I0205 07:04:29.362876 12892 solver.cpp:237] Iteration 320, loss = 0.748783
I0205 07:04:29.362931 12892 solver.cpp:253]     Train net output #0: loss = 0.748783 (* 1 = 0.748783 loss)
I0205 07:04:29.362941 12892 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0205 07:04:34.221065 12892 solver.cpp:237] Iteration 330, loss = 0.730752
I0205 07:04:34.221122 12892 solver.cpp:253]     Train net output #0: loss = 0.730752 (* 1 = 0.730752 loss)
I0205 07:04:34.221132 12892 sgd_solver.cpp:106] Iteration 330, lr = 0.001
I0205 07:04:39.078644 12892 solver.cpp:237] Iteration 340, loss = 0.715454
I0205 07:04:39.078694 12892 solver.cpp:253]     Train net output #0: loss = 0.715454 (* 1 = 0.715454 loss)
I0205 07:04:39.078706 12892 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0205 07:04:43.937501 12892 solver.cpp:237] Iteration 350, loss = 0.731474
I0205 07:04:43.937554 12892 solver.cpp:253]     Train net output #0: loss = 0.731474 (* 1 = 0.731474 loss)
I0205 07:04:43.937566 12892 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I0205 07:04:48.794920 12892 solver.cpp:237] Iteration 360, loss = 0.716158
I0205 07:04:48.794983 12892 solver.cpp:253]     Train net output #0: loss = 0.716158 (* 1 = 0.716158 loss)
I0205 07:04:48.794996 12892 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0205 07:04:53.655799 12892 solver.cpp:237] Iteration 370, loss = 0.718665
I0205 07:04:53.655854 12892 solver.cpp:253]     Train net output #0: loss = 0.718665 (* 1 = 0.718665 loss)
I0205 07:04:53.655865 12892 sgd_solver.cpp:106] Iteration 370, lr = 0.001
I0205 07:04:58.512470 12892 solver.cpp:237] Iteration 380, loss = 0.731354
I0205 07:04:58.512650 12892 solver.cpp:253]     Train net output #0: loss = 0.731354 (* 1 = 0.731354 loss)
I0205 07:04:58.512663 12892 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0205 07:05:03.374316 12892 solver.cpp:237] Iteration 390, loss = 0.719752
I0205 07:05:03.374371 12892 solver.cpp:253]     Train net output #0: loss = 0.719752 (* 1 = 0.719752 loss)
I0205 07:05:03.374382 12892 sgd_solver.cpp:106] Iteration 390, lr = 0.001
I0205 07:05:07.746369 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_400.caffemodel
I0205 07:05:07.748353 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_400.solverstate
I0205 07:05:07.749148 12892 solver.cpp:341] Iteration 400, Testing net (#0)
I0205 07:05:10.114034 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:05:10.114081 12892 solver.cpp:409]     Test net output #1: loss = 0.693449 (* 1 = 0.693449 loss)
I0205 07:05:10.599238 12892 solver.cpp:237] Iteration 400, loss = 0.704907
I0205 07:05:10.599285 12892 solver.cpp:253]     Train net output #0: loss = 0.704907 (* 1 = 0.704907 loss)
I0205 07:05:10.599297 12892 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0205 07:05:15.456944 12892 solver.cpp:237] Iteration 410, loss = 0.724439
I0205 07:05:15.457000 12892 solver.cpp:253]     Train net output #0: loss = 0.724439 (* 1 = 0.724439 loss)
I0205 07:05:15.457010 12892 sgd_solver.cpp:106] Iteration 410, lr = 0.001
I0205 07:05:20.312418 12892 solver.cpp:237] Iteration 420, loss = 0.701807
I0205 07:05:20.312469 12892 solver.cpp:253]     Train net output #0: loss = 0.701807 (* 1 = 0.701807 loss)
I0205 07:05:20.312479 12892 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0205 07:05:25.169904 12892 solver.cpp:237] Iteration 430, loss = 0.685332
I0205 07:05:25.169958 12892 solver.cpp:253]     Train net output #0: loss = 0.685332 (* 1 = 0.685332 loss)
I0205 07:05:25.169975 12892 sgd_solver.cpp:106] Iteration 430, lr = 0.001
I0205 07:05:30.032515 12892 solver.cpp:237] Iteration 440, loss = 0.717215
I0205 07:05:30.032729 12892 solver.cpp:253]     Train net output #0: loss = 0.717215 (* 1 = 0.717215 loss)
I0205 07:05:30.032742 12892 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0205 07:05:34.901659 12892 solver.cpp:237] Iteration 450, loss = 0.697342
I0205 07:05:34.901712 12892 solver.cpp:253]     Train net output #0: loss = 0.697342 (* 1 = 0.697342 loss)
I0205 07:05:34.901723 12892 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I0205 07:05:39.757941 12892 solver.cpp:237] Iteration 460, loss = 0.712725
I0205 07:05:39.757989 12892 solver.cpp:253]     Train net output #0: loss = 0.712725 (* 1 = 0.712725 loss)
I0205 07:05:39.758000 12892 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0205 07:05:44.633582 12892 solver.cpp:237] Iteration 470, loss = 0.720673
I0205 07:05:44.633633 12892 solver.cpp:253]     Train net output #0: loss = 0.720673 (* 1 = 0.720673 loss)
I0205 07:05:44.633643 12892 sgd_solver.cpp:106] Iteration 470, lr = 0.001
I0205 07:05:49.526010 12892 solver.cpp:237] Iteration 480, loss = 0.695771
I0205 07:05:49.526062 12892 solver.cpp:253]     Train net output #0: loss = 0.695771 (* 1 = 0.695771 loss)
I0205 07:05:49.526072 12892 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0205 07:05:54.421092 12892 solver.cpp:237] Iteration 490, loss = 0.727072
I0205 07:05:54.421149 12892 solver.cpp:253]     Train net output #0: loss = 0.727072 (* 1 = 0.727072 loss)
I0205 07:05:54.421159 12892 sgd_solver.cpp:106] Iteration 490, lr = 0.001
I0205 07:05:58.823451 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_500.caffemodel
I0205 07:05:58.825443 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_500.solverstate
I0205 07:05:58.826236 12892 solver.cpp:341] Iteration 500, Testing net (#0)
I0205 07:06:01.203858 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:06:01.204051 12892 solver.cpp:409]     Test net output #1: loss = 0.705499 (* 1 = 0.705499 loss)
I0205 07:06:01.690135 12892 solver.cpp:237] Iteration 500, loss = 0.70378
I0205 07:06:01.690188 12892 solver.cpp:253]     Train net output #0: loss = 0.70378 (* 1 = 0.70378 loss)
I0205 07:06:01.690199 12892 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0205 07:06:06.554118 12892 solver.cpp:237] Iteration 510, loss = 0.722969
I0205 07:06:06.554172 12892 solver.cpp:253]     Train net output #0: loss = 0.722969 (* 1 = 0.722969 loss)
I0205 07:06:06.554183 12892 sgd_solver.cpp:106] Iteration 510, lr = 0.001
I0205 07:06:11.413702 12892 solver.cpp:237] Iteration 520, loss = 0.707393
I0205 07:06:11.413749 12892 solver.cpp:253]     Train net output #0: loss = 0.707393 (* 1 = 0.707393 loss)
I0205 07:06:11.413760 12892 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I0205 07:06:16.274142 12892 solver.cpp:237] Iteration 530, loss = 0.710999
I0205 07:06:16.274191 12892 solver.cpp:253]     Train net output #0: loss = 0.710999 (* 1 = 0.710999 loss)
I0205 07:06:16.274202 12892 sgd_solver.cpp:106] Iteration 530, lr = 0.001
I0205 07:06:21.166076 12892 solver.cpp:237] Iteration 540, loss = 0.728638
I0205 07:06:21.166132 12892 solver.cpp:253]     Train net output #0: loss = 0.728638 (* 1 = 0.728638 loss)
I0205 07:06:21.166143 12892 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I0205 07:06:26.065839 12892 solver.cpp:237] Iteration 550, loss = 0.721305
I0205 07:06:26.065888 12892 solver.cpp:253]     Train net output #0: loss = 0.721305 (* 1 = 0.721305 loss)
I0205 07:06:26.065899 12892 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I0205 07:06:30.958655 12892 solver.cpp:237] Iteration 560, loss = 0.700681
I0205 07:06:30.958704 12892 solver.cpp:253]     Train net output #0: loss = 0.700681 (* 1 = 0.700681 loss)
I0205 07:06:30.958715 12892 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I0205 07:06:35.851150 12892 solver.cpp:237] Iteration 570, loss = 0.735182
I0205 07:06:35.851348 12892 solver.cpp:253]     Train net output #0: loss = 0.735182 (* 1 = 0.735182 loss)
I0205 07:06:35.851362 12892 sgd_solver.cpp:106] Iteration 570, lr = 0.001
I0205 07:06:40.714562 12892 solver.cpp:237] Iteration 580, loss = 0.693391
I0205 07:06:40.714615 12892 solver.cpp:253]     Train net output #0: loss = 0.693391 (* 1 = 0.693391 loss)
I0205 07:06:40.714632 12892 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I0205 07:06:45.531795 12892 solver.cpp:237] Iteration 590, loss = 0.703964
I0205 07:06:45.531877 12892 solver.cpp:253]     Train net output #0: loss = 0.703964 (* 1 = 0.703964 loss)
I0205 07:06:45.531891 12892 sgd_solver.cpp:106] Iteration 590, lr = 0.001
I0205 07:06:49.863941 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_600.caffemodel
I0205 07:06:49.866150 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_600.solverstate
I0205 07:06:49.866964 12892 solver.cpp:341] Iteration 600, Testing net (#0)
I0205 07:06:52.196565 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:06:52.196638 12892 solver.cpp:409]     Test net output #1: loss = 0.69917 (* 1 = 0.69917 loss)
I0205 07:06:52.677273 12892 solver.cpp:237] Iteration 600, loss = 0.691418
I0205 07:06:52.677348 12892 solver.cpp:253]     Train net output #0: loss = 0.691418 (* 1 = 0.691418 loss)
I0205 07:06:52.677362 12892 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0205 07:06:57.490255 12892 solver.cpp:237] Iteration 610, loss = 0.692876
I0205 07:06:57.490339 12892 solver.cpp:253]     Train net output #0: loss = 0.692876 (* 1 = 0.692876 loss)
I0205 07:06:57.490355 12892 sgd_solver.cpp:106] Iteration 610, lr = 0.001
I0205 07:07:02.310292 12892 solver.cpp:237] Iteration 620, loss = 0.704859
I0205 07:07:02.310369 12892 solver.cpp:253]     Train net output #0: loss = 0.704859 (* 1 = 0.704859 loss)
I0205 07:07:02.310382 12892 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I0205 07:07:07.159127 12892 solver.cpp:237] Iteration 630, loss = 0.732521
I0205 07:07:07.159350 12892 solver.cpp:253]     Train net output #0: loss = 0.732521 (* 1 = 0.732521 loss)
I0205 07:07:07.159368 12892 sgd_solver.cpp:106] Iteration 630, lr = 0.001
I0205 07:07:12.009301 12892 solver.cpp:237] Iteration 640, loss = 0.71617
I0205 07:07:12.009394 12892 solver.cpp:253]     Train net output #0: loss = 0.71617 (* 1 = 0.71617 loss)
I0205 07:07:12.009408 12892 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I0205 07:07:16.860335 12892 solver.cpp:237] Iteration 650, loss = 0.711404
I0205 07:07:16.860414 12892 solver.cpp:253]     Train net output #0: loss = 0.711404 (* 1 = 0.711404 loss)
I0205 07:07:16.860426 12892 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I0205 07:07:21.709404 12892 solver.cpp:237] Iteration 660, loss = 0.692343
I0205 07:07:21.709481 12892 solver.cpp:253]     Train net output #0: loss = 0.692343 (* 1 = 0.692343 loss)
I0205 07:07:21.709496 12892 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I0205 07:07:26.562708 12892 solver.cpp:237] Iteration 670, loss = 0.702143
I0205 07:07:26.562788 12892 solver.cpp:253]     Train net output #0: loss = 0.702143 (* 1 = 0.702143 loss)
I0205 07:07:26.562800 12892 sgd_solver.cpp:106] Iteration 670, lr = 0.001
I0205 07:07:31.420167 12892 solver.cpp:237] Iteration 680, loss = 0.722029
I0205 07:07:31.420244 12892 solver.cpp:253]     Train net output #0: loss = 0.722029 (* 1 = 0.722029 loss)
I0205 07:07:31.420258 12892 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I0205 07:07:36.271674 12892 solver.cpp:237] Iteration 690, loss = 0.709545
I0205 07:07:36.271755 12892 solver.cpp:253]     Train net output #0: loss = 0.709545 (* 1 = 0.709545 loss)
I0205 07:07:36.271770 12892 sgd_solver.cpp:106] Iteration 690, lr = 0.001
I0205 07:07:40.644029 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_700.caffemodel
I0205 07:07:40.646383 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_700.solverstate
I0205 07:07:40.647258 12892 solver.cpp:341] Iteration 700, Testing net (#0)
I0205 07:07:42.994858 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:07:42.994937 12892 solver.cpp:409]     Test net output #1: loss = 0.695951 (* 1 = 0.695951 loss)
I0205 07:07:43.481266 12892 solver.cpp:237] Iteration 700, loss = 0.743644
I0205 07:07:43.481341 12892 solver.cpp:253]     Train net output #0: loss = 0.743644 (* 1 = 0.743644 loss)
I0205 07:07:43.481356 12892 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0205 07:07:48.335578 12892 solver.cpp:237] Iteration 710, loss = 0.697179
I0205 07:07:48.335656 12892 solver.cpp:253]     Train net output #0: loss = 0.697179 (* 1 = 0.697179 loss)
I0205 07:07:48.335670 12892 sgd_solver.cpp:106] Iteration 710, lr = 0.001
I0205 07:07:53.189396 12892 solver.cpp:237] Iteration 720, loss = 0.704222
I0205 07:07:53.189488 12892 solver.cpp:253]     Train net output #0: loss = 0.704222 (* 1 = 0.704222 loss)
I0205 07:07:53.189502 12892 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I0205 07:07:58.044391 12892 solver.cpp:237] Iteration 730, loss = 0.739333
I0205 07:07:58.044472 12892 solver.cpp:253]     Train net output #0: loss = 0.739333 (* 1 = 0.739333 loss)
I0205 07:07:58.044486 12892 sgd_solver.cpp:106] Iteration 730, lr = 0.001
I0205 07:08:02.896872 12892 solver.cpp:237] Iteration 740, loss = 0.691996
I0205 07:08:02.896949 12892 solver.cpp:253]     Train net output #0: loss = 0.691996 (* 1 = 0.691996 loss)
I0205 07:08:02.896963 12892 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I0205 07:08:07.749717 12892 solver.cpp:237] Iteration 750, loss = 0.704652
I0205 07:08:07.749802 12892 solver.cpp:253]     Train net output #0: loss = 0.704652 (* 1 = 0.704652 loss)
I0205 07:08:07.749816 12892 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I0205 07:08:12.602875 12892 solver.cpp:237] Iteration 760, loss = 0.696298
I0205 07:08:12.603113 12892 solver.cpp:253]     Train net output #0: loss = 0.696298 (* 1 = 0.696298 loss)
I0205 07:08:12.603130 12892 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I0205 07:08:17.456610 12892 solver.cpp:237] Iteration 770, loss = 0.703465
I0205 07:08:17.456696 12892 solver.cpp:253]     Train net output #0: loss = 0.703465 (* 1 = 0.703465 loss)
I0205 07:08:17.456723 12892 sgd_solver.cpp:106] Iteration 770, lr = 0.001
I0205 07:08:22.306300 12892 solver.cpp:237] Iteration 780, loss = 0.733302
I0205 07:08:22.306375 12892 solver.cpp:253]     Train net output #0: loss = 0.733302 (* 1 = 0.733302 loss)
I0205 07:08:22.306388 12892 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I0205 07:08:27.158159 12892 solver.cpp:237] Iteration 790, loss = 0.722309
I0205 07:08:27.158238 12892 solver.cpp:253]     Train net output #0: loss = 0.722309 (* 1 = 0.722309 loss)
I0205 07:08:27.158252 12892 sgd_solver.cpp:106] Iteration 790, lr = 0.001
I0205 07:08:31.522727 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_800.caffemodel
I0205 07:08:31.524868 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_800.solverstate
I0205 07:08:31.525703 12892 solver.cpp:341] Iteration 800, Testing net (#0)
I0205 07:08:33.872841 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:08:33.872910 12892 solver.cpp:409]     Test net output #1: loss = 0.696155 (* 1 = 0.696155 loss)
I0205 07:08:34.356240 12892 solver.cpp:237] Iteration 800, loss = 0.684323
I0205 07:08:34.356315 12892 solver.cpp:253]     Train net output #0: loss = 0.684323 (* 1 = 0.684323 loss)
I0205 07:08:34.356329 12892 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0205 07:08:39.206696 12892 solver.cpp:237] Iteration 810, loss = 0.753144
I0205 07:08:39.206774 12892 solver.cpp:253]     Train net output #0: loss = 0.753144 (* 1 = 0.753144 loss)
I0205 07:08:39.206789 12892 sgd_solver.cpp:106] Iteration 810, lr = 0.001
I0205 07:08:44.056937 12892 solver.cpp:237] Iteration 820, loss = 0.700547
I0205 07:08:44.057199 12892 solver.cpp:253]     Train net output #0: loss = 0.700547 (* 1 = 0.700547 loss)
I0205 07:08:44.057215 12892 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I0205 07:08:48.908429 12892 solver.cpp:237] Iteration 830, loss = 0.706521
I0205 07:08:48.908509 12892 solver.cpp:253]     Train net output #0: loss = 0.706521 (* 1 = 0.706521 loss)
I0205 07:08:48.908522 12892 sgd_solver.cpp:106] Iteration 830, lr = 0.001
I0205 07:08:53.759953 12892 solver.cpp:237] Iteration 840, loss = 0.707062
I0205 07:08:53.760037 12892 solver.cpp:253]     Train net output #0: loss = 0.707062 (* 1 = 0.707062 loss)
I0205 07:08:53.760057 12892 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I0205 07:08:58.608434 12892 solver.cpp:237] Iteration 850, loss = 0.708571
I0205 07:08:58.608515 12892 solver.cpp:253]     Train net output #0: loss = 0.708571 (* 1 = 0.708571 loss)
I0205 07:08:58.608528 12892 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I0205 07:09:03.462923 12892 solver.cpp:237] Iteration 860, loss = 0.710483
I0205 07:09:03.463002 12892 solver.cpp:253]     Train net output #0: loss = 0.710483 (* 1 = 0.710483 loss)
I0205 07:09:03.463016 12892 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I0205 07:09:08.317754 12892 solver.cpp:237] Iteration 870, loss = 0.713835
I0205 07:09:08.317837 12892 solver.cpp:253]     Train net output #0: loss = 0.713835 (* 1 = 0.713835 loss)
I0205 07:09:08.317852 12892 sgd_solver.cpp:106] Iteration 870, lr = 0.001
I0205 07:09:13.167366 12892 solver.cpp:237] Iteration 880, loss = 0.680213
I0205 07:09:13.167448 12892 solver.cpp:253]     Train net output #0: loss = 0.680213 (* 1 = 0.680213 loss)
I0205 07:09:13.167462 12892 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I0205 07:09:18.019567 12892 solver.cpp:237] Iteration 890, loss = 0.743284
I0205 07:09:18.019788 12892 solver.cpp:253]     Train net output #0: loss = 0.743284 (* 1 = 0.743284 loss)
I0205 07:09:18.019805 12892 sgd_solver.cpp:106] Iteration 890, lr = 0.001
I0205 07:09:22.385300 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_900.caffemodel
I0205 07:09:22.387413 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_900.solverstate
I0205 07:09:22.388221 12892 solver.cpp:341] Iteration 900, Testing net (#0)
I0205 07:09:24.735535 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:09:24.735610 12892 solver.cpp:409]     Test net output #1: loss = 0.704823 (* 1 = 0.704823 loss)
I0205 07:09:25.220144 12892 solver.cpp:237] Iteration 900, loss = 0.686241
I0205 07:09:25.220217 12892 solver.cpp:253]     Train net output #0: loss = 0.686241 (* 1 = 0.686241 loss)
I0205 07:09:25.220232 12892 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0205 07:09:30.073272 12892 solver.cpp:237] Iteration 910, loss = 0.72863
I0205 07:09:30.073353 12892 solver.cpp:253]     Train net output #0: loss = 0.72863 (* 1 = 0.72863 loss)
I0205 07:09:30.073366 12892 sgd_solver.cpp:106] Iteration 910, lr = 0.001
I0205 07:09:34.922673 12892 solver.cpp:237] Iteration 920, loss = 0.70615
I0205 07:09:34.922754 12892 solver.cpp:253]     Train net output #0: loss = 0.70615 (* 1 = 0.70615 loss)
I0205 07:09:34.922767 12892 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I0205 07:09:39.773847 12892 solver.cpp:237] Iteration 930, loss = 0.7179
I0205 07:09:39.773913 12892 solver.cpp:253]     Train net output #0: loss = 0.7179 (* 1 = 0.7179 loss)
I0205 07:09:39.773927 12892 sgd_solver.cpp:106] Iteration 930, lr = 0.001
I0205 07:09:44.631175 12892 solver.cpp:237] Iteration 940, loss = 0.71265
I0205 07:09:44.631254 12892 solver.cpp:253]     Train net output #0: loss = 0.71265 (* 1 = 0.71265 loss)
I0205 07:09:44.631268 12892 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I0205 07:09:49.484809 12892 solver.cpp:237] Iteration 950, loss = 0.691605
I0205 07:09:49.485080 12892 solver.cpp:253]     Train net output #0: loss = 0.691605 (* 1 = 0.691605 loss)
I0205 07:09:49.485097 12892 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I0205 07:09:54.341096 12892 solver.cpp:237] Iteration 960, loss = 0.667612
I0205 07:09:54.341178 12892 solver.cpp:253]     Train net output #0: loss = 0.667612 (* 1 = 0.667612 loss)
I0205 07:09:54.341192 12892 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I0205 07:09:59.192359 12892 solver.cpp:237] Iteration 970, loss = 0.711917
I0205 07:09:59.192457 12892 solver.cpp:253]     Train net output #0: loss = 0.711917 (* 1 = 0.711917 loss)
I0205 07:09:59.192472 12892 sgd_solver.cpp:106] Iteration 970, lr = 0.001
I0205 07:10:04.050290 12892 solver.cpp:237] Iteration 980, loss = 0.712198
I0205 07:10:04.050370 12892 solver.cpp:253]     Train net output #0: loss = 0.712198 (* 1 = 0.712198 loss)
I0205 07:10:04.050384 12892 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I0205 07:10:08.900692 12892 solver.cpp:237] Iteration 990, loss = 0.711392
I0205 07:10:08.900769 12892 solver.cpp:253]     Train net output #0: loss = 0.711392 (* 1 = 0.711392 loss)
I0205 07:10:08.900782 12892 sgd_solver.cpp:106] Iteration 990, lr = 0.001
I0205 07:10:13.263651 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1000.caffemodel
I0205 07:10:13.265794 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1000.solverstate
I0205 07:10:13.266598 12892 solver.cpp:341] Iteration 1000, Testing net (#0)
I0205 07:10:15.613592 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:10:15.613667 12892 solver.cpp:409]     Test net output #1: loss = 0.694832 (* 1 = 0.694832 loss)
I0205 07:10:16.103639 12892 solver.cpp:237] Iteration 1000, loss = 0.683069
I0205 07:10:16.103715 12892 solver.cpp:253]     Train net output #0: loss = 0.683069 (* 1 = 0.683069 loss)
I0205 07:10:16.103731 12892 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0205 07:10:20.955066 12892 solver.cpp:237] Iteration 1010, loss = 0.694926
I0205 07:10:20.955338 12892 solver.cpp:253]     Train net output #0: loss = 0.694926 (* 1 = 0.694926 loss)
I0205 07:10:20.955354 12892 sgd_solver.cpp:106] Iteration 1010, lr = 0.001
I0205 07:10:25.809819 12892 solver.cpp:237] Iteration 1020, loss = 0.743244
I0205 07:10:25.809898 12892 solver.cpp:253]     Train net output #0: loss = 0.743244 (* 1 = 0.743244 loss)
I0205 07:10:25.809912 12892 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I0205 07:10:30.663941 12892 solver.cpp:237] Iteration 1030, loss = 0.705106
I0205 07:10:30.664021 12892 solver.cpp:253]     Train net output #0: loss = 0.705106 (* 1 = 0.705106 loss)
I0205 07:10:30.664036 12892 sgd_solver.cpp:106] Iteration 1030, lr = 0.001
I0205 07:10:35.513149 12892 solver.cpp:237] Iteration 1040, loss = 0.687295
I0205 07:10:35.513250 12892 solver.cpp:253]     Train net output #0: loss = 0.687295 (* 1 = 0.687295 loss)
I0205 07:10:35.513265 12892 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I0205 07:10:40.362860 12892 solver.cpp:237] Iteration 1050, loss = 0.729714
I0205 07:10:40.362926 12892 solver.cpp:253]     Train net output #0: loss = 0.729714 (* 1 = 0.729714 loss)
I0205 07:10:40.362941 12892 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I0205 07:10:45.214859 12892 solver.cpp:237] Iteration 1060, loss = 0.673602
I0205 07:10:45.214947 12892 solver.cpp:253]     Train net output #0: loss = 0.673602 (* 1 = 0.673602 loss)
I0205 07:10:45.214962 12892 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I0205 07:10:50.065469 12892 solver.cpp:237] Iteration 1070, loss = 0.699767
I0205 07:10:50.065546 12892 solver.cpp:253]     Train net output #0: loss = 0.699767 (* 1 = 0.699767 loss)
I0205 07:10:50.065559 12892 sgd_solver.cpp:106] Iteration 1070, lr = 0.001
I0205 07:10:54.939172 12892 solver.cpp:237] Iteration 1080, loss = 0.705778
I0205 07:10:54.939370 12892 solver.cpp:253]     Train net output #0: loss = 0.705778 (* 1 = 0.705778 loss)
I0205 07:10:54.939384 12892 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I0205 07:10:59.790151 12892 solver.cpp:237] Iteration 1090, loss = 0.694726
I0205 07:10:59.790210 12892 solver.cpp:253]     Train net output #0: loss = 0.694726 (* 1 = 0.694726 loss)
I0205 07:10:59.790220 12892 sgd_solver.cpp:106] Iteration 1090, lr = 0.001
I0205 07:11:04.166467 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1100.caffemodel
I0205 07:11:04.168457 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1100.solverstate
I0205 07:11:04.169257 12892 solver.cpp:341] Iteration 1100, Testing net (#0)
I0205 07:11:06.530719 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:11:06.530769 12892 solver.cpp:409]     Test net output #1: loss = 0.693958 (* 1 = 0.693958 loss)
I0205 07:11:07.015717 12892 solver.cpp:237] Iteration 1100, loss = 0.735748
I0205 07:11:07.015768 12892 solver.cpp:253]     Train net output #0: loss = 0.735748 (* 1 = 0.735748 loss)
I0205 07:11:07.015779 12892 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I0205 07:11:11.873486 12892 solver.cpp:237] Iteration 1110, loss = 0.686222
I0205 07:11:11.873541 12892 solver.cpp:253]     Train net output #0: loss = 0.686222 (* 1 = 0.686222 loss)
I0205 07:11:11.873553 12892 sgd_solver.cpp:106] Iteration 1110, lr = 0.001
I0205 07:11:16.728584 12892 solver.cpp:237] Iteration 1120, loss = 0.706529
I0205 07:11:16.728636 12892 solver.cpp:253]     Train net output #0: loss = 0.706529 (* 1 = 0.706529 loss)
I0205 07:11:16.728646 12892 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I0205 07:11:21.583991 12892 solver.cpp:237] Iteration 1130, loss = 0.736878
I0205 07:11:21.584048 12892 solver.cpp:253]     Train net output #0: loss = 0.736878 (* 1 = 0.736878 loss)
I0205 07:11:21.584060 12892 sgd_solver.cpp:106] Iteration 1130, lr = 0.001
I0205 07:11:26.438477 12892 solver.cpp:237] Iteration 1140, loss = 0.692158
I0205 07:11:26.438685 12892 solver.cpp:253]     Train net output #0: loss = 0.692158 (* 1 = 0.692158 loss)
I0205 07:11:26.438704 12892 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I0205 07:11:31.294311 12892 solver.cpp:237] Iteration 1150, loss = 0.684475
I0205 07:11:31.294364 12892 solver.cpp:253]     Train net output #0: loss = 0.684475 (* 1 = 0.684475 loss)
I0205 07:11:31.294375 12892 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I0205 07:11:36.149323 12892 solver.cpp:237] Iteration 1160, loss = 0.719306
I0205 07:11:36.149375 12892 solver.cpp:253]     Train net output #0: loss = 0.719306 (* 1 = 0.719306 loss)
I0205 07:11:36.149386 12892 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I0205 07:11:41.004603 12892 solver.cpp:237] Iteration 1170, loss = 0.707317
I0205 07:11:41.004658 12892 solver.cpp:253]     Train net output #0: loss = 0.707317 (* 1 = 0.707317 loss)
I0205 07:11:41.004668 12892 sgd_solver.cpp:106] Iteration 1170, lr = 0.001
I0205 07:11:45.861114 12892 solver.cpp:237] Iteration 1180, loss = 0.702883
I0205 07:11:45.861169 12892 solver.cpp:253]     Train net output #0: loss = 0.702883 (* 1 = 0.702883 loss)
I0205 07:11:45.861181 12892 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I0205 07:11:50.715262 12892 solver.cpp:237] Iteration 1190, loss = 0.713603
I0205 07:11:50.715314 12892 solver.cpp:253]     Train net output #0: loss = 0.713603 (* 1 = 0.713603 loss)
I0205 07:11:50.715325 12892 sgd_solver.cpp:106] Iteration 1190, lr = 0.001
I0205 07:11:55.087223 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1200.caffemodel
I0205 07:11:55.089210 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1200.solverstate
I0205 07:11:55.090005 12892 solver.cpp:341] Iteration 1200, Testing net (#0)
I0205 07:11:57.454378 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:11:57.454562 12892 solver.cpp:409]     Test net output #1: loss = 0.694683 (* 1 = 0.694683 loss)
I0205 07:11:57.939553 12892 solver.cpp:237] Iteration 1200, loss = 0.707788
I0205 07:11:57.939604 12892 solver.cpp:253]     Train net output #0: loss = 0.707788 (* 1 = 0.707788 loss)
I0205 07:11:57.939615 12892 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0205 07:12:02.811835 12892 solver.cpp:237] Iteration 1210, loss = 0.727415
I0205 07:12:02.811888 12892 solver.cpp:253]     Train net output #0: loss = 0.727415 (* 1 = 0.727415 loss)
I0205 07:12:02.811899 12892 sgd_solver.cpp:106] Iteration 1210, lr = 0.001
I0205 07:12:07.683437 12892 solver.cpp:237] Iteration 1220, loss = 0.696516
I0205 07:12:07.683519 12892 solver.cpp:253]     Train net output #0: loss = 0.696516 (* 1 = 0.696516 loss)
I0205 07:12:07.683533 12892 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I0205 07:12:12.535465 12892 solver.cpp:237] Iteration 1230, loss = 0.704152
I0205 07:12:12.535547 12892 solver.cpp:253]     Train net output #0: loss = 0.704152 (* 1 = 0.704152 loss)
I0205 07:12:12.535562 12892 sgd_solver.cpp:106] Iteration 1230, lr = 0.001
I0205 07:12:17.387740 12892 solver.cpp:237] Iteration 1240, loss = 0.689036
I0205 07:12:17.387815 12892 solver.cpp:253]     Train net output #0: loss = 0.689036 (* 1 = 0.689036 loss)
I0205 07:12:17.387830 12892 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I0205 07:12:22.241683 12892 solver.cpp:237] Iteration 1250, loss = 0.708342
I0205 07:12:22.241761 12892 solver.cpp:253]     Train net output #0: loss = 0.708342 (* 1 = 0.708342 loss)
I0205 07:12:22.241776 12892 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I0205 07:12:27.096042 12892 solver.cpp:237] Iteration 1260, loss = 0.705273
I0205 07:12:27.096127 12892 solver.cpp:253]     Train net output #0: loss = 0.705273 (* 1 = 0.705273 loss)
I0205 07:12:27.096141 12892 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I0205 07:12:31.952366 12892 solver.cpp:237] Iteration 1270, loss = 0.688034
I0205 07:12:31.952635 12892 solver.cpp:253]     Train net output #0: loss = 0.688034 (* 1 = 0.688034 loss)
I0205 07:12:31.952656 12892 sgd_solver.cpp:106] Iteration 1270, lr = 0.001
I0205 07:12:36.808980 12892 solver.cpp:237] Iteration 1280, loss = 0.699565
I0205 07:12:36.809072 12892 solver.cpp:253]     Train net output #0: loss = 0.699565 (* 1 = 0.699565 loss)
I0205 07:12:36.809093 12892 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I0205 07:12:41.667316 12892 solver.cpp:237] Iteration 1290, loss = 0.713744
I0205 07:12:41.667392 12892 solver.cpp:253]     Train net output #0: loss = 0.713744 (* 1 = 0.713744 loss)
I0205 07:12:41.667404 12892 sgd_solver.cpp:106] Iteration 1290, lr = 0.001
I0205 07:12:46.038805 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1300.caffemodel
I0205 07:12:46.040959 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1300.solverstate
I0205 07:12:46.041761 12892 solver.cpp:341] Iteration 1300, Testing net (#0)
I0205 07:12:48.392462 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:12:48.392526 12892 solver.cpp:409]     Test net output #1: loss = 0.699606 (* 1 = 0.699606 loss)
I0205 07:12:48.877102 12892 solver.cpp:237] Iteration 1300, loss = 0.686627
I0205 07:12:48.877167 12892 solver.cpp:253]     Train net output #0: loss = 0.686627 (* 1 = 0.686627 loss)
I0205 07:12:48.877182 12892 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I0205 07:12:53.731353 12892 solver.cpp:237] Iteration 1310, loss = 0.693194
I0205 07:12:53.731436 12892 solver.cpp:253]     Train net output #0: loss = 0.693194 (* 1 = 0.693194 loss)
I0205 07:12:53.731449 12892 sgd_solver.cpp:106] Iteration 1310, lr = 0.001
I0205 07:12:58.592094 12892 solver.cpp:237] Iteration 1320, loss = 0.717293
I0205 07:12:58.592169 12892 solver.cpp:253]     Train net output #0: loss = 0.717293 (* 1 = 0.717293 loss)
I0205 07:12:58.592182 12892 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I0205 07:13:03.448389 12892 solver.cpp:237] Iteration 1330, loss = 0.69743
I0205 07:13:03.448607 12892 solver.cpp:253]     Train net output #0: loss = 0.69743 (* 1 = 0.69743 loss)
I0205 07:13:03.448624 12892 sgd_solver.cpp:106] Iteration 1330, lr = 0.001
I0205 07:13:08.305443 12892 solver.cpp:237] Iteration 1340, loss = 0.715657
I0205 07:13:08.305531 12892 solver.cpp:253]     Train net output #0: loss = 0.715657 (* 1 = 0.715657 loss)
I0205 07:13:08.305546 12892 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I0205 07:13:13.155292 12892 solver.cpp:237] Iteration 1350, loss = 0.709262
I0205 07:13:13.155374 12892 solver.cpp:253]     Train net output #0: loss = 0.709262 (* 1 = 0.709262 loss)
I0205 07:13:13.155387 12892 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I0205 07:13:18.008038 12892 solver.cpp:237] Iteration 1360, loss = 0.687969
I0205 07:13:18.008118 12892 solver.cpp:253]     Train net output #0: loss = 0.687969 (* 1 = 0.687969 loss)
I0205 07:13:18.008133 12892 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I0205 07:13:22.860551 12892 solver.cpp:237] Iteration 1370, loss = 0.72408
I0205 07:13:22.860621 12892 solver.cpp:253]     Train net output #0: loss = 0.72408 (* 1 = 0.72408 loss)
I0205 07:13:22.860635 12892 sgd_solver.cpp:106] Iteration 1370, lr = 0.001
I0205 07:13:27.711560 12892 solver.cpp:237] Iteration 1380, loss = 0.705693
I0205 07:13:27.711637 12892 solver.cpp:253]     Train net output #0: loss = 0.705693 (* 1 = 0.705693 loss)
I0205 07:13:27.711652 12892 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I0205 07:13:32.565086 12892 solver.cpp:237] Iteration 1390, loss = 0.709006
I0205 07:13:32.565171 12892 solver.cpp:253]     Train net output #0: loss = 0.709006 (* 1 = 0.709006 loss)
I0205 07:13:32.565186 12892 sgd_solver.cpp:106] Iteration 1390, lr = 0.001
I0205 07:13:36.929394 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1400.caffemodel
I0205 07:13:36.931772 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1400.solverstate
I0205 07:13:36.932621 12892 solver.cpp:341] Iteration 1400, Testing net (#0)
I0205 07:13:39.279551 12892 solver.cpp:409]     Test net output #0: accuracy = 0.501
I0205 07:13:39.279623 12892 solver.cpp:409]     Test net output #1: loss = 0.693053 (* 1 = 0.693053 loss)
I0205 07:13:39.764206 12892 solver.cpp:237] Iteration 1400, loss = 0.704125
I0205 07:13:39.764279 12892 solver.cpp:253]     Train net output #0: loss = 0.704125 (* 1 = 0.704125 loss)
I0205 07:13:39.764293 12892 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0205 07:13:44.616264 12892 solver.cpp:237] Iteration 1410, loss = 0.696582
I0205 07:13:44.616343 12892 solver.cpp:253]     Train net output #0: loss = 0.696582 (* 1 = 0.696582 loss)
I0205 07:13:44.616358 12892 sgd_solver.cpp:106] Iteration 1410, lr = 0.001
I0205 07:13:49.466790 12892 solver.cpp:237] Iteration 1420, loss = 0.703336
I0205 07:13:49.466871 12892 solver.cpp:253]     Train net output #0: loss = 0.703336 (* 1 = 0.703336 loss)
I0205 07:13:49.466886 12892 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I0205 07:13:54.316061 12892 solver.cpp:237] Iteration 1430, loss = 0.714672
I0205 07:13:54.316138 12892 solver.cpp:253]     Train net output #0: loss = 0.714672 (* 1 = 0.714672 loss)
I0205 07:13:54.316154 12892 sgd_solver.cpp:106] Iteration 1430, lr = 0.001
I0205 07:13:59.168323 12892 solver.cpp:237] Iteration 1440, loss = 0.695005
I0205 07:13:59.488742 12892 solver.cpp:253]     Train net output #0: loss = 0.695005 (* 1 = 0.695005 loss)
I0205 07:13:59.488788 12892 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I0205 07:14:04.349961 12892 solver.cpp:237] Iteration 1450, loss = 0.725781
I0205 07:14:04.350044 12892 solver.cpp:253]     Train net output #0: loss = 0.725781 (* 1 = 0.725781 loss)
I0205 07:14:04.350067 12892 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I0205 07:14:09.197798 12892 solver.cpp:237] Iteration 1460, loss = 0.6843
I0205 07:14:09.198030 12892 solver.cpp:253]     Train net output #0: loss = 0.6843 (* 1 = 0.6843 loss)
I0205 07:14:09.198047 12892 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I0205 07:14:14.034514 12892 solver.cpp:237] Iteration 1470, loss = 0.695254
I0205 07:14:14.034598 12892 solver.cpp:253]     Train net output #0: loss = 0.695254 (* 1 = 0.695254 loss)
I0205 07:14:14.034612 12892 sgd_solver.cpp:106] Iteration 1470, lr = 0.001
I0205 07:14:18.850528 12892 solver.cpp:237] Iteration 1480, loss = 0.681704
I0205 07:14:18.850615 12892 solver.cpp:253]     Train net output #0: loss = 0.681704 (* 1 = 0.681704 loss)
I0205 07:14:18.850628 12892 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I0205 07:14:23.664253 12892 solver.cpp:237] Iteration 1490, loss = 0.695195
I0205 07:14:23.664335 12892 solver.cpp:253]     Train net output #0: loss = 0.695195 (* 1 = 0.695195 loss)
I0205 07:14:23.664348 12892 sgd_solver.cpp:106] Iteration 1490, lr = 0.001
I0205 07:14:27.995770 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1500.caffemodel
I0205 07:14:27.997884 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1500.solverstate
I0205 07:14:27.998685 12892 solver.cpp:341] Iteration 1500, Testing net (#0)
I0205 07:14:30.330730 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:14:30.330802 12892 solver.cpp:409]     Test net output #1: loss = 0.694492 (* 1 = 0.694492 loss)
I0205 07:14:30.811880 12892 solver.cpp:237] Iteration 1500, loss = 0.712345
I0205 07:14:30.811954 12892 solver.cpp:253]     Train net output #0: loss = 0.712345 (* 1 = 0.712345 loss)
I0205 07:14:30.811969 12892 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I0205 07:14:35.626690 12892 solver.cpp:237] Iteration 1510, loss = 0.696538
I0205 07:14:35.626786 12892 solver.cpp:253]     Train net output #0: loss = 0.696538 (* 1 = 0.696538 loss)
I0205 07:14:35.626801 12892 sgd_solver.cpp:106] Iteration 1510, lr = 0.001
I0205 07:14:40.440701 12892 solver.cpp:237] Iteration 1520, loss = 0.680406
I0205 07:14:40.440949 12892 solver.cpp:253]     Train net output #0: loss = 0.680406 (* 1 = 0.680406 loss)
I0205 07:14:40.440966 12892 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I0205 07:14:45.255486 12892 solver.cpp:237] Iteration 1530, loss = 0.731623
I0205 07:14:45.255568 12892 solver.cpp:253]     Train net output #0: loss = 0.731623 (* 1 = 0.731623 loss)
I0205 07:14:45.255584 12892 sgd_solver.cpp:106] Iteration 1530, lr = 0.001
I0205 07:14:50.071131 12892 solver.cpp:237] Iteration 1540, loss = 0.694229
I0205 07:14:50.071213 12892 solver.cpp:253]     Train net output #0: loss = 0.694229 (* 1 = 0.694229 loss)
I0205 07:14:50.071226 12892 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I0205 07:14:54.884300 12892 solver.cpp:237] Iteration 1550, loss = 0.694792
I0205 07:14:54.884377 12892 solver.cpp:253]     Train net output #0: loss = 0.694792 (* 1 = 0.694792 loss)
I0205 07:14:54.884392 12892 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I0205 07:14:59.698930 12892 solver.cpp:237] Iteration 1560, loss = 0.698374
I0205 07:14:59.699012 12892 solver.cpp:253]     Train net output #0: loss = 0.698374 (* 1 = 0.698374 loss)
I0205 07:14:59.699025 12892 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I0205 07:15:04.512384 12892 solver.cpp:237] Iteration 1570, loss = 0.693447
I0205 07:15:04.512467 12892 solver.cpp:253]     Train net output #0: loss = 0.693447 (* 1 = 0.693447 loss)
I0205 07:15:04.512480 12892 sgd_solver.cpp:106] Iteration 1570, lr = 0.001
I0205 07:15:09.328024 12892 solver.cpp:237] Iteration 1580, loss = 0.724378
I0205 07:15:09.328111 12892 solver.cpp:253]     Train net output #0: loss = 0.724378 (* 1 = 0.724378 loss)
I0205 07:15:09.328125 12892 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I0205 07:15:14.146132 12892 solver.cpp:237] Iteration 1590, loss = 0.704811
I0205 07:15:14.146365 12892 solver.cpp:253]     Train net output #0: loss = 0.704811 (* 1 = 0.704811 loss)
I0205 07:15:14.146383 12892 sgd_solver.cpp:106] Iteration 1590, lr = 0.001
I0205 07:15:18.479235 12892 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1600.caffemodel
I0205 07:15:18.481366 12892 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num8_conv2num8_conv3num8_conv4num8_conv5num8_seed24/snaps/snap__iter_1600.solverstate
I0205 07:15:18.715598 12892 solver.cpp:321] Iteration 1600, loss = 0.690376
I0205 07:15:18.715667 12892 solver.cpp:341] Iteration 1600, Testing net (#0)
I0205 07:15:21.057257 12892 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 07:15:21.057334 12892 solver.cpp:409]     Test net output #1: loss = 0.694988 (* 1 = 0.694988 loss)
I0205 07:15:21.057344 12892 solver.cpp:326] Optimization Done.
I0205 07:15:21.057350 12892 caffe.cpp:215] Optimization Done.
