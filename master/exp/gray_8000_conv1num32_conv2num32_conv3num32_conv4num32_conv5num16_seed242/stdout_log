I0205 04:37:44.408677 12230 caffe.cpp:177] Use CPU.
I0205 04:37:44.409592 12230 solver.cpp:48] Initializing solver from parameters: 
test_iter: 10
test_interval: 100
base_lr: 0.001
display: 10
max_iter: 1600
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 100
snapshot_prefix: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap_"
solver_mode: CPU
random_seed: 242
net: "/home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/train_val.prototxt"
I0205 04:37:44.409765 12230 solver.cpp:91] Creating training net from net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/train_val.prototxt
I0205 04:37:44.410394 12230 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0205 04:37:44.410430 12230 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0205 04:37:44.410681 12230 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 04:37:44.410830 12230 layer_factory.hpp:77] Creating layer data
I0205 04:37:44.411034 12230 net.cpp:106] Creating Layer data
I0205 04:37:44.411056 12230 net.cpp:411] data -> data
I0205 04:37:44.411165 12230 net.cpp:411] data -> label
I0205 04:37:44.411193 12230 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db/mean_file.binaryproto
I0205 04:37:44.411326 12231 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_train_lum_db
I0205 04:37:44.412345 12230 data_layer.cpp:41] output data size: 100,1,227,227
I0205 04:37:44.443754 12230 net.cpp:150] Setting up data
I0205 04:37:44.443830 12230 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 04:37:44.443840 12230 net.cpp:157] Top shape: 100 (100)
I0205 04:37:44.443846 12230 net.cpp:165] Memory required for data: 20612000
I0205 04:37:44.443866 12230 layer_factory.hpp:77] Creating layer conv1
I0205 04:37:44.443892 12230 net.cpp:106] Creating Layer conv1
I0205 04:37:44.443902 12230 net.cpp:454] conv1 <- data
I0205 04:37:44.443922 12230 net.cpp:411] conv1 -> conv1
I0205 04:37:44.444053 12230 net.cpp:150] Setting up conv1
I0205 04:37:44.444084 12230 net.cpp:157] Top shape: 100 32 55 55 (9680000)
I0205 04:37:44.444097 12230 net.cpp:165] Memory required for data: 59332000
I0205 04:37:44.444115 12230 layer_factory.hpp:77] Creating layer relu1
I0205 04:37:44.444128 12230 net.cpp:106] Creating Layer relu1
I0205 04:37:44.444134 12230 net.cpp:454] relu1 <- conv1
I0205 04:37:44.444144 12230 net.cpp:397] relu1 -> conv1 (in-place)
I0205 04:37:44.444159 12230 net.cpp:150] Setting up relu1
I0205 04:37:44.444167 12230 net.cpp:157] Top shape: 100 32 55 55 (9680000)
I0205 04:37:44.444174 12230 net.cpp:165] Memory required for data: 98052000
I0205 04:37:44.444180 12230 layer_factory.hpp:77] Creating layer pool1
I0205 04:37:44.444195 12230 net.cpp:106] Creating Layer pool1
I0205 04:37:44.444201 12230 net.cpp:454] pool1 <- conv1
I0205 04:37:44.444210 12230 net.cpp:411] pool1 -> pool1
I0205 04:37:44.444242 12230 net.cpp:150] Setting up pool1
I0205 04:37:44.444250 12230 net.cpp:157] Top shape: 100 32 27 27 (2332800)
I0205 04:37:44.444255 12230 net.cpp:165] Memory required for data: 107383200
I0205 04:37:44.444262 12230 layer_factory.hpp:77] Creating layer norm1
I0205 04:37:44.444290 12230 net.cpp:106] Creating Layer norm1
I0205 04:37:44.444296 12230 net.cpp:454] norm1 <- pool1
I0205 04:37:44.444306 12230 net.cpp:411] norm1 -> norm1
I0205 04:37:44.444325 12230 net.cpp:150] Setting up norm1
I0205 04:37:44.444335 12230 net.cpp:157] Top shape: 100 32 27 27 (2332800)
I0205 04:37:44.444340 12230 net.cpp:165] Memory required for data: 116714400
I0205 04:37:44.444346 12230 layer_factory.hpp:77] Creating layer conv2
I0205 04:37:44.444360 12230 net.cpp:106] Creating Layer conv2
I0205 04:37:44.444367 12230 net.cpp:454] conv2 <- norm1
I0205 04:37:44.444377 12230 net.cpp:411] conv2 -> conv2
I0205 04:37:44.444506 12230 net.cpp:150] Setting up conv2
I0205 04:37:44.444515 12230 net.cpp:157] Top shape: 100 32 27 27 (2332800)
I0205 04:37:44.444522 12230 net.cpp:165] Memory required for data: 126045600
I0205 04:37:44.444533 12230 layer_factory.hpp:77] Creating layer relu2
I0205 04:37:44.444543 12230 net.cpp:106] Creating Layer relu2
I0205 04:37:44.444550 12230 net.cpp:454] relu2 <- conv2
I0205 04:37:44.444567 12230 net.cpp:397] relu2 -> conv2 (in-place)
I0205 04:37:44.444579 12230 net.cpp:150] Setting up relu2
I0205 04:37:44.444586 12230 net.cpp:157] Top shape: 100 32 27 27 (2332800)
I0205 04:37:44.444592 12230 net.cpp:165] Memory required for data: 135376800
I0205 04:37:44.444598 12230 layer_factory.hpp:77] Creating layer pool2
I0205 04:37:44.444607 12230 net.cpp:106] Creating Layer pool2
I0205 04:37:44.444612 12230 net.cpp:454] pool2 <- conv2
I0205 04:37:44.444622 12230 net.cpp:411] pool2 -> pool2
I0205 04:37:44.444635 12230 net.cpp:150] Setting up pool2
I0205 04:37:44.444643 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.444648 12230 net.cpp:165] Memory required for data: 137540000
I0205 04:37:44.444654 12230 layer_factory.hpp:77] Creating layer norm2
I0205 04:37:44.444664 12230 net.cpp:106] Creating Layer norm2
I0205 04:37:44.444670 12230 net.cpp:454] norm2 <- pool2
I0205 04:37:44.444680 12230 net.cpp:411] norm2 -> norm2
I0205 04:37:44.444691 12230 net.cpp:150] Setting up norm2
I0205 04:37:44.444699 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.444705 12230 net.cpp:165] Memory required for data: 139703200
I0205 04:37:44.444710 12230 layer_factory.hpp:77] Creating layer conv3
I0205 04:37:44.444718 12230 net.cpp:106] Creating Layer conv3
I0205 04:37:44.444725 12230 net.cpp:454] conv3 <- norm2
I0205 04:37:44.444732 12230 net.cpp:411] conv3 -> conv3
I0205 04:37:44.444861 12230 net.cpp:150] Setting up conv3
I0205 04:37:44.444874 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.444880 12230 net.cpp:165] Memory required for data: 141866400
I0205 04:37:44.444890 12230 layer_factory.hpp:77] Creating layer relu3
I0205 04:37:44.444905 12230 net.cpp:106] Creating Layer relu3
I0205 04:37:44.444913 12230 net.cpp:454] relu3 <- conv3
I0205 04:37:44.444921 12230 net.cpp:397] relu3 -> conv3 (in-place)
I0205 04:37:44.444931 12230 net.cpp:150] Setting up relu3
I0205 04:37:44.444937 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.444942 12230 net.cpp:165] Memory required for data: 144029600
I0205 04:37:44.444948 12230 layer_factory.hpp:77] Creating layer conv4
I0205 04:37:44.444962 12230 net.cpp:106] Creating Layer conv4
I0205 04:37:44.444968 12230 net.cpp:454] conv4 <- conv3
I0205 04:37:44.444978 12230 net.cpp:411] conv4 -> conv4
I0205 04:37:44.445051 12230 net.cpp:150] Setting up conv4
I0205 04:37:44.445060 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.445065 12230 net.cpp:165] Memory required for data: 146192800
I0205 04:37:44.445076 12230 layer_factory.hpp:77] Creating layer relu4
I0205 04:37:44.445085 12230 net.cpp:106] Creating Layer relu4
I0205 04:37:44.445106 12230 net.cpp:454] relu4 <- conv4
I0205 04:37:44.445113 12230 net.cpp:397] relu4 -> conv4 (in-place)
I0205 04:37:44.445123 12230 net.cpp:150] Setting up relu4
I0205 04:37:44.445130 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.445137 12230 net.cpp:165] Memory required for data: 148356000
I0205 04:37:44.445147 12230 layer_factory.hpp:77] Creating layer conv5
I0205 04:37:44.445168 12230 net.cpp:106] Creating Layer conv5
I0205 04:37:44.445173 12230 net.cpp:454] conv5 <- conv4
I0205 04:37:44.445181 12230 net.cpp:411] conv5 -> conv5
I0205 04:37:44.445235 12230 net.cpp:150] Setting up conv5
I0205 04:37:44.445243 12230 net.cpp:157] Top shape: 100 16 13 13 (270400)
I0205 04:37:44.445248 12230 net.cpp:165] Memory required for data: 149437600
I0205 04:37:44.445261 12230 layer_factory.hpp:77] Creating layer relu5
I0205 04:37:44.445268 12230 net.cpp:106] Creating Layer relu5
I0205 04:37:44.445273 12230 net.cpp:454] relu5 <- conv5
I0205 04:37:44.445284 12230 net.cpp:397] relu5 -> conv5 (in-place)
I0205 04:37:44.445292 12230 net.cpp:150] Setting up relu5
I0205 04:37:44.445299 12230 net.cpp:157] Top shape: 100 16 13 13 (270400)
I0205 04:37:44.445305 12230 net.cpp:165] Memory required for data: 150519200
I0205 04:37:44.445310 12230 layer_factory.hpp:77] Creating layer pool5
I0205 04:37:44.445318 12230 net.cpp:106] Creating Layer pool5
I0205 04:37:44.445324 12230 net.cpp:454] pool5 <- conv5
I0205 04:37:44.445335 12230 net.cpp:411] pool5 -> pool5
I0205 04:37:44.445348 12230 net.cpp:150] Setting up pool5
I0205 04:37:44.445358 12230 net.cpp:157] Top shape: 100 16 6 6 (57600)
I0205 04:37:44.445363 12230 net.cpp:165] Memory required for data: 150749600
I0205 04:37:44.445368 12230 layer_factory.hpp:77] Creating layer fc6
I0205 04:37:44.445384 12230 net.cpp:106] Creating Layer fc6
I0205 04:37:44.445389 12230 net.cpp:454] fc6 <- pool5
I0205 04:37:44.445401 12230 net.cpp:411] fc6 -> fc6
I0205 04:37:44.447013 12230 net.cpp:150] Setting up fc6
I0205 04:37:44.447028 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.447033 12230 net.cpp:165] Memory required for data: 150852000
I0205 04:37:44.447042 12230 layer_factory.hpp:77] Creating layer relu6
I0205 04:37:44.447053 12230 net.cpp:106] Creating Layer relu6
I0205 04:37:44.447060 12230 net.cpp:454] relu6 <- fc6
I0205 04:37:44.447067 12230 net.cpp:397] relu6 -> fc6 (in-place)
I0205 04:37:44.447078 12230 net.cpp:150] Setting up relu6
I0205 04:37:44.447085 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.447096 12230 net.cpp:165] Memory required for data: 150954400
I0205 04:37:44.447103 12230 layer_factory.hpp:77] Creating layer drop6
I0205 04:37:44.447113 12230 net.cpp:106] Creating Layer drop6
I0205 04:37:44.447119 12230 net.cpp:454] drop6 <- fc6
I0205 04:37:44.447126 12230 net.cpp:397] drop6 -> fc6 (in-place)
I0205 04:37:44.447146 12230 net.cpp:150] Setting up drop6
I0205 04:37:44.447154 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.447160 12230 net.cpp:165] Memory required for data: 151056800
I0205 04:37:44.447165 12230 layer_factory.hpp:77] Creating layer fc7
I0205 04:37:44.447175 12230 net.cpp:106] Creating Layer fc7
I0205 04:37:44.447181 12230 net.cpp:454] fc7 <- fc6
I0205 04:37:44.447192 12230 net.cpp:411] fc7 -> fc7
I0205 04:37:44.447908 12230 net.cpp:150] Setting up fc7
I0205 04:37:44.447921 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.447926 12230 net.cpp:165] Memory required for data: 151159200
I0205 04:37:44.447934 12230 layer_factory.hpp:77] Creating layer relu7
I0205 04:37:44.447942 12230 net.cpp:106] Creating Layer relu7
I0205 04:37:44.447953 12230 net.cpp:454] relu7 <- fc7
I0205 04:37:44.447963 12230 net.cpp:397] relu7 -> fc7 (in-place)
I0205 04:37:44.447973 12230 net.cpp:150] Setting up relu7
I0205 04:37:44.447980 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.447985 12230 net.cpp:165] Memory required for data: 151261600
I0205 04:37:44.447991 12230 layer_factory.hpp:77] Creating layer drop7
I0205 04:37:44.448001 12230 net.cpp:106] Creating Layer drop7
I0205 04:37:44.448009 12230 net.cpp:454] drop7 <- fc7
I0205 04:37:44.448019 12230 net.cpp:397] drop7 -> fc7 (in-place)
I0205 04:37:44.448029 12230 net.cpp:150] Setting up drop7
I0205 04:37:44.448036 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.448041 12230 net.cpp:165] Memory required for data: 151364000
I0205 04:37:44.448047 12230 layer_factory.hpp:77] Creating layer fc8
I0205 04:37:44.448060 12230 net.cpp:106] Creating Layer fc8
I0205 04:37:44.448073 12230 net.cpp:454] fc8 <- fc7
I0205 04:37:44.448084 12230 net.cpp:411] fc8 -> fc8
I0205 04:37:44.448115 12230 net.cpp:150] Setting up fc8
I0205 04:37:44.448124 12230 net.cpp:157] Top shape: 100 2 (200)
I0205 04:37:44.448129 12230 net.cpp:165] Memory required for data: 151364800
I0205 04:37:44.448143 12230 layer_factory.hpp:77] Creating layer loss
I0205 04:37:44.448153 12230 net.cpp:106] Creating Layer loss
I0205 04:37:44.448160 12230 net.cpp:454] loss <- fc8
I0205 04:37:44.448166 12230 net.cpp:454] loss <- label
I0205 04:37:44.448176 12230 net.cpp:411] loss -> loss
I0205 04:37:44.448192 12230 layer_factory.hpp:77] Creating layer loss
I0205 04:37:44.448220 12230 net.cpp:150] Setting up loss
I0205 04:37:44.448228 12230 net.cpp:157] Top shape: (1)
I0205 04:37:44.448233 12230 net.cpp:160]     with loss weight 1
I0205 04:37:44.448258 12230 net.cpp:165] Memory required for data: 151364804
I0205 04:37:44.448269 12230 net.cpp:226] loss needs backward computation.
I0205 04:37:44.448277 12230 net.cpp:226] fc8 needs backward computation.
I0205 04:37:44.448283 12230 net.cpp:226] drop7 needs backward computation.
I0205 04:37:44.448289 12230 net.cpp:226] relu7 needs backward computation.
I0205 04:37:44.448294 12230 net.cpp:226] fc7 needs backward computation.
I0205 04:37:44.448300 12230 net.cpp:226] drop6 needs backward computation.
I0205 04:37:44.448307 12230 net.cpp:226] relu6 needs backward computation.
I0205 04:37:44.448312 12230 net.cpp:226] fc6 needs backward computation.
I0205 04:37:44.448318 12230 net.cpp:226] pool5 needs backward computation.
I0205 04:37:44.448323 12230 net.cpp:226] relu5 needs backward computation.
I0205 04:37:44.448329 12230 net.cpp:226] conv5 needs backward computation.
I0205 04:37:44.448335 12230 net.cpp:226] relu4 needs backward computation.
I0205 04:37:44.448340 12230 net.cpp:226] conv4 needs backward computation.
I0205 04:37:44.448346 12230 net.cpp:226] relu3 needs backward computation.
I0205 04:37:44.448353 12230 net.cpp:226] conv3 needs backward computation.
I0205 04:37:44.448361 12230 net.cpp:226] norm2 needs backward computation.
I0205 04:37:44.448369 12230 net.cpp:226] pool2 needs backward computation.
I0205 04:37:44.448374 12230 net.cpp:226] relu2 needs backward computation.
I0205 04:37:44.448380 12230 net.cpp:226] conv2 needs backward computation.
I0205 04:37:44.448386 12230 net.cpp:226] norm1 needs backward computation.
I0205 04:37:44.448392 12230 net.cpp:226] pool1 needs backward computation.
I0205 04:37:44.448401 12230 net.cpp:226] relu1 needs backward computation.
I0205 04:37:44.448407 12230 net.cpp:226] conv1 needs backward computation.
I0205 04:37:44.448413 12230 net.cpp:228] data does not need backward computation.
I0205 04:37:44.448420 12230 net.cpp:270] This network produces output loss
I0205 04:37:44.448447 12230 net.cpp:283] Network initialization done.
I0205 04:37:44.449187 12230 solver.cpp:181] Creating test net (#0) specified by net file: /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/train_val.prototxt
I0205 04:37:44.449246 12230 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0205 04:37:44.449539 12230 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto"
  }
  data_param {
    source: "/home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0205 04:37:44.449722 12230 layer_factory.hpp:77] Creating layer data
I0205 04:37:44.449892 12230 net.cpp:106] Creating Layer data
I0205 04:37:44.449911 12230 net.cpp:411] data -> data
I0205 04:37:44.449925 12230 net.cpp:411] data -> label
I0205 04:37:44.449937 12230 data_transformer.cpp:25] Loading mean file from: /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db/mean_file.binaryproto
I0205 04:37:44.450160 12235 db_lmdb.cpp:38] Opened lmdb /home/pearlstl/proj_cv/master/lmdb/synth1_test_lum_db
I0205 04:37:44.450930 12230 data_layer.cpp:41] output data size: 100,1,227,227
I0205 04:37:44.480881 12230 net.cpp:150] Setting up data
I0205 04:37:44.480901 12230 net.cpp:157] Top shape: 100 1 227 227 (5152900)
I0205 04:37:44.480908 12230 net.cpp:157] Top shape: 100 (100)
I0205 04:37:44.480914 12230 net.cpp:165] Memory required for data: 20612000
I0205 04:37:44.480921 12230 layer_factory.hpp:77] Creating layer label_data_1_split
I0205 04:37:44.480936 12230 net.cpp:106] Creating Layer label_data_1_split
I0205 04:37:44.480942 12230 net.cpp:454] label_data_1_split <- label
I0205 04:37:44.480952 12230 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0205 04:37:44.480963 12230 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0205 04:37:44.480975 12230 net.cpp:150] Setting up label_data_1_split
I0205 04:37:44.480983 12230 net.cpp:157] Top shape: 100 (100)
I0205 04:37:44.480988 12230 net.cpp:157] Top shape: 100 (100)
I0205 04:37:44.480994 12230 net.cpp:165] Memory required for data: 20612800
I0205 04:37:44.480999 12230 layer_factory.hpp:77] Creating layer conv1
I0205 04:37:44.481012 12230 net.cpp:106] Creating Layer conv1
I0205 04:37:44.481019 12230 net.cpp:454] conv1 <- data
I0205 04:37:44.481027 12230 net.cpp:411] conv1 -> conv1
I0205 04:37:44.481099 12230 net.cpp:150] Setting up conv1
I0205 04:37:44.481112 12230 net.cpp:157] Top shape: 100 32 55 55 (9680000)
I0205 04:37:44.481117 12230 net.cpp:165] Memory required for data: 59332800
I0205 04:37:44.481132 12230 layer_factory.hpp:77] Creating layer relu1
I0205 04:37:44.481142 12230 net.cpp:106] Creating Layer relu1
I0205 04:37:44.481148 12230 net.cpp:454] relu1 <- conv1
I0205 04:37:44.481155 12230 net.cpp:397] relu1 -> conv1 (in-place)
I0205 04:37:44.481168 12230 net.cpp:150] Setting up relu1
I0205 04:37:44.481174 12230 net.cpp:157] Top shape: 100 32 55 55 (9680000)
I0205 04:37:44.481180 12230 net.cpp:165] Memory required for data: 98052800
I0205 04:37:44.481185 12230 layer_factory.hpp:77] Creating layer pool1
I0205 04:37:44.481195 12230 net.cpp:106] Creating Layer pool1
I0205 04:37:44.481201 12230 net.cpp:454] pool1 <- conv1
I0205 04:37:44.481209 12230 net.cpp:411] pool1 -> pool1
I0205 04:37:44.481221 12230 net.cpp:150] Setting up pool1
I0205 04:37:44.481230 12230 net.cpp:157] Top shape: 100 32 27 27 (2332800)
I0205 04:37:44.481235 12230 net.cpp:165] Memory required for data: 107384000
I0205 04:37:44.481240 12230 layer_factory.hpp:77] Creating layer norm1
I0205 04:37:44.481248 12230 net.cpp:106] Creating Layer norm1
I0205 04:37:44.481254 12230 net.cpp:454] norm1 <- pool1
I0205 04:37:44.481264 12230 net.cpp:411] norm1 -> norm1
I0205 04:37:44.481276 12230 net.cpp:150] Setting up norm1
I0205 04:37:44.481282 12230 net.cpp:157] Top shape: 100 32 27 27 (2332800)
I0205 04:37:44.481290 12230 net.cpp:165] Memory required for data: 116715200
I0205 04:37:44.481295 12230 layer_factory.hpp:77] Creating layer conv2
I0205 04:37:44.481305 12230 net.cpp:106] Creating Layer conv2
I0205 04:37:44.481312 12230 net.cpp:454] conv2 <- norm1
I0205 04:37:44.481318 12230 net.cpp:411] conv2 -> conv2
I0205 04:37:44.481447 12230 net.cpp:150] Setting up conv2
I0205 04:37:44.481456 12230 net.cpp:157] Top shape: 100 32 27 27 (2332800)
I0205 04:37:44.481462 12230 net.cpp:165] Memory required for data: 126046400
I0205 04:37:44.481472 12230 layer_factory.hpp:77] Creating layer relu2
I0205 04:37:44.481480 12230 net.cpp:106] Creating Layer relu2
I0205 04:37:44.481487 12230 net.cpp:454] relu2 <- conv2
I0205 04:37:44.481501 12230 net.cpp:397] relu2 -> conv2 (in-place)
I0205 04:37:44.481519 12230 net.cpp:150] Setting up relu2
I0205 04:37:44.481528 12230 net.cpp:157] Top shape: 100 32 27 27 (2332800)
I0205 04:37:44.481533 12230 net.cpp:165] Memory required for data: 135377600
I0205 04:37:44.481537 12230 layer_factory.hpp:77] Creating layer pool2
I0205 04:37:44.481547 12230 net.cpp:106] Creating Layer pool2
I0205 04:37:44.481554 12230 net.cpp:454] pool2 <- conv2
I0205 04:37:44.481564 12230 net.cpp:411] pool2 -> pool2
I0205 04:37:44.481575 12230 net.cpp:150] Setting up pool2
I0205 04:37:44.481583 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.481588 12230 net.cpp:165] Memory required for data: 137540800
I0205 04:37:44.481596 12230 layer_factory.hpp:77] Creating layer norm2
I0205 04:37:44.481606 12230 net.cpp:106] Creating Layer norm2
I0205 04:37:44.481611 12230 net.cpp:454] norm2 <- pool2
I0205 04:37:44.481618 12230 net.cpp:411] norm2 -> norm2
I0205 04:37:44.481628 12230 net.cpp:150] Setting up norm2
I0205 04:37:44.481636 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.481640 12230 net.cpp:165] Memory required for data: 139704000
I0205 04:37:44.481645 12230 layer_factory.hpp:77] Creating layer conv3
I0205 04:37:44.481655 12230 net.cpp:106] Creating Layer conv3
I0205 04:37:44.481662 12230 net.cpp:454] conv3 <- norm2
I0205 04:37:44.481668 12230 net.cpp:411] conv3 -> conv3
I0205 04:37:44.481766 12230 net.cpp:150] Setting up conv3
I0205 04:37:44.481775 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.481781 12230 net.cpp:165] Memory required for data: 141867200
I0205 04:37:44.481791 12230 layer_factory.hpp:77] Creating layer relu3
I0205 04:37:44.481799 12230 net.cpp:106] Creating Layer relu3
I0205 04:37:44.481806 12230 net.cpp:454] relu3 <- conv3
I0205 04:37:44.481812 12230 net.cpp:397] relu3 -> conv3 (in-place)
I0205 04:37:44.481822 12230 net.cpp:150] Setting up relu3
I0205 04:37:44.481828 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.481833 12230 net.cpp:165] Memory required for data: 144030400
I0205 04:37:44.481839 12230 layer_factory.hpp:77] Creating layer conv4
I0205 04:37:44.481848 12230 net.cpp:106] Creating Layer conv4
I0205 04:37:44.481855 12230 net.cpp:454] conv4 <- conv3
I0205 04:37:44.481864 12230 net.cpp:411] conv4 -> conv4
I0205 04:37:44.481923 12230 net.cpp:150] Setting up conv4
I0205 04:37:44.481931 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.481936 12230 net.cpp:165] Memory required for data: 146193600
I0205 04:37:44.481945 12230 layer_factory.hpp:77] Creating layer relu4
I0205 04:37:44.481952 12230 net.cpp:106] Creating Layer relu4
I0205 04:37:44.481958 12230 net.cpp:454] relu4 <- conv4
I0205 04:37:44.481966 12230 net.cpp:397] relu4 -> conv4 (in-place)
I0205 04:37:44.481973 12230 net.cpp:150] Setting up relu4
I0205 04:37:44.481981 12230 net.cpp:157] Top shape: 100 32 13 13 (540800)
I0205 04:37:44.481986 12230 net.cpp:165] Memory required for data: 148356800
I0205 04:37:44.481992 12230 layer_factory.hpp:77] Creating layer conv5
I0205 04:37:44.482002 12230 net.cpp:106] Creating Layer conv5
I0205 04:37:44.482007 12230 net.cpp:454] conv5 <- conv4
I0205 04:37:44.482017 12230 net.cpp:411] conv5 -> conv5
I0205 04:37:44.482056 12230 net.cpp:150] Setting up conv5
I0205 04:37:44.482064 12230 net.cpp:157] Top shape: 100 16 13 13 (270400)
I0205 04:37:44.482071 12230 net.cpp:165] Memory required for data: 149438400
I0205 04:37:44.482084 12230 layer_factory.hpp:77] Creating layer relu5
I0205 04:37:44.482098 12230 net.cpp:106] Creating Layer relu5
I0205 04:37:44.482105 12230 net.cpp:454] relu5 <- conv5
I0205 04:37:44.482112 12230 net.cpp:397] relu5 -> conv5 (in-place)
I0205 04:37:44.482121 12230 net.cpp:150] Setting up relu5
I0205 04:37:44.482128 12230 net.cpp:157] Top shape: 100 16 13 13 (270400)
I0205 04:37:44.482133 12230 net.cpp:165] Memory required for data: 150520000
I0205 04:37:44.482138 12230 layer_factory.hpp:77] Creating layer pool5
I0205 04:37:44.482148 12230 net.cpp:106] Creating Layer pool5
I0205 04:37:44.482158 12230 net.cpp:454] pool5 <- conv5
I0205 04:37:44.482178 12230 net.cpp:411] pool5 -> pool5
I0205 04:37:44.482189 12230 net.cpp:150] Setting up pool5
I0205 04:37:44.482197 12230 net.cpp:157] Top shape: 100 16 6 6 (57600)
I0205 04:37:44.482203 12230 net.cpp:165] Memory required for data: 150750400
I0205 04:37:44.482208 12230 layer_factory.hpp:77] Creating layer fc6
I0205 04:37:44.482218 12230 net.cpp:106] Creating Layer fc6
I0205 04:37:44.482224 12230 net.cpp:454] fc6 <- pool5
I0205 04:37:44.482234 12230 net.cpp:411] fc6 -> fc6
I0205 04:37:44.483878 12230 net.cpp:150] Setting up fc6
I0205 04:37:44.483891 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.483896 12230 net.cpp:165] Memory required for data: 150852800
I0205 04:37:44.483906 12230 layer_factory.hpp:77] Creating layer relu6
I0205 04:37:44.483913 12230 net.cpp:106] Creating Layer relu6
I0205 04:37:44.483919 12230 net.cpp:454] relu6 <- fc6
I0205 04:37:44.483927 12230 net.cpp:397] relu6 -> fc6 (in-place)
I0205 04:37:44.483937 12230 net.cpp:150] Setting up relu6
I0205 04:37:44.483943 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.483948 12230 net.cpp:165] Memory required for data: 150955200
I0205 04:37:44.483954 12230 layer_factory.hpp:77] Creating layer drop6
I0205 04:37:44.483963 12230 net.cpp:106] Creating Layer drop6
I0205 04:37:44.483971 12230 net.cpp:454] drop6 <- fc6
I0205 04:37:44.483979 12230 net.cpp:397] drop6 -> fc6 (in-place)
I0205 04:37:44.483990 12230 net.cpp:150] Setting up drop6
I0205 04:37:44.483999 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.484006 12230 net.cpp:165] Memory required for data: 151057600
I0205 04:37:44.484012 12230 layer_factory.hpp:77] Creating layer fc7
I0205 04:37:44.484021 12230 net.cpp:106] Creating Layer fc7
I0205 04:37:44.484026 12230 net.cpp:454] fc7 <- fc6
I0205 04:37:44.484035 12230 net.cpp:411] fc7 -> fc7
I0205 04:37:44.484796 12230 net.cpp:150] Setting up fc7
I0205 04:37:44.484807 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.484813 12230 net.cpp:165] Memory required for data: 151160000
I0205 04:37:44.484822 12230 layer_factory.hpp:77] Creating layer relu7
I0205 04:37:44.484830 12230 net.cpp:106] Creating Layer relu7
I0205 04:37:44.484836 12230 net.cpp:454] relu7 <- fc7
I0205 04:37:44.484846 12230 net.cpp:397] relu7 -> fc7 (in-place)
I0205 04:37:44.484855 12230 net.cpp:150] Setting up relu7
I0205 04:37:44.484863 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.484868 12230 net.cpp:165] Memory required for data: 151262400
I0205 04:37:44.484872 12230 layer_factory.hpp:77] Creating layer drop7
I0205 04:37:44.484880 12230 net.cpp:106] Creating Layer drop7
I0205 04:37:44.484886 12230 net.cpp:454] drop7 <- fc7
I0205 04:37:44.484894 12230 net.cpp:397] drop7 -> fc7 (in-place)
I0205 04:37:44.484905 12230 net.cpp:150] Setting up drop7
I0205 04:37:44.484912 12230 net.cpp:157] Top shape: 100 256 (25600)
I0205 04:37:44.484920 12230 net.cpp:165] Memory required for data: 151364800
I0205 04:37:44.484927 12230 layer_factory.hpp:77] Creating layer fc8
I0205 04:37:44.484936 12230 net.cpp:106] Creating Layer fc8
I0205 04:37:44.484942 12230 net.cpp:454] fc8 <- fc7
I0205 04:37:44.484951 12230 net.cpp:411] fc8 -> fc8
I0205 04:37:44.484979 12230 net.cpp:150] Setting up fc8
I0205 04:37:44.484988 12230 net.cpp:157] Top shape: 100 2 (200)
I0205 04:37:44.484993 12230 net.cpp:165] Memory required for data: 151365600
I0205 04:37:44.485002 12230 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0205 04:37:44.485013 12230 net.cpp:106] Creating Layer fc8_fc8_0_split
I0205 04:37:44.485020 12230 net.cpp:454] fc8_fc8_0_split <- fc8
I0205 04:37:44.485028 12230 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0205 04:37:44.485038 12230 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0205 04:37:44.485047 12230 net.cpp:150] Setting up fc8_fc8_0_split
I0205 04:37:44.485057 12230 net.cpp:157] Top shape: 100 2 (200)
I0205 04:37:44.485064 12230 net.cpp:157] Top shape: 100 2 (200)
I0205 04:37:44.485069 12230 net.cpp:165] Memory required for data: 151367200
I0205 04:37:44.485077 12230 layer_factory.hpp:77] Creating layer accuracy
I0205 04:37:44.485121 12230 net.cpp:106] Creating Layer accuracy
I0205 04:37:44.485129 12230 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0205 04:37:44.485136 12230 net.cpp:454] accuracy <- label_data_1_split_0
I0205 04:37:44.485144 12230 net.cpp:411] accuracy -> accuracy
I0205 04:37:44.485157 12230 net.cpp:150] Setting up accuracy
I0205 04:37:44.485165 12230 net.cpp:157] Top shape: (1)
I0205 04:37:44.485170 12230 net.cpp:165] Memory required for data: 151367204
I0205 04:37:44.485177 12230 layer_factory.hpp:77] Creating layer loss
I0205 04:37:44.485188 12230 net.cpp:106] Creating Layer loss
I0205 04:37:44.485194 12230 net.cpp:454] loss <- fc8_fc8_0_split_1
I0205 04:37:44.485201 12230 net.cpp:454] loss <- label_data_1_split_1
I0205 04:37:44.485209 12230 net.cpp:411] loss -> loss
I0205 04:37:44.485222 12230 layer_factory.hpp:77] Creating layer loss
I0205 04:37:44.485255 12230 net.cpp:150] Setting up loss
I0205 04:37:44.485265 12230 net.cpp:157] Top shape: (1)
I0205 04:37:44.485270 12230 net.cpp:160]     with loss weight 1
I0205 04:37:44.485281 12230 net.cpp:165] Memory required for data: 151367208
I0205 04:37:44.485287 12230 net.cpp:226] loss needs backward computation.
I0205 04:37:44.485293 12230 net.cpp:228] accuracy does not need backward computation.
I0205 04:37:44.485301 12230 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0205 04:37:44.485307 12230 net.cpp:226] fc8 needs backward computation.
I0205 04:37:44.485313 12230 net.cpp:226] drop7 needs backward computation.
I0205 04:37:44.485319 12230 net.cpp:226] relu7 needs backward computation.
I0205 04:37:44.485324 12230 net.cpp:226] fc7 needs backward computation.
I0205 04:37:44.485330 12230 net.cpp:226] drop6 needs backward computation.
I0205 04:37:44.485335 12230 net.cpp:226] relu6 needs backward computation.
I0205 04:37:44.485342 12230 net.cpp:226] fc6 needs backward computation.
I0205 04:37:44.485347 12230 net.cpp:226] pool5 needs backward computation.
I0205 04:37:44.485353 12230 net.cpp:226] relu5 needs backward computation.
I0205 04:37:44.485359 12230 net.cpp:226] conv5 needs backward computation.
I0205 04:37:44.485364 12230 net.cpp:226] relu4 needs backward computation.
I0205 04:37:44.485370 12230 net.cpp:226] conv4 needs backward computation.
I0205 04:37:44.485378 12230 net.cpp:226] relu3 needs backward computation.
I0205 04:37:44.485383 12230 net.cpp:226] conv3 needs backward computation.
I0205 04:37:44.485390 12230 net.cpp:226] norm2 needs backward computation.
I0205 04:37:44.485396 12230 net.cpp:226] pool2 needs backward computation.
I0205 04:37:44.485402 12230 net.cpp:226] relu2 needs backward computation.
I0205 04:37:44.485409 12230 net.cpp:226] conv2 needs backward computation.
I0205 04:37:44.485415 12230 net.cpp:226] norm1 needs backward computation.
I0205 04:37:44.485421 12230 net.cpp:226] pool1 needs backward computation.
I0205 04:37:44.485427 12230 net.cpp:226] relu1 needs backward computation.
I0205 04:37:44.485433 12230 net.cpp:226] conv1 needs backward computation.
I0205 04:37:44.485440 12230 net.cpp:228] label_data_1_split does not need backward computation.
I0205 04:37:44.485447 12230 net.cpp:228] data does not need backward computation.
I0205 04:37:44.485452 12230 net.cpp:270] This network produces output accuracy
I0205 04:37:44.485458 12230 net.cpp:270] This network produces output loss
I0205 04:37:44.485487 12230 net.cpp:283] Network initialization done.
I0205 04:37:44.485581 12230 solver.cpp:60] Solver scaffolding done.
I0205 04:37:44.485638 12230 caffe.cpp:212] Starting Optimization
I0205 04:37:44.485646 12230 solver.cpp:288] Solving CaffeNet
I0205 04:37:44.485651 12230 solver.cpp:289] Learning Rate Policy: step
I0205 04:37:44.486531 12230 solver.cpp:341] Iteration 0, Testing net (#0)
I0205 04:37:44.486637 12230 blocking_queue.cpp:50] Data layer prefetch queue empty
I0205 04:37:51.772757 12230 solver.cpp:409]     Test net output #0: accuracy = 0.5
I0205 04:37:51.772815 12230 solver.cpp:409]     Test net output #1: loss = 2.7591 (* 1 = 2.7591 loss)
I0205 04:37:53.411528 12230 solver.cpp:237] Iteration 0, loss = 3.70006
I0205 04:37:53.411613 12230 solver.cpp:253]     Train net output #0: loss = 3.70006 (* 1 = 3.70006 loss)
I0205 04:37:53.411628 12230 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0205 04:38:11.865062 12230 solver.cpp:237] Iteration 10, loss = 1.81618
I0205 04:38:11.865134 12230 solver.cpp:253]     Train net output #0: loss = 1.81618 (* 1 = 1.81618 loss)
I0205 04:38:11.865146 12230 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0205 04:38:37.226318 12230 solver.cpp:237] Iteration 20, loss = 1.01777
I0205 04:38:37.226444 12230 solver.cpp:253]     Train net output #0: loss = 1.01777 (* 1 = 1.01777 loss)
I0205 04:38:37.226459 12230 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0205 04:39:11.026927 12230 solver.cpp:237] Iteration 30, loss = 1.07058
I0205 04:39:11.027168 12230 solver.cpp:253]     Train net output #0: loss = 1.07058 (* 1 = 1.07058 loss)
I0205 04:39:11.027184 12230 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0205 04:39:47.033392 12230 solver.cpp:237] Iteration 40, loss = 0.929568
I0205 04:39:47.033613 12230 solver.cpp:253]     Train net output #0: loss = 0.929568 (* 1 = 0.929568 loss)
I0205 04:39:47.033629 12230 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0205 04:40:22.090507 12230 solver.cpp:237] Iteration 50, loss = 0.750363
I0205 04:40:22.090749 12230 solver.cpp:253]     Train net output #0: loss = 0.750363 (* 1 = 0.750363 loss)
I0205 04:40:22.090764 12230 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0205 04:40:56.414753 12230 solver.cpp:237] Iteration 60, loss = 0.723943
I0205 04:40:56.414970 12230 solver.cpp:253]     Train net output #0: loss = 0.723943 (* 1 = 0.723943 loss)
I0205 04:40:56.414986 12230 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0205 04:41:27.036794 12230 solver.cpp:237] Iteration 70, loss = 0.590129
I0205 04:41:27.036999 12230 solver.cpp:253]     Train net output #0: loss = 0.590129 (* 1 = 0.590129 loss)
I0205 04:41:27.037015 12230 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0205 04:41:56.606267 12230 solver.cpp:237] Iteration 80, loss = 0.514057
I0205 04:41:56.606351 12230 solver.cpp:253]     Train net output #0: loss = 0.514057 (* 1 = 0.514057 loss)
I0205 04:41:56.606364 12230 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0205 04:42:25.585865 12230 solver.cpp:237] Iteration 90, loss = 0.289373
I0205 04:42:25.586091 12230 solver.cpp:253]     Train net output #0: loss = 0.289373 (* 1 = 0.289373 loss)
I0205 04:42:25.586107 12230 sgd_solver.cpp:106] Iteration 90, lr = 0.001
I0205 04:42:51.793354 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_100.caffemodel
I0205 04:42:51.797641 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_100.solverstate
I0205 04:42:51.799028 12230 solver.cpp:341] Iteration 100, Testing net (#0)
I0205 04:43:06.123505 12230 solver.cpp:409]     Test net output #0: accuracy = 0.951
I0205 04:43:06.123709 12230 solver.cpp:409]     Test net output #1: loss = 0.161431 (* 1 = 0.161431 loss)
I0205 04:43:09.109091 12230 solver.cpp:237] Iteration 100, loss = 0.321957
I0205 04:43:09.109172 12230 solver.cpp:253]     Train net output #0: loss = 0.321957 (* 1 = 0.321957 loss)
I0205 04:43:09.109185 12230 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0205 04:43:39.059500 12230 solver.cpp:237] Iteration 110, loss = 0.321804
I0205 04:43:39.059731 12230 solver.cpp:253]     Train net output #0: loss = 0.321804 (* 1 = 0.321804 loss)
I0205 04:43:39.059747 12230 sgd_solver.cpp:106] Iteration 110, lr = 0.001
I0205 04:44:08.543090 12230 solver.cpp:237] Iteration 120, loss = 0.173537
I0205 04:44:08.543179 12230 solver.cpp:253]     Train net output #0: loss = 0.173537 (* 1 = 0.173537 loss)
I0205 04:44:08.543192 12230 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0205 04:44:37.112287 12230 solver.cpp:237] Iteration 130, loss = 0.130726
I0205 04:44:37.112540 12230 solver.cpp:253]     Train net output #0: loss = 0.130726 (* 1 = 0.130726 loss)
I0205 04:44:37.112565 12230 sgd_solver.cpp:106] Iteration 130, lr = 0.001
I0205 04:45:04.912715 12230 solver.cpp:237] Iteration 140, loss = 0.0806682
I0205 04:45:04.912801 12230 solver.cpp:253]     Train net output #0: loss = 0.0806682 (* 1 = 0.0806682 loss)
I0205 04:45:04.912813 12230 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0205 04:45:33.303833 12230 solver.cpp:237] Iteration 150, loss = 0.0505939
I0205 04:45:33.304091 12230 solver.cpp:253]     Train net output #0: loss = 0.0505939 (* 1 = 0.0505939 loss)
I0205 04:45:33.304107 12230 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0205 04:46:02.502673 12230 solver.cpp:237] Iteration 160, loss = 0.0636802
I0205 04:46:02.502756 12230 solver.cpp:253]     Train net output #0: loss = 0.0636801 (* 1 = 0.0636801 loss)
I0205 04:46:02.502769 12230 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0205 04:46:32.251036 12230 solver.cpp:237] Iteration 170, loss = 0.0618737
I0205 04:46:32.251251 12230 solver.cpp:253]     Train net output #0: loss = 0.0618737 (* 1 = 0.0618737 loss)
I0205 04:46:32.251268 12230 sgd_solver.cpp:106] Iteration 170, lr = 0.001
I0205 04:47:02.342483 12230 solver.cpp:237] Iteration 180, loss = 0.142263
I0205 04:47:02.342694 12230 solver.cpp:253]     Train net output #0: loss = 0.142263 (* 1 = 0.142263 loss)
I0205 04:47:02.342710 12230 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0205 04:47:32.605170 12230 solver.cpp:237] Iteration 190, loss = 0.153471
I0205 04:47:32.605386 12230 solver.cpp:253]     Train net output #0: loss = 0.153471 (* 1 = 0.153471 loss)
I0205 04:47:32.605402 12230 sgd_solver.cpp:106] Iteration 190, lr = 0.001
I0205 04:47:59.972427 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_200.caffemodel
I0205 04:47:59.975955 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_200.solverstate
I0205 04:47:59.977329 12230 solver.cpp:341] Iteration 200, Testing net (#0)
I0205 04:48:14.428644 12230 solver.cpp:409]     Test net output #0: accuracy = 0.995
I0205 04:48:14.428850 12230 solver.cpp:409]     Test net output #1: loss = 0.0251311 (* 1 = 0.0251311 loss)
I0205 04:48:17.454200 12230 solver.cpp:237] Iteration 200, loss = 0.0513604
I0205 04:48:17.454278 12230 solver.cpp:253]     Train net output #0: loss = 0.0513604 (* 1 = 0.0513604 loss)
I0205 04:48:17.454291 12230 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0205 04:48:47.611415 12230 solver.cpp:237] Iteration 210, loss = 0.0542656
I0205 04:48:47.611631 12230 solver.cpp:253]     Train net output #0: loss = 0.0542656 (* 1 = 0.0542656 loss)
I0205 04:48:47.611649 12230 sgd_solver.cpp:106] Iteration 210, lr = 0.001
I0205 04:49:17.589483 12230 solver.cpp:237] Iteration 220, loss = 0.0208199
I0205 04:49:17.589570 12230 solver.cpp:253]     Train net output #0: loss = 0.0208199 (* 1 = 0.0208199 loss)
I0205 04:49:17.589583 12230 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0205 04:49:47.919569 12230 solver.cpp:237] Iteration 230, loss = 0.0580316
I0205 04:49:47.919776 12230 solver.cpp:253]     Train net output #0: loss = 0.0580315 (* 1 = 0.0580315 loss)
I0205 04:49:47.919792 12230 sgd_solver.cpp:106] Iteration 230, lr = 0.001
I0205 04:50:18.396739 12230 solver.cpp:237] Iteration 240, loss = 0.110992
I0205 04:50:18.396963 12230 solver.cpp:253]     Train net output #0: loss = 0.110992 (* 1 = 0.110992 loss)
I0205 04:50:18.396980 12230 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0205 04:50:49.357656 12230 solver.cpp:237] Iteration 250, loss = 0.0487477
I0205 04:50:49.358423 12230 solver.cpp:253]     Train net output #0: loss = 0.0487477 (* 1 = 0.0487477 loss)
I0205 04:50:49.358450 12230 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0205 04:51:20.493404 12230 solver.cpp:237] Iteration 260, loss = 0.0136559
I0205 04:51:20.493659 12230 solver.cpp:253]     Train net output #0: loss = 0.0136558 (* 1 = 0.0136558 loss)
I0205 04:51:20.493680 12230 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0205 04:51:51.559267 12230 solver.cpp:237] Iteration 270, loss = 0.0540787
I0205 04:51:51.559487 12230 solver.cpp:253]     Train net output #0: loss = 0.0540787 (* 1 = 0.0540787 loss)
I0205 04:51:51.559504 12230 sgd_solver.cpp:106] Iteration 270, lr = 0.001
I0205 04:52:23.103839 12230 solver.cpp:237] Iteration 280, loss = 0.0336283
I0205 04:52:23.104084 12230 solver.cpp:253]     Train net output #0: loss = 0.0336283 (* 1 = 0.0336283 loss)
I0205 04:52:23.104099 12230 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0205 04:52:54.731258 12230 solver.cpp:237] Iteration 290, loss = 0.017694
I0205 04:52:54.731474 12230 solver.cpp:253]     Train net output #0: loss = 0.017694 (* 1 = 0.017694 loss)
I0205 04:52:54.731490 12230 sgd_solver.cpp:106] Iteration 290, lr = 0.001
I0205 04:53:23.740717 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_300.caffemodel
I0205 04:53:23.744292 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_300.solverstate
I0205 04:53:23.745676 12230 solver.cpp:341] Iteration 300, Testing net (#0)
I0205 04:53:39.152318 12230 solver.cpp:409]     Test net output #0: accuracy = 0.993
I0205 04:53:39.152519 12230 solver.cpp:409]     Test net output #1: loss = 0.0167183 (* 1 = 0.0167183 loss)
I0205 04:53:42.373636 12230 solver.cpp:237] Iteration 300, loss = 0.0511373
I0205 04:53:42.373713 12230 solver.cpp:253]     Train net output #0: loss = 0.0511373 (* 1 = 0.0511373 loss)
I0205 04:53:42.373726 12230 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0205 04:54:15.100639 12230 solver.cpp:237] Iteration 310, loss = 0.0446478
I0205 04:54:15.100847 12230 solver.cpp:253]     Train net output #0: loss = 0.0446477 (* 1 = 0.0446477 loss)
I0205 04:54:15.100863 12230 sgd_solver.cpp:106] Iteration 310, lr = 0.001
I0205 04:54:48.015651 12230 solver.cpp:237] Iteration 320, loss = 0.0646392
I0205 04:54:48.015851 12230 solver.cpp:253]     Train net output #0: loss = 0.0646391 (* 1 = 0.0646391 loss)
I0205 04:54:48.015868 12230 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0205 04:55:21.127861 12230 solver.cpp:237] Iteration 330, loss = 0.0116772
I0205 04:55:21.128093 12230 solver.cpp:253]     Train net output #0: loss = 0.0116771 (* 1 = 0.0116771 loss)
I0205 04:55:21.128108 12230 sgd_solver.cpp:106] Iteration 330, lr = 0.001
I0205 04:55:54.370215 12230 solver.cpp:237] Iteration 340, loss = 0.00543828
I0205 04:55:54.370430 12230 solver.cpp:253]     Train net output #0: loss = 0.00543824 (* 1 = 0.00543824 loss)
I0205 04:55:54.370447 12230 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0205 04:56:27.507699 12230 solver.cpp:237] Iteration 350, loss = 0.0130653
I0205 04:56:27.507930 12230 solver.cpp:253]     Train net output #0: loss = 0.0130653 (* 1 = 0.0130653 loss)
I0205 04:56:27.507946 12230 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I0205 04:57:00.574435 12230 solver.cpp:237] Iteration 360, loss = 0.0229247
I0205 04:57:00.574647 12230 solver.cpp:253]     Train net output #0: loss = 0.0229246 (* 1 = 0.0229246 loss)
I0205 04:57:00.574664 12230 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0205 04:57:33.782301 12230 solver.cpp:237] Iteration 370, loss = 0.122244
I0205 04:57:33.782503 12230 solver.cpp:253]     Train net output #0: loss = 0.122244 (* 1 = 0.122244 loss)
I0205 04:57:33.782518 12230 sgd_solver.cpp:106] Iteration 370, lr = 0.001
I0205 04:58:07.439246 12230 solver.cpp:237] Iteration 380, loss = 0.0123697
I0205 04:58:07.439440 12230 solver.cpp:253]     Train net output #0: loss = 0.0123697 (* 1 = 0.0123697 loss)
I0205 04:58:07.439455 12230 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0205 04:58:41.327764 12230 solver.cpp:237] Iteration 390, loss = 0.00303786
I0205 04:58:41.328011 12230 solver.cpp:253]     Train net output #0: loss = 0.00303781 (* 1 = 0.00303781 loss)
I0205 04:58:41.328030 12230 sgd_solver.cpp:106] Iteration 390, lr = 0.001
I0205 04:59:11.802109 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_400.caffemodel
I0205 04:59:11.806000 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_400.solverstate
I0205 04:59:11.807442 12230 solver.cpp:341] Iteration 400, Testing net (#0)
I0205 04:59:28.235970 12230 solver.cpp:409]     Test net output #0: accuracy = 0.989
I0205 04:59:28.236024 12230 solver.cpp:409]     Test net output #1: loss = 0.029419 (* 1 = 0.029419 loss)
I0205 04:59:31.631525 12230 solver.cpp:237] Iteration 400, loss = 0.0751772
I0205 04:59:31.631587 12230 solver.cpp:253]     Train net output #0: loss = 0.0751771 (* 1 = 0.0751771 loss)
I0205 04:59:31.631602 12230 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0205 05:00:05.483623 12230 solver.cpp:237] Iteration 410, loss = 0.0201683
I0205 05:00:05.483836 12230 solver.cpp:253]     Train net output #0: loss = 0.0201683 (* 1 = 0.0201683 loss)
I0205 05:00:05.483853 12230 sgd_solver.cpp:106] Iteration 410, lr = 0.001
I0205 05:00:39.612184 12230 solver.cpp:237] Iteration 420, loss = 0.014761
I0205 05:00:39.612406 12230 solver.cpp:253]     Train net output #0: loss = 0.014761 (* 1 = 0.014761 loss)
I0205 05:00:39.612422 12230 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0205 05:01:13.516904 12230 solver.cpp:237] Iteration 430, loss = 0.00449784
I0205 05:01:13.517127 12230 solver.cpp:253]     Train net output #0: loss = 0.0044978 (* 1 = 0.0044978 loss)
I0205 05:01:13.517144 12230 sgd_solver.cpp:106] Iteration 430, lr = 0.001
I0205 05:01:47.181718 12230 solver.cpp:237] Iteration 440, loss = 0.0349376
I0205 05:01:47.181962 12230 solver.cpp:253]     Train net output #0: loss = 0.0349375 (* 1 = 0.0349375 loss)
I0205 05:01:47.181979 12230 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0205 05:02:20.973548 12230 solver.cpp:237] Iteration 450, loss = 0.0256468
I0205 05:02:20.973762 12230 solver.cpp:253]     Train net output #0: loss = 0.0256468 (* 1 = 0.0256468 loss)
I0205 05:02:20.973778 12230 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I0205 05:02:54.868125 12230 solver.cpp:237] Iteration 460, loss = 0.00204024
I0205 05:02:54.868353 12230 solver.cpp:253]     Train net output #0: loss = 0.00204019 (* 1 = 0.00204019 loss)
I0205 05:02:54.868369 12230 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0205 05:03:29.013695 12230 solver.cpp:237] Iteration 470, loss = 0.00615034
I0205 05:03:29.013908 12230 solver.cpp:253]     Train net output #0: loss = 0.00615029 (* 1 = 0.00615029 loss)
I0205 05:03:29.013926 12230 sgd_solver.cpp:106] Iteration 470, lr = 0.001
I0205 05:04:03.009027 12230 solver.cpp:237] Iteration 480, loss = 0.0255251
I0205 05:04:03.009259 12230 solver.cpp:253]     Train net output #0: loss = 0.025525 (* 1 = 0.025525 loss)
I0205 05:04:03.009275 12230 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0205 05:04:36.435385 12230 solver.cpp:237] Iteration 490, loss = 0.0317203
I0205 05:04:36.435605 12230 solver.cpp:253]     Train net output #0: loss = 0.0317203 (* 1 = 0.0317203 loss)
I0205 05:04:36.435621 12230 sgd_solver.cpp:106] Iteration 490, lr = 0.001
I0205 05:05:06.329092 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_500.caffemodel
I0205 05:05:06.332782 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_500.solverstate
I0205 05:05:06.334174 12230 solver.cpp:341] Iteration 500, Testing net (#0)
I0205 05:05:22.082834 12230 solver.cpp:409]     Test net output #0: accuracy = 0.995
I0205 05:05:22.083091 12230 solver.cpp:409]     Test net output #1: loss = 0.0214893 (* 1 = 0.0214893 loss)
I0205 05:05:25.387704 12230 solver.cpp:237] Iteration 500, loss = 0.0456154
I0205 05:05:25.387771 12230 solver.cpp:253]     Train net output #0: loss = 0.0456154 (* 1 = 0.0456154 loss)
I0205 05:05:25.387784 12230 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0205 05:05:58.732417 12230 solver.cpp:237] Iteration 510, loss = 0.00465867
I0205 05:05:58.732645 12230 solver.cpp:253]     Train net output #0: loss = 0.00465863 (* 1 = 0.00465863 loss)
I0205 05:05:58.732661 12230 sgd_solver.cpp:106] Iteration 510, lr = 0.001
I0205 05:06:32.867563 12230 solver.cpp:237] Iteration 520, loss = 0.0367031
I0205 05:06:32.867756 12230 solver.cpp:253]     Train net output #0: loss = 0.0367031 (* 1 = 0.0367031 loss)
I0205 05:06:32.867772 12230 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I0205 05:07:06.970688 12230 solver.cpp:237] Iteration 530, loss = 0.0211995
I0205 05:07:06.970899 12230 solver.cpp:253]     Train net output #0: loss = 0.0211995 (* 1 = 0.0211995 loss)
I0205 05:07:06.970916 12230 sgd_solver.cpp:106] Iteration 530, lr = 0.001
I0205 05:07:41.034934 12230 solver.cpp:237] Iteration 540, loss = 0.00176417
I0205 05:07:41.035130 12230 solver.cpp:253]     Train net output #0: loss = 0.00176413 (* 1 = 0.00176413 loss)
I0205 05:07:41.035145 12230 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I0205 05:08:14.839484 12230 solver.cpp:237] Iteration 550, loss = 0.00196207
I0205 05:08:14.839668 12230 solver.cpp:253]     Train net output #0: loss = 0.00196202 (* 1 = 0.00196202 loss)
I0205 05:08:14.839682 12230 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I0205 05:08:48.706387 12230 solver.cpp:237] Iteration 560, loss = 0.00501365
I0205 05:08:48.706573 12230 solver.cpp:253]     Train net output #0: loss = 0.00501361 (* 1 = 0.00501361 loss)
I0205 05:08:48.706588 12230 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I0205 05:09:22.047829 12230 solver.cpp:237] Iteration 570, loss = 0.0147038
I0205 05:09:22.048030 12230 solver.cpp:253]     Train net output #0: loss = 0.0147038 (* 1 = 0.0147038 loss)
I0205 05:09:22.048046 12230 sgd_solver.cpp:106] Iteration 570, lr = 0.001
I0205 05:09:55.337206 12230 solver.cpp:237] Iteration 580, loss = 0.00674073
I0205 05:09:55.337404 12230 solver.cpp:253]     Train net output #0: loss = 0.00674068 (* 1 = 0.00674068 loss)
I0205 05:09:55.337417 12230 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I0205 05:10:28.638470 12230 solver.cpp:237] Iteration 590, loss = 0.0285381
I0205 05:10:28.638659 12230 solver.cpp:253]     Train net output #0: loss = 0.0285381 (* 1 = 0.0285381 loss)
I0205 05:10:28.638675 12230 sgd_solver.cpp:106] Iteration 590, lr = 0.001
I0205 05:10:58.364230 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_600.caffemodel
I0205 05:10:58.368348 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_600.solverstate
I0205 05:10:58.369760 12230 solver.cpp:341] Iteration 600, Testing net (#0)
I0205 05:11:14.257943 12230 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 05:11:14.258133 12230 solver.cpp:409]     Test net output #1: loss = 0.00874032 (* 1 = 0.00874032 loss)
I0205 05:11:17.564216 12230 solver.cpp:237] Iteration 600, loss = 0.00156743
I0205 05:11:17.564280 12230 solver.cpp:253]     Train net output #0: loss = 0.00156738 (* 1 = 0.00156738 loss)
I0205 05:11:17.564294 12230 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0205 05:11:50.396919 12230 solver.cpp:237] Iteration 610, loss = 0.000973823
I0205 05:11:50.397115 12230 solver.cpp:253]     Train net output #0: loss = 0.000973776 (* 1 = 0.000973776 loss)
I0205 05:11:50.397130 12230 sgd_solver.cpp:106] Iteration 610, lr = 0.001
I0205 05:12:23.344769 12230 solver.cpp:237] Iteration 620, loss = 0.000798456
I0205 05:12:23.345008 12230 solver.cpp:253]     Train net output #0: loss = 0.000798409 (* 1 = 0.000798409 loss)
I0205 05:12:23.345031 12230 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I0205 05:12:56.465035 12230 solver.cpp:237] Iteration 630, loss = 0.00643054
I0205 05:12:56.465219 12230 solver.cpp:253]     Train net output #0: loss = 0.00643049 (* 1 = 0.00643049 loss)
I0205 05:12:56.465234 12230 sgd_solver.cpp:106] Iteration 630, lr = 0.001
I0205 05:13:29.590091 12230 solver.cpp:237] Iteration 640, loss = 0.000991242
I0205 05:13:29.590275 12230 solver.cpp:253]     Train net output #0: loss = 0.000991188 (* 1 = 0.000991188 loss)
I0205 05:13:29.590288 12230 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I0205 05:14:02.594861 12230 solver.cpp:237] Iteration 650, loss = 0.00701333
I0205 05:14:02.595075 12230 solver.cpp:253]     Train net output #0: loss = 0.00701328 (* 1 = 0.00701328 loss)
I0205 05:14:02.595090 12230 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I0205 05:14:35.686347 12230 solver.cpp:237] Iteration 660, loss = 0.00115784
I0205 05:14:35.686537 12230 solver.cpp:253]     Train net output #0: loss = 0.00115779 (* 1 = 0.00115779 loss)
I0205 05:14:35.686552 12230 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I0205 05:15:08.897060 12230 solver.cpp:237] Iteration 670, loss = 0.00702005
I0205 05:15:08.897239 12230 solver.cpp:253]     Train net output #0: loss = 0.00702 (* 1 = 0.00702 loss)
I0205 05:15:08.897254 12230 sgd_solver.cpp:106] Iteration 670, lr = 0.001
I0205 05:15:42.339527 12230 solver.cpp:237] Iteration 680, loss = 0.00162598
I0205 05:15:42.339716 12230 solver.cpp:253]     Train net output #0: loss = 0.00162593 (* 1 = 0.00162593 loss)
I0205 05:15:42.339730 12230 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I0205 05:16:15.959508 12230 solver.cpp:237] Iteration 690, loss = 0.0153123
I0205 05:16:15.959731 12230 solver.cpp:253]     Train net output #0: loss = 0.0153123 (* 1 = 0.0153123 loss)
I0205 05:16:15.959748 12230 sgd_solver.cpp:106] Iteration 690, lr = 0.001
I0205 05:16:46.168977 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_700.caffemodel
I0205 05:16:46.172768 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_700.solverstate
I0205 05:16:46.174139 12230 solver.cpp:341] Iteration 700, Testing net (#0)
I0205 05:17:02.421205 12230 solver.cpp:409]     Test net output #0: accuracy = 0.999
I0205 05:17:02.421262 12230 solver.cpp:409]     Test net output #1: loss = 0.0055323 (* 1 = 0.0055323 loss)
I0205 05:17:05.897341 12230 solver.cpp:237] Iteration 700, loss = 0.00140585
I0205 05:17:05.897398 12230 solver.cpp:253]     Train net output #0: loss = 0.0014058 (* 1 = 0.0014058 loss)
I0205 05:17:05.897411 12230 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0205 05:17:39.675290 12230 solver.cpp:237] Iteration 710, loss = 0.00817865
I0205 05:17:39.675483 12230 solver.cpp:253]     Train net output #0: loss = 0.0081786 (* 1 = 0.0081786 loss)
I0205 05:17:39.675498 12230 sgd_solver.cpp:106] Iteration 710, lr = 0.001
I0205 05:18:13.694718 12230 solver.cpp:237] Iteration 720, loss = 0.00521487
I0205 05:18:13.694902 12230 solver.cpp:253]     Train net output #0: loss = 0.00521482 (* 1 = 0.00521482 loss)
I0205 05:18:13.694917 12230 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I0205 05:18:47.518273 12230 solver.cpp:237] Iteration 730, loss = 0.101878
I0205 05:18:47.518496 12230 solver.cpp:253]     Train net output #0: loss = 0.101878 (* 1 = 0.101878 loss)
I0205 05:18:47.518513 12230 sgd_solver.cpp:106] Iteration 730, lr = 0.001
I0205 05:19:21.949352 12230 solver.cpp:237] Iteration 740, loss = 0.265414
I0205 05:19:21.949560 12230 solver.cpp:253]     Train net output #0: loss = 0.265414 (* 1 = 0.265414 loss)
I0205 05:19:21.949578 12230 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I0205 05:19:56.717594 12230 solver.cpp:237] Iteration 750, loss = 0.143134
I0205 05:19:56.717875 12230 solver.cpp:253]     Train net output #0: loss = 0.143134 (* 1 = 0.143134 loss)
I0205 05:19:56.717897 12230 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I0205 05:20:32.507093 12230 solver.cpp:237] Iteration 760, loss = 0.155038
I0205 05:20:32.507315 12230 solver.cpp:253]     Train net output #0: loss = 0.155038 (* 1 = 0.155038 loss)
I0205 05:20:32.507333 12230 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I0205 05:21:08.970227 12230 solver.cpp:237] Iteration 770, loss = 0.0155812
I0205 05:21:08.970444 12230 solver.cpp:253]     Train net output #0: loss = 0.0155811 (* 1 = 0.0155811 loss)
I0205 05:21:08.970461 12230 sgd_solver.cpp:106] Iteration 770, lr = 0.001
I0205 05:21:45.222024 12230 solver.cpp:237] Iteration 780, loss = 0.014697
I0205 05:21:45.222255 12230 solver.cpp:253]     Train net output #0: loss = 0.014697 (* 1 = 0.014697 loss)
I0205 05:21:45.222272 12230 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I0205 05:22:21.122100 12230 solver.cpp:237] Iteration 790, loss = 0.00162587
I0205 05:22:21.122329 12230 solver.cpp:253]     Train net output #0: loss = 0.00162582 (* 1 = 0.00162582 loss)
I0205 05:22:21.122346 12230 sgd_solver.cpp:106] Iteration 790, lr = 0.001
I0205 05:22:53.368309 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_800.caffemodel
I0205 05:22:53.372362 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_800.solverstate
I0205 05:22:53.373754 12230 solver.cpp:341] Iteration 800, Testing net (#0)
I0205 05:23:10.477416 12230 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 05:23:10.477494 12230 solver.cpp:409]     Test net output #1: loss = 0.00936568 (* 1 = 0.00936568 loss)
I0205 05:23:14.035979 12230 solver.cpp:237] Iteration 800, loss = 0.00374781
I0205 05:23:14.036063 12230 solver.cpp:253]     Train net output #0: loss = 0.00374776 (* 1 = 0.00374776 loss)
I0205 05:23:14.036077 12230 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0205 05:23:49.514334 12230 solver.cpp:237] Iteration 810, loss = 0.0279216
I0205 05:23:49.514555 12230 solver.cpp:253]     Train net output #0: loss = 0.0279215 (* 1 = 0.0279215 loss)
I0205 05:23:49.514571 12230 sgd_solver.cpp:106] Iteration 810, lr = 0.001
I0205 05:24:25.249178 12230 solver.cpp:237] Iteration 820, loss = 0.0098081
I0205 05:24:25.249402 12230 solver.cpp:253]     Train net output #0: loss = 0.00980805 (* 1 = 0.00980805 loss)
I0205 05:24:25.249418 12230 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I0205 05:25:00.825249 12230 solver.cpp:237] Iteration 830, loss = 0.0284915
I0205 05:25:00.825458 12230 solver.cpp:253]     Train net output #0: loss = 0.0284915 (* 1 = 0.0284915 loss)
I0205 05:25:00.825474 12230 sgd_solver.cpp:106] Iteration 830, lr = 0.001
I0205 05:25:36.362159 12230 solver.cpp:237] Iteration 840, loss = 0.00140266
I0205 05:25:36.362743 12230 solver.cpp:253]     Train net output #0: loss = 0.00140261 (* 1 = 0.00140261 loss)
I0205 05:25:36.362759 12230 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I0205 05:26:11.603170 12230 solver.cpp:237] Iteration 850, loss = 0.00122318
I0205 05:26:11.603387 12230 solver.cpp:253]     Train net output #0: loss = 0.00122313 (* 1 = 0.00122313 loss)
I0205 05:26:11.603405 12230 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I0205 05:26:46.976179 12230 solver.cpp:237] Iteration 860, loss = 0.00555056
I0205 05:26:46.976407 12230 solver.cpp:253]     Train net output #0: loss = 0.00555051 (* 1 = 0.00555051 loss)
I0205 05:26:46.976423 12230 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I0205 05:27:22.787428 12230 solver.cpp:237] Iteration 870, loss = 0.000873919
I0205 05:27:22.787637 12230 solver.cpp:253]     Train net output #0: loss = 0.000873867 (* 1 = 0.000873867 loss)
I0205 05:27:22.787652 12230 sgd_solver.cpp:106] Iteration 870, lr = 0.001
I0205 05:27:59.100425 12230 solver.cpp:237] Iteration 880, loss = 0.0353739
I0205 05:27:59.100662 12230 solver.cpp:253]     Train net output #0: loss = 0.0353739 (* 1 = 0.0353739 loss)
I0205 05:27:59.100687 12230 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I0205 05:28:34.917430 12230 solver.cpp:237] Iteration 890, loss = 0.00138172
I0205 05:28:34.917677 12230 solver.cpp:253]     Train net output #0: loss = 0.00138167 (* 1 = 0.00138167 loss)
I0205 05:28:34.917693 12230 sgd_solver.cpp:106] Iteration 890, lr = 0.001
I0205 05:29:07.140064 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_900.caffemodel
I0205 05:29:07.143888 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_900.solverstate
I0205 05:29:07.145287 12230 solver.cpp:341] Iteration 900, Testing net (#0)
I0205 05:29:24.410435 12230 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 05:29:24.410513 12230 solver.cpp:409]     Test net output #1: loss = 0.00620289 (* 1 = 0.00620289 loss)
I0205 05:29:27.934257 12230 solver.cpp:237] Iteration 900, loss = 0.00472125
I0205 05:29:27.934331 12230 solver.cpp:253]     Train net output #0: loss = 0.0047212 (* 1 = 0.0047212 loss)
I0205 05:29:27.934345 12230 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0205 05:30:03.478253 12230 solver.cpp:237] Iteration 910, loss = 0.0031674
I0205 05:30:03.478461 12230 solver.cpp:253]     Train net output #0: loss = 0.00316735 (* 1 = 0.00316735 loss)
I0205 05:30:03.478477 12230 sgd_solver.cpp:106] Iteration 910, lr = 0.001
I0205 05:30:39.047576 12230 solver.cpp:237] Iteration 920, loss = 0.0020502
I0205 05:30:39.047801 12230 solver.cpp:253]     Train net output #0: loss = 0.00205015 (* 1 = 0.00205015 loss)
I0205 05:30:39.047817 12230 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I0205 05:31:15.000636 12230 solver.cpp:237] Iteration 930, loss = 0.00359393
I0205 05:31:15.000838 12230 solver.cpp:253]     Train net output #0: loss = 0.00359388 (* 1 = 0.00359388 loss)
I0205 05:31:15.000854 12230 sgd_solver.cpp:106] Iteration 930, lr = 0.001
I0205 05:31:50.794611 12230 solver.cpp:237] Iteration 940, loss = 0.00278265
I0205 05:31:50.794837 12230 solver.cpp:253]     Train net output #0: loss = 0.00278261 (* 1 = 0.00278261 loss)
I0205 05:31:50.794854 12230 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I0205 05:32:26.447168 12230 solver.cpp:237] Iteration 950, loss = 0.00144417
I0205 05:32:26.447405 12230 solver.cpp:253]     Train net output #0: loss = 0.00144412 (* 1 = 0.00144412 loss)
I0205 05:32:26.447422 12230 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I0205 05:33:02.036942 12230 solver.cpp:237] Iteration 960, loss = 0.00293026
I0205 05:33:02.037165 12230 solver.cpp:253]     Train net output #0: loss = 0.00293021 (* 1 = 0.00293021 loss)
I0205 05:33:02.037183 12230 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I0205 05:33:37.407855 12230 solver.cpp:237] Iteration 970, loss = 0.0273888
I0205 05:33:37.408088 12230 solver.cpp:253]     Train net output #0: loss = 0.0273887 (* 1 = 0.0273887 loss)
I0205 05:33:37.408105 12230 sgd_solver.cpp:106] Iteration 970, lr = 0.001
I0205 05:34:12.833474 12230 solver.cpp:237] Iteration 980, loss = 0.00074068
I0205 05:34:12.833688 12230 solver.cpp:253]     Train net output #0: loss = 0.000740633 (* 1 = 0.000740633 loss)
I0205 05:34:12.833704 12230 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I0205 05:34:48.281698 12230 solver.cpp:237] Iteration 990, loss = 0.0122808
I0205 05:34:48.281926 12230 solver.cpp:253]     Train net output #0: loss = 0.0122807 (* 1 = 0.0122807 loss)
I0205 05:34:48.281944 12230 sgd_solver.cpp:106] Iteration 990, lr = 0.001
I0205 05:35:20.072531 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1000.caffemodel
I0205 05:35:20.076341 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1000.solverstate
I0205 05:35:20.077729 12230 solver.cpp:341] Iteration 1000, Testing net (#0)
I0205 05:35:37.014052 12230 solver.cpp:409]     Test net output #0: accuracy = 0.997
I0205 05:35:37.014127 12230 solver.cpp:409]     Test net output #1: loss = 0.0048954 (* 1 = 0.0048954 loss)
I0205 05:35:40.549779 12230 solver.cpp:237] Iteration 1000, loss = 0.0363392
I0205 05:35:40.549857 12230 solver.cpp:253]     Train net output #0: loss = 0.0363392 (* 1 = 0.0363392 loss)
I0205 05:35:40.549870 12230 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0205 05:36:15.544780 12230 solver.cpp:237] Iteration 1010, loss = 0.0202545
I0205 05:36:15.545059 12230 solver.cpp:253]     Train net output #0: loss = 0.0202544 (* 1 = 0.0202544 loss)
I0205 05:36:15.545078 12230 sgd_solver.cpp:106] Iteration 1010, lr = 0.001
I0205 05:36:50.475946 12230 solver.cpp:237] Iteration 1020, loss = 0.00434387
I0205 05:36:50.476191 12230 solver.cpp:253]     Train net output #0: loss = 0.00434382 (* 1 = 0.00434382 loss)
I0205 05:36:50.476207 12230 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I0205 05:37:25.483409 12230 solver.cpp:237] Iteration 1030, loss = 0.000197502
I0205 05:37:25.483640 12230 solver.cpp:253]     Train net output #0: loss = 0.000197455 (* 1 = 0.000197455 loss)
I0205 05:37:25.483655 12230 sgd_solver.cpp:106] Iteration 1030, lr = 0.001
I0205 05:38:00.264461 12230 solver.cpp:237] Iteration 1040, loss = 0.00124578
I0205 05:38:00.264684 12230 solver.cpp:253]     Train net output #0: loss = 0.00124574 (* 1 = 0.00124574 loss)
I0205 05:38:00.264700 12230 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I0205 05:38:35.161376 12230 solver.cpp:237] Iteration 1050, loss = 0.000713857
I0205 05:38:35.161589 12230 solver.cpp:253]     Train net output #0: loss = 0.000713813 (* 1 = 0.000713813 loss)
I0205 05:38:35.161605 12230 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I0205 05:39:10.282768 12230 solver.cpp:237] Iteration 1060, loss = 0.000697628
I0205 05:39:10.282994 12230 solver.cpp:253]     Train net output #0: loss = 0.000697585 (* 1 = 0.000697585 loss)
I0205 05:39:10.283010 12230 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I0205 05:39:45.104262 12230 solver.cpp:237] Iteration 1070, loss = 0.00714689
I0205 05:39:45.104472 12230 solver.cpp:253]     Train net output #0: loss = 0.00714685 (* 1 = 0.00714685 loss)
I0205 05:39:45.104488 12230 sgd_solver.cpp:106] Iteration 1070, lr = 0.001
I0205 05:40:20.527391 12230 solver.cpp:237] Iteration 1080, loss = 0.00155644
I0205 05:40:20.527612 12230 solver.cpp:253]     Train net output #0: loss = 0.0015564 (* 1 = 0.0015564 loss)
I0205 05:40:20.527628 12230 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I0205 05:40:55.564566 12230 solver.cpp:237] Iteration 1090, loss = 0.0043608
I0205 05:40:55.564779 12230 solver.cpp:253]     Train net output #0: loss = 0.00436076 (* 1 = 0.00436076 loss)
I0205 05:40:55.564795 12230 sgd_solver.cpp:106] Iteration 1090, lr = 0.001
I0205 05:41:27.003370 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1100.caffemodel
I0205 05:41:27.007155 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1100.solverstate
I0205 05:41:27.008520 12230 solver.cpp:341] Iteration 1100, Testing net (#0)
I0205 05:41:43.851160 12230 solver.cpp:409]     Test net output #0: accuracy = 0.997
I0205 05:41:43.851233 12230 solver.cpp:409]     Test net output #1: loss = 0.0167946 (* 1 = 0.0167946 loss)
I0205 05:41:47.405375 12230 solver.cpp:237] Iteration 1100, loss = 0.000387326
I0205 05:41:47.405455 12230 solver.cpp:253]     Train net output #0: loss = 0.000387286 (* 1 = 0.000387286 loss)
I0205 05:41:47.405468 12230 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I0205 05:42:22.615427 12230 solver.cpp:237] Iteration 1110, loss = 0.000203256
I0205 05:42:22.615664 12230 solver.cpp:253]     Train net output #0: loss = 0.000203216 (* 1 = 0.000203216 loss)
I0205 05:42:22.615684 12230 sgd_solver.cpp:106] Iteration 1110, lr = 0.001
I0205 05:42:58.001034 12230 solver.cpp:237] Iteration 1120, loss = 0.000331997
I0205 05:42:58.001233 12230 solver.cpp:253]     Train net output #0: loss = 0.000331958 (* 1 = 0.000331958 loss)
I0205 05:42:58.001250 12230 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I0205 05:43:33.211323 12230 solver.cpp:237] Iteration 1130, loss = 0.000573132
I0205 05:43:33.211544 12230 solver.cpp:253]     Train net output #0: loss = 0.000573095 (* 1 = 0.000573095 loss)
I0205 05:43:33.211560 12230 sgd_solver.cpp:106] Iteration 1130, lr = 0.001
I0205 05:44:08.330760 12230 solver.cpp:237] Iteration 1140, loss = 0.00266023
I0205 05:44:08.330993 12230 solver.cpp:253]     Train net output #0: loss = 0.0026602 (* 1 = 0.0026602 loss)
I0205 05:44:08.331010 12230 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I0205 05:44:43.377945 12230 solver.cpp:237] Iteration 1150, loss = 0.0582249
I0205 05:44:43.380250 12230 solver.cpp:253]     Train net output #0: loss = 0.0582249 (* 1 = 0.0582249 loss)
I0205 05:44:43.380266 12230 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I0205 05:45:18.577898 12230 solver.cpp:237] Iteration 1160, loss = 0.00110311
I0205 05:45:18.578140 12230 solver.cpp:253]     Train net output #0: loss = 0.00110307 (* 1 = 0.00110307 loss)
I0205 05:45:18.578157 12230 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I0205 05:45:53.820363 12230 solver.cpp:237] Iteration 1170, loss = 0.000402269
I0205 05:45:53.820583 12230 solver.cpp:253]     Train net output #0: loss = 0.000402233 (* 1 = 0.000402233 loss)
I0205 05:45:53.820600 12230 sgd_solver.cpp:106] Iteration 1170, lr = 0.001
I0205 05:46:29.110913 12230 solver.cpp:237] Iteration 1180, loss = 0.000464849
I0205 05:46:29.111143 12230 solver.cpp:253]     Train net output #0: loss = 0.000464813 (* 1 = 0.000464813 loss)
I0205 05:46:29.111160 12230 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I0205 05:47:04.240476 12230 solver.cpp:237] Iteration 1190, loss = 0.00235621
I0205 05:47:04.240687 12230 solver.cpp:253]     Train net output #0: loss = 0.00235617 (* 1 = 0.00235617 loss)
I0205 05:47:04.240703 12230 sgd_solver.cpp:106] Iteration 1190, lr = 0.001
I0205 05:47:35.848153 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1200.caffemodel
I0205 05:47:35.864570 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1200.solverstate
I0205 05:47:35.866804 12230 solver.cpp:341] Iteration 1200, Testing net (#0)
I0205 05:47:52.761817 12230 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 05:47:52.761888 12230 solver.cpp:409]     Test net output #1: loss = 0.0138861 (* 1 = 0.0138861 loss)
I0205 05:47:56.225158 12230 solver.cpp:237] Iteration 1200, loss = 0.000812739
I0205 05:47:56.225229 12230 solver.cpp:253]     Train net output #0: loss = 0.000812702 (* 1 = 0.000812702 loss)
I0205 05:47:56.225240 12230 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0205 05:48:31.474045 12230 solver.cpp:237] Iteration 1210, loss = 0.000458332
I0205 05:48:32.010277 12230 solver.cpp:253]     Train net output #0: loss = 0.000458292 (* 1 = 0.000458292 loss)
I0205 05:48:32.010315 12230 sgd_solver.cpp:106] Iteration 1210, lr = 0.001
I0205 05:49:07.084681 12230 solver.cpp:237] Iteration 1220, loss = 0.000233189
I0205 05:49:07.084884 12230 solver.cpp:253]     Train net output #0: loss = 0.00023315 (* 1 = 0.00023315 loss)
I0205 05:49:07.084899 12230 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I0205 05:49:42.056854 12230 solver.cpp:237] Iteration 1230, loss = 0.0351164
I0205 05:49:42.057096 12230 solver.cpp:253]     Train net output #0: loss = 0.0351164 (* 1 = 0.0351164 loss)
I0205 05:49:42.057111 12230 sgd_solver.cpp:106] Iteration 1230, lr = 0.001
I0205 05:50:16.926610 12230 solver.cpp:237] Iteration 1240, loss = 0.000107694
I0205 05:50:16.926863 12230 solver.cpp:253]     Train net output #0: loss = 0.000107659 (* 1 = 0.000107659 loss)
I0205 05:50:16.926879 12230 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I0205 05:50:51.831848 12230 solver.cpp:237] Iteration 1250, loss = 0.016349
I0205 05:50:51.832080 12230 solver.cpp:253]     Train net output #0: loss = 0.016349 (* 1 = 0.016349 loss)
I0205 05:50:51.832095 12230 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I0205 05:51:26.783452 12230 solver.cpp:237] Iteration 1260, loss = 0.000470845
I0205 05:51:26.783653 12230 solver.cpp:253]     Train net output #0: loss = 0.000470807 (* 1 = 0.000470807 loss)
I0205 05:51:26.783668 12230 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I0205 05:52:01.645038 12230 solver.cpp:237] Iteration 1270, loss = 0.000620311
I0205 05:52:01.645259 12230 solver.cpp:253]     Train net output #0: loss = 0.000620274 (* 1 = 0.000620274 loss)
I0205 05:52:01.645274 12230 sgd_solver.cpp:106] Iteration 1270, lr = 0.001
I0205 05:52:36.877643 12230 solver.cpp:237] Iteration 1280, loss = 0.00166625
I0205 05:52:36.877861 12230 solver.cpp:253]     Train net output #0: loss = 0.00166621 (* 1 = 0.00166621 loss)
I0205 05:52:36.877876 12230 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I0205 05:53:11.934104 12230 solver.cpp:237] Iteration 1290, loss = 0.0071552
I0205 05:53:11.934300 12230 solver.cpp:253]     Train net output #0: loss = 0.00715516 (* 1 = 0.00715516 loss)
I0205 05:53:11.934314 12230 sgd_solver.cpp:106] Iteration 1290, lr = 0.001
I0205 05:53:43.722061 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1300.caffemodel
I0205 05:53:43.725811 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1300.solverstate
I0205 05:53:43.727183 12230 solver.cpp:341] Iteration 1300, Testing net (#0)
I0205 05:54:00.556448 12230 solver.cpp:409]     Test net output #0: accuracy = 0.996
I0205 05:54:00.556509 12230 solver.cpp:409]     Test net output #1: loss = 0.0219884 (* 1 = 0.0219884 loss)
I0205 05:54:04.026548 12230 solver.cpp:237] Iteration 1300, loss = 0.0867605
I0205 05:54:04.026615 12230 solver.cpp:253]     Train net output #0: loss = 0.0867604 (* 1 = 0.0867604 loss)
I0205 05:54:04.026628 12230 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I0205 05:54:39.071570 12230 solver.cpp:237] Iteration 1310, loss = 0.0499791
I0205 05:54:39.071765 12230 solver.cpp:253]     Train net output #0: loss = 0.049979 (* 1 = 0.049979 loss)
I0205 05:54:39.071779 12230 sgd_solver.cpp:106] Iteration 1310, lr = 0.001
I0205 05:55:14.236830 12230 solver.cpp:237] Iteration 1320, loss = 0.00613191
I0205 05:55:14.237035 12230 solver.cpp:253]     Train net output #0: loss = 0.00613188 (* 1 = 0.00613188 loss)
I0205 05:55:14.237049 12230 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I0205 05:55:49.601707 12230 solver.cpp:237] Iteration 1330, loss = 0.000592919
I0205 05:55:49.601912 12230 solver.cpp:253]     Train net output #0: loss = 0.000592886 (* 1 = 0.000592886 loss)
I0205 05:55:49.601927 12230 sgd_solver.cpp:106] Iteration 1330, lr = 0.001
I0205 05:56:25.060225 12230 solver.cpp:237] Iteration 1340, loss = 0.000296295
I0205 05:56:25.060432 12230 solver.cpp:253]     Train net output #0: loss = 0.000296258 (* 1 = 0.000296258 loss)
I0205 05:56:25.060446 12230 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I0205 05:57:00.861989 12230 solver.cpp:237] Iteration 1350, loss = 0.000851838
I0205 05:57:00.862215 12230 solver.cpp:253]     Train net output #0: loss = 0.0008518 (* 1 = 0.0008518 loss)
I0205 05:57:00.862228 12230 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I0205 05:57:36.500246 12230 solver.cpp:237] Iteration 1360, loss = 0.000685325
I0205 05:57:36.500452 12230 solver.cpp:253]     Train net output #0: loss = 0.000685287 (* 1 = 0.000685287 loss)
I0205 05:57:36.500464 12230 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I0205 05:58:12.016706 12230 solver.cpp:237] Iteration 1370, loss = 0.00686926
I0205 05:58:12.016934 12230 solver.cpp:253]     Train net output #0: loss = 0.00686922 (* 1 = 0.00686922 loss)
I0205 05:58:12.016949 12230 sgd_solver.cpp:106] Iteration 1370, lr = 0.001
I0205 05:58:47.564652 12230 solver.cpp:237] Iteration 1380, loss = 0.00396981
I0205 05:58:47.564863 12230 solver.cpp:253]     Train net output #0: loss = 0.00396977 (* 1 = 0.00396977 loss)
I0205 05:58:47.564878 12230 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I0205 05:59:22.939714 12230 solver.cpp:237] Iteration 1390, loss = 0.0104301
I0205 05:59:22.939914 12230 solver.cpp:253]     Train net output #0: loss = 0.01043 (* 1 = 0.01043 loss)
I0205 05:59:22.939929 12230 sgd_solver.cpp:106] Iteration 1390, lr = 0.001
I0205 05:59:54.976513 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1400.caffemodel
I0205 05:59:54.980257 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1400.solverstate
I0205 05:59:54.981611 12230 solver.cpp:341] Iteration 1400, Testing net (#0)
I0205 06:00:12.191148 12230 solver.cpp:409]     Test net output #0: accuracy = 0.999
I0205 06:00:12.191211 12230 solver.cpp:409]     Test net output #1: loss = 0.00200026 (* 1 = 0.00200026 loss)
I0205 06:00:15.715276 12230 solver.cpp:237] Iteration 1400, loss = 0.000877282
I0205 06:00:15.715347 12230 solver.cpp:253]     Train net output #0: loss = 0.000877244 (* 1 = 0.000877244 loss)
I0205 06:00:15.715359 12230 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0205 06:00:51.321270 12230 solver.cpp:237] Iteration 1410, loss = 0.000573548
I0205 06:00:51.321467 12230 solver.cpp:253]     Train net output #0: loss = 0.000573509 (* 1 = 0.000573509 loss)
I0205 06:00:51.321481 12230 sgd_solver.cpp:106] Iteration 1410, lr = 0.001
I0205 06:01:26.915210 12230 solver.cpp:237] Iteration 1420, loss = 0.00108381
I0205 06:01:26.915421 12230 solver.cpp:253]     Train net output #0: loss = 0.00108377 (* 1 = 0.00108377 loss)
I0205 06:01:26.915436 12230 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I0205 06:02:02.922849 12230 solver.cpp:237] Iteration 1430, loss = 0.0189522
I0205 06:02:02.923048 12230 solver.cpp:253]     Train net output #0: loss = 0.0189521 (* 1 = 0.0189521 loss)
I0205 06:02:02.923063 12230 sgd_solver.cpp:106] Iteration 1430, lr = 0.001
I0205 06:02:38.821220 12230 solver.cpp:237] Iteration 1440, loss = 0.00031984
I0205 06:02:38.821416 12230 solver.cpp:253]     Train net output #0: loss = 0.000319802 (* 1 = 0.000319802 loss)
I0205 06:02:38.821431 12230 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I0205 06:03:14.585980 12230 solver.cpp:237] Iteration 1450, loss = 0.000992088
I0205 06:03:14.586172 12230 solver.cpp:253]     Train net output #0: loss = 0.000992051 (* 1 = 0.000992051 loss)
I0205 06:03:14.586186 12230 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I0205 06:03:50.364456 12230 solver.cpp:237] Iteration 1460, loss = 0.0180373
I0205 06:03:50.364645 12230 solver.cpp:253]     Train net output #0: loss = 0.0180373 (* 1 = 0.0180373 loss)
I0205 06:03:50.364658 12230 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I0205 06:04:25.907368 12230 solver.cpp:237] Iteration 1470, loss = 0.0392891
I0205 06:04:25.907567 12230 solver.cpp:253]     Train net output #0: loss = 0.0392891 (* 1 = 0.0392891 loss)
I0205 06:04:25.907582 12230 sgd_solver.cpp:106] Iteration 1470, lr = 0.001
I0205 06:05:00.907145 12230 solver.cpp:237] Iteration 1480, loss = 0.000338808
I0205 06:05:00.907327 12230 solver.cpp:253]     Train net output #0: loss = 0.000338787 (* 1 = 0.000338787 loss)
I0205 06:05:00.907343 12230 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I0205 06:05:35.531374 12230 solver.cpp:237] Iteration 1490, loss = 0.000356655
I0205 06:05:35.531661 12230 solver.cpp:253]     Train net output #0: loss = 0.000356633 (* 1 = 0.000356633 loss)
I0205 06:05:35.531677 12230 sgd_solver.cpp:106] Iteration 1490, lr = 0.001
I0205 06:06:06.455221 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1500.caffemodel
I0205 06:06:06.458945 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1500.solverstate
I0205 06:06:06.460357 12230 solver.cpp:341] Iteration 1500, Testing net (#0)
I0205 06:06:22.942317 12230 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 06:06:22.942373 12230 solver.cpp:409]     Test net output #1: loss = 0.0103765 (* 1 = 0.0103765 loss)
I0205 06:06:26.371670 12230 solver.cpp:237] Iteration 1500, loss = 0.000346919
I0205 06:06:26.371728 12230 solver.cpp:253]     Train net output #0: loss = 0.000346897 (* 1 = 0.000346897 loss)
I0205 06:06:26.371740 12230 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I0205 06:07:00.539115 12230 solver.cpp:237] Iteration 1510, loss = 9.10709e-05
I0205 06:07:00.539321 12230 solver.cpp:253]     Train net output #0: loss = 9.10484e-05 (* 1 = 9.10484e-05 loss)
I0205 06:07:00.539336 12230 sgd_solver.cpp:106] Iteration 1510, lr = 0.001
I0205 06:07:34.999399 12230 solver.cpp:237] Iteration 1520, loss = 0.000797766
I0205 06:07:34.999586 12230 solver.cpp:253]     Train net output #0: loss = 0.000797744 (* 1 = 0.000797744 loss)
I0205 06:07:34.999601 12230 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I0205 06:08:09.059905 12230 solver.cpp:237] Iteration 1530, loss = 0.000370135
I0205 06:08:09.060124 12230 solver.cpp:253]     Train net output #0: loss = 0.000370112 (* 1 = 0.000370112 loss)
I0205 06:08:09.060138 12230 sgd_solver.cpp:106] Iteration 1530, lr = 0.001
I0205 06:08:43.457777 12230 solver.cpp:237] Iteration 1540, loss = 0.00334694
I0205 06:08:43.457960 12230 solver.cpp:253]     Train net output #0: loss = 0.00334691 (* 1 = 0.00334691 loss)
I0205 06:08:43.457974 12230 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I0205 06:09:17.565086 12230 solver.cpp:237] Iteration 1550, loss = 0.00387493
I0205 06:09:17.565299 12230 solver.cpp:253]     Train net output #0: loss = 0.00387491 (* 1 = 0.00387491 loss)
I0205 06:09:17.565315 12230 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I0205 06:09:51.655196 12230 solver.cpp:237] Iteration 1560, loss = 0.00441143
I0205 06:09:51.655380 12230 solver.cpp:253]     Train net output #0: loss = 0.00441141 (* 1 = 0.00441141 loss)
I0205 06:09:51.655395 12230 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I0205 06:10:25.623672 12230 solver.cpp:237] Iteration 1570, loss = 9.87917e-05
I0205 06:10:25.623873 12230 solver.cpp:253]     Train net output #0: loss = 9.87681e-05 (* 1 = 9.87681e-05 loss)
I0205 06:10:25.623888 12230 sgd_solver.cpp:106] Iteration 1570, lr = 0.001
I0205 06:10:59.565265 12230 solver.cpp:237] Iteration 1580, loss = 8.54153e-05
I0205 06:10:59.565462 12230 solver.cpp:253]     Train net output #0: loss = 8.53931e-05 (* 1 = 8.53931e-05 loss)
I0205 06:10:59.565477 12230 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I0205 06:11:33.603024 12230 solver.cpp:237] Iteration 1590, loss = 0.000395048
I0205 06:11:33.603207 12230 solver.cpp:253]     Train net output #0: loss = 0.000395027 (* 1 = 0.000395027 loss)
I0205 06:11:33.603222 12230 sgd_solver.cpp:106] Iteration 1590, lr = 0.001
I0205 06:12:04.268831 12230 solver.cpp:459] Snapshotting to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1600.caffemodel
I0205 06:12:04.272716 12230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file /home/pearlstl/proj_cv/master/exp/gray_8000_conv1num32_conv2num32_conv3num32_conv4num32_conv5num16_seed242/snaps/snap__iter_1600.solverstate
I0205 06:12:05.893466 12230 solver.cpp:321] Iteration 1600, loss = 0.000848817
I0205 06:12:05.893515 12230 solver.cpp:341] Iteration 1600, Testing net (#0)
I0205 06:12:22.327117 12230 solver.cpp:409]     Test net output #0: accuracy = 0.998
I0205 06:12:22.327193 12230 solver.cpp:409]     Test net output #1: loss = 0.00617698 (* 1 = 0.00617698 loss)
I0205 06:12:22.327201 12230 solver.cpp:326] Optimization Done.
I0205 06:12:22.327206 12230 caffe.cpp:215] Optimization Done.
